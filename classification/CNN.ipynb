{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "class Mydataset(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        if train:\n",
    "            print('loading trainig dataset')\n",
    "            file = 'nn_train_poetry.csv'\n",
    "        else:\n",
    "            print('loading testing dataset')\n",
    "            file = 'nn_test_poetry.csv'\n",
    "        with open(file, 'r', newline='', encoding='utf-8') as myFile:  \n",
    "            rdr = csv.reader(myFile)\n",
    "            temp = list(rdr)\n",
    "            self.x = [a[0][:200] for a in temp]\n",
    "            self.y = [a[1] for a in temp]\n",
    "        self.len = len(self.x)\n",
    "        self.labels = list(sorted(set(self.y)))\n",
    "        self.alphabet = list(sorted(set(['+','-','\\t'])))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "        \n",
    "    def get_labels(self):\n",
    "        return self.labels\n",
    "    \n",
    "    def get_alphabet(self):\n",
    "        return self.alphabet\n",
    "    \n",
    "    def get_alphabet_id(self, c):\n",
    "        return self.alphabet.index(c)\n",
    "    \n",
    "    def get_label(self, id):\n",
    "        return self.labels[id]\n",
    "    \n",
    "    def get_label_id(self, label):\n",
    "        return self.labels.index(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading trainig dataset\n",
      "loading testing dataset\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Mydataset()\n",
    "test_dataset = Mydataset(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "N_LABELS = len(train_dataset.get_labels())\n",
    "N_ALPHABET = len(train_dataset.get_alphabet())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    pass\n",
    "\n",
    "args = args()\n",
    "\n",
    "args.class_num = N_LABELS\n",
    "args.kernel_num = 100\n",
    "args.kernel_sizes = [2,3,4,5]\n",
    "args.dropout = 0.5\n",
    "args.static = True\n",
    "args.lr = 0.001\n",
    "args.epochs = 256\n",
    "args.embeding_num = N_ALPHABET\n",
    "args.embeding_dim = N_ALPHABET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_Text(\n",
       "  (embed): Embedding(3, 3)\n",
       "  (convs1): ModuleList(\n",
       "    (0): Conv2d(1, 100, kernel_size=(2, 3), stride=(1, 1))\n",
       "    (1): Conv2d(1, 100, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (2): Conv2d(1, 100, kernel_size=(4, 3), stride=(1, 1))\n",
       "    (3): Conv2d(1, 100, kernel_size=(5, 3), stride=(1, 1))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (fc1): Linear(in_features=400, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class CNN_Text(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(CNN_Text, self).__init__()\n",
    "        self.args = args\n",
    "        \n",
    "        C = args.class_num\n",
    "        Ci = 1\n",
    "        Co = args.kernel_num\n",
    "        Ks = args.kernel_sizes\n",
    "        V = args.embeding_num\n",
    "        D = args.embeding_dim\n",
    "        \n",
    "        self.embed = nn.Embedding(V, D)\n",
    "\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
    "\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "\n",
    "        self.fc1 = nn.Linear(len(Ks)*Co, C)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.args.static:\n",
    "            x = Variable(x)\n",
    "\n",
    "        x = x.unsqueeze(1)  # (N, Ci, W, D)\n",
    "\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n",
    "\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
    "\n",
    "        x = torch.cat(x, 1)\n",
    "\n",
    "        x = self.dropout(x)  # (N, len(Ks)*Co)\n",
    "\n",
    "        logit = self.fc1(x)  # (N, C)\n",
    "        return logit\n",
    "model = CNN_Text(args)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.log_interval = 10\n",
    "args.test_interval = 50\n",
    "\n",
    "def train(model, train_loader, test_loader, args):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    model.train()\n",
    "    \n",
    "    steps = 0\n",
    "    \n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        batch = 0\n",
    "        for x, y in train_loader:\n",
    "            x_t, y_t = build_tensor(x, y)\n",
    "            optimizer.zero_grad()\n",
    "            logit = model(x_t)\n",
    "            loss = F.cross_entropy(logit, y_t)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            steps += 1\n",
    "            if steps % args.log_interval == 0:\n",
    "                corrects = (torch.max(logit, 1)[1].view(y_t.size()).data == y_t.data).sum()\n",
    "                accuracy = 100.0 * corrects/y_t.shape[0]\n",
    "                print(\n",
    "                    '\\rBatch[{}] - loss: {:.6f}  acc: {:.4f}%({}/{})'.format(steps,\n",
    "                                                                             loss.data[0], \n",
    "                                                                             accuracy,\n",
    "                                                                             corrects,\n",
    "                                                                             y_t.shape[0]))\n",
    "            if steps % args.test_interval == 0:\n",
    "                acc = eval(test_loader, model, args)\n",
    "                \n",
    "def eval(test_loader, model, args):\n",
    "    model.eval()\n",
    "    corrects, avg_loss = 0, 0\n",
    "    for x, y in test_loader:\n",
    "        x_t, y_t = build_tensor(x, y)\n",
    "        logit = model(x_t)\n",
    "        loss = F.cross_entropy(logit, y_t, size_average=False)\n",
    "        avg_loss += loss.data[0]\n",
    "        corrects += (torch.max(logit, 1)[1].view(y_t.size()).data == y_t.data).sum()\n",
    "    size = len(test_loader.dataset)\n",
    "    avg_loss /= size\n",
    "    accuracy = 100.0 * corrects/size\n",
    "    print('\\nEvaluation - loss: {:.6f}  acc: {:.4f}%({}/{}) \\n'.format(avg_loss, \n",
    "                                                                       accuracy, \n",
    "                                                                       corrects, \n",
    "                                                                       size))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\settings\\anaconda\\lib\\site-packages\\ipykernel\\__main__.py:25: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch[10] - loss: 0.728508  acc: 49.0000%(63/128)\n",
      "Batch[20] - loss: 0.705693  acc: 54.0000%(70/128)\n",
      "Batch[30] - loss: 0.681475  acc: 48.0000%(39/80)\n",
      "Batch[40] - loss: 0.667217  acc: 54.0000%(70/128)\n",
      "Batch[50] - loss: 0.664646  acc: 53.0000%(68/128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\settings\\anaconda\\lib\\site-packages\\ipykernel\\__main__.py:39: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation - loss: 0.655617  acc: 67.0000%(141/208) \n",
      "\n",
      "Batch[60] - loss: 0.652416  acc: 62.0000%(50/80)\n",
      "Batch[70] - loss: 0.626612  acc: 69.0000%(89/128)\n",
      "Batch[80] - loss: 0.625888  acc: 69.0000%(89/128)\n",
      "Batch[90] - loss: 0.593255  acc: 75.0000%(60/80)\n",
      "Batch[100] - loss: 0.582155  acc: 75.0000%(97/128)\n",
      "\n",
      "Evaluation - loss: 0.619004  acc: 67.0000%(140/208) \n",
      "\n",
      "Batch[110] - loss: 0.578546  acc: 74.0000%(95/128)\n",
      "Batch[120] - loss: 0.549161  acc: 76.0000%(61/80)\n",
      "Batch[130] - loss: 0.549414  acc: 75.0000%(97/128)\n",
      "Batch[140] - loss: 0.562012  acc: 73.0000%(94/128)\n",
      "Batch[150] - loss: 0.492411  acc: 76.0000%(61/80)\n",
      "\n",
      "Evaluation - loss: 0.595296  acc: 68.0000%(142/208) \n",
      "\n",
      "Batch[160] - loss: 0.502294  acc: 82.0000%(106/128)\n",
      "Batch[170] - loss: 0.537878  acc: 82.0000%(105/128)\n",
      "Batch[180] - loss: 0.487722  acc: 76.0000%(61/80)\n",
      "Batch[190] - loss: 0.483826  acc: 79.0000%(102/128)\n",
      "Batch[200] - loss: 0.586275  acc: 72.0000%(93/128)\n",
      "\n",
      "Evaluation - loss: 0.602046  acc: 70.0000%(147/208) \n",
      "\n",
      "Batch[210] - loss: 0.510617  acc: 81.0000%(65/80)\n",
      "Batch[220] - loss: 0.574636  acc: 71.0000%(92/128)\n",
      "Batch[230] - loss: 0.490524  acc: 77.0000%(99/128)\n",
      "Batch[240] - loss: 0.565367  acc: 71.0000%(57/80)\n",
      "Batch[250] - loss: 0.493308  acc: 75.0000%(97/128)\n",
      "\n",
      "Evaluation - loss: 0.582908  acc: 72.0000%(151/208) \n",
      "\n",
      "Batch[260] - loss: 0.492197  acc: 78.0000%(101/128)\n",
      "Batch[270] - loss: 0.536762  acc: 72.0000%(58/80)\n",
      "Batch[280] - loss: 0.515657  acc: 74.0000%(95/128)\n",
      "Batch[290] - loss: 0.493088  acc: 76.0000%(98/128)\n",
      "Batch[300] - loss: 0.520600  acc: 73.0000%(59/80)\n",
      "\n",
      "Evaluation - loss: 0.579810  acc: 70.0000%(147/208) \n",
      "\n",
      "Batch[310] - loss: 0.531600  acc: 75.0000%(96/128)\n",
      "Batch[320] - loss: 0.535702  acc: 72.0000%(93/128)\n",
      "Batch[330] - loss: 0.450239  acc: 82.0000%(66/80)\n",
      "Batch[340] - loss: 0.518927  acc: 76.0000%(98/128)\n",
      "Batch[350] - loss: 0.515060  acc: 75.0000%(97/128)\n",
      "\n",
      "Evaluation - loss: 0.576351  acc: 71.0000%(149/208) \n",
      "\n",
      "Batch[360] - loss: 0.508249  acc: 71.0000%(57/80)\n",
      "Batch[370] - loss: 0.500596  acc: 75.0000%(96/128)\n",
      "Batch[380] - loss: 0.516517  acc: 71.0000%(92/128)\n",
      "Batch[390] - loss: 0.410599  acc: 80.0000%(64/80)\n",
      "Batch[400] - loss: 0.502861  acc: 71.0000%(92/128)\n",
      "\n",
      "Evaluation - loss: 0.582050  acc: 71.0000%(149/208) \n",
      "\n",
      "Batch[410] - loss: 0.517767  acc: 76.0000%(98/128)\n",
      "Batch[420] - loss: 0.544383  acc: 70.0000%(56/80)\n",
      "Batch[430] - loss: 0.602236  acc: 71.0000%(91/128)\n",
      "Batch[440] - loss: 0.489308  acc: 76.0000%(98/128)\n",
      "Batch[450] - loss: 0.477817  acc: 80.0000%(64/80)\n",
      "\n",
      "Evaluation - loss: 0.572017  acc: 71.0000%(149/208) \n",
      "\n",
      "Batch[460] - loss: 0.493832  acc: 76.0000%(98/128)\n",
      "Batch[470] - loss: 0.476399  acc: 76.0000%(98/128)\n",
      "Batch[480] - loss: 0.481090  acc: 72.0000%(58/80)\n",
      "Batch[490] - loss: 0.530593  acc: 72.0000%(93/128)\n",
      "Batch[500] - loss: 0.469964  acc: 78.0000%(101/128)\n",
      "\n",
      "Evaluation - loss: 0.576192  acc: 70.0000%(146/208) \n",
      "\n",
      "Batch[510] - loss: 0.459655  acc: 76.0000%(61/80)\n",
      "Batch[520] - loss: 0.469022  acc: 78.0000%(101/128)\n",
      "Batch[530] - loss: 0.438603  acc: 82.0000%(105/128)\n",
      "Batch[540] - loss: 0.494337  acc: 77.0000%(62/80)\n",
      "Batch[550] - loss: 0.481007  acc: 75.0000%(97/128)\n",
      "\n",
      "Evaluation - loss: 0.581924  acc: 69.0000%(144/208) \n",
      "\n",
      "Batch[560] - loss: 0.499706  acc: 71.0000%(92/128)\n",
      "Batch[570] - loss: 0.458332  acc: 76.0000%(61/80)\n",
      "Batch[580] - loss: 0.446192  acc: 80.0000%(103/128)\n",
      "Batch[590] - loss: 0.486895  acc: 75.0000%(96/128)\n",
      "Batch[600] - loss: 0.481936  acc: 76.0000%(61/80)\n",
      "\n",
      "Evaluation - loss: 0.569686  acc: 71.0000%(149/208) \n",
      "\n",
      "Batch[610] - loss: 0.482875  acc: 76.0000%(98/128)\n",
      "Batch[620] - loss: 0.535171  acc: 76.0000%(98/128)\n",
      "Batch[630] - loss: 0.497126  acc: 75.0000%(60/80)\n",
      "Batch[640] - loss: 0.561312  acc: 72.0000%(93/128)\n",
      "Batch[650] - loss: 0.494473  acc: 75.0000%(96/128)\n",
      "\n",
      "Evaluation - loss: 0.568430  acc: 71.0000%(149/208) \n",
      "\n",
      "Batch[660] - loss: 0.466768  acc: 73.0000%(59/80)\n",
      "Batch[670] - loss: 0.515268  acc: 73.0000%(94/128)\n",
      "Batch[680] - loss: 0.461702  acc: 73.0000%(94/128)\n",
      "Batch[690] - loss: 0.561110  acc: 70.0000%(56/80)\n",
      "Batch[700] - loss: 0.510405  acc: 73.0000%(94/128)\n",
      "\n",
      "Evaluation - loss: 0.575061  acc: 72.0000%(151/208) \n",
      "\n",
      "Batch[710] - loss: 0.508252  acc: 77.0000%(99/128)\n",
      "Batch[720] - loss: 0.386354  acc: 81.0000%(65/80)\n",
      "Batch[730] - loss: 0.504609  acc: 74.0000%(95/128)\n",
      "Batch[740] - loss: 0.418264  acc: 79.0000%(102/128)\n",
      "Batch[750] - loss: 0.534409  acc: 72.0000%(58/80)\n",
      "\n",
      "Evaluation - loss: 0.571818  acc: 69.0000%(145/208) \n",
      "\n",
      "Batch[760] - loss: 0.459101  acc: 77.0000%(99/128)\n",
      "Batch[770] - loss: 0.461712  acc: 81.0000%(104/128)\n",
      "Batch[780] - loss: 0.523083  acc: 73.0000%(59/80)\n",
      "Batch[790] - loss: 0.464923  acc: 77.0000%(99/128)\n",
      "Batch[800] - loss: 0.512294  acc: 74.0000%(95/128)\n",
      "\n",
      "Evaluation - loss: 0.567283  acc: 71.0000%(148/208) \n",
      "\n",
      "Batch[810] - loss: 0.354711  acc: 85.0000%(68/80)\n",
      "Batch[820] - loss: 0.459068  acc: 79.0000%(102/128)\n",
      "Batch[830] - loss: 0.493239  acc: 78.0000%(101/128)\n",
      "Batch[840] - loss: 0.484127  acc: 78.0000%(63/80)\n",
      "Batch[850] - loss: 0.527234  acc: 69.0000%(89/128)\n",
      "\n",
      "Evaluation - loss: 0.567372  acc: 72.0000%(151/208) \n",
      "\n",
      "Batch[860] - loss: 0.486793  acc: 75.0000%(97/128)\n",
      "Batch[870] - loss: 0.527310  acc: 76.0000%(61/80)\n",
      "Batch[880] - loss: 0.517862  acc: 75.0000%(97/128)\n",
      "Batch[890] - loss: 0.480431  acc: 77.0000%(99/128)\n",
      "Batch[900] - loss: 0.435951  acc: 80.0000%(64/80)\n",
      "\n",
      "Evaluation - loss: 0.566326  acc: 71.0000%(149/208) \n",
      "\n",
      "Batch[910] - loss: 0.485876  acc: 75.0000%(97/128)\n",
      "Batch[920] - loss: 0.369723  acc: 82.0000%(105/128)\n",
      "Batch[930] - loss: 0.391815  acc: 85.0000%(68/80)\n",
      "Batch[940] - loss: 0.483093  acc: 77.0000%(99/128)\n",
      "Batch[950] - loss: 0.455536  acc: 79.0000%(102/128)\n",
      "\n",
      "Evaluation - loss: 0.566142  acc: 72.0000%(150/208) \n",
      "\n",
      "Batch[960] - loss: 0.494541  acc: 72.0000%(58/80)\n",
      "Batch[970] - loss: 0.526400  acc: 69.0000%(89/128)\n",
      "Batch[980] - loss: 0.466195  acc: 77.0000%(99/128)\n",
      "Batch[990] - loss: 0.376659  acc: 85.0000%(68/80)\n",
      "Batch[1000] - loss: 0.509576  acc: 75.0000%(96/128)\n",
      "\n",
      "Evaluation - loss: 0.564823  acc: 71.0000%(149/208) \n",
      "\n",
      "Batch[1010] - loss: 0.469660  acc: 80.0000%(103/128)\n",
      "Batch[1020] - loss: 0.479449  acc: 77.0000%(62/80)\n",
      "Batch[1030] - loss: 0.434536  acc: 78.0000%(100/128)\n",
      "Batch[1040] - loss: 0.407242  acc: 80.0000%(103/128)\n",
      "Batch[1050] - loss: 0.471340  acc: 78.0000%(63/80)\n",
      "\n",
      "Evaluation - loss: 0.564836  acc: 71.0000%(149/208) \n",
      "\n",
      "Batch[1060] - loss: 0.458166  acc: 79.0000%(102/128)\n",
      "Batch[1070] - loss: 0.482846  acc: 79.0000%(102/128)\n",
      "Batch[1080] - loss: 0.401652  acc: 82.0000%(66/80)\n",
      "Batch[1090] - loss: 0.474582  acc: 77.0000%(99/128)\n",
      "Batch[1100] - loss: 0.499080  acc: 71.0000%(91/128)\n",
      "\n",
      "Evaluation - loss: 0.565810  acc: 69.0000%(144/208) \n",
      "\n",
      "Batch[1110] - loss: 0.535584  acc: 76.0000%(61/80)\n",
      "Batch[1120] - loss: 0.432022  acc: 75.0000%(96/128)\n",
      "Batch[1130] - loss: 0.468632  acc: 81.0000%(104/128)\n",
      "Batch[1140] - loss: 0.429367  acc: 82.0000%(66/80)\n",
      "Batch[1150] - loss: 0.453026  acc: 77.0000%(99/128)\n",
      "\n",
      "Evaluation - loss: 0.563672  acc: 72.0000%(150/208) \n",
      "\n",
      "Batch[1160] - loss: 0.426584  acc: 80.0000%(103/128)\n",
      "Batch[1170] - loss: 0.465867  acc: 73.0000%(59/80)\n",
      "Batch[1180] - loss: 0.465848  acc: 75.0000%(97/128)\n",
      "Batch[1190] - loss: 0.447004  acc: 80.0000%(103/128)\n",
      "Batch[1200] - loss: 0.475725  acc: 77.0000%(62/80)\n",
      "\n",
      "Evaluation - loss: 0.564946  acc: 70.0000%(147/208) \n",
      "\n",
      "Batch[1210] - loss: 0.454673  acc: 78.0000%(101/128)\n",
      "Batch[1220] - loss: 0.457366  acc: 75.0000%(96/128)\n",
      "Batch[1230] - loss: 0.468957  acc: 75.0000%(60/80)\n",
      "Batch[1240] - loss: 0.510989  acc: 76.0000%(98/128)\n",
      "Batch[1250] - loss: 0.447600  acc: 80.0000%(103/128)\n",
      "\n",
      "Evaluation - loss: 0.569035  acc: 68.0000%(143/208) \n",
      "\n",
      "Batch[1260] - loss: 0.477261  acc: 76.0000%(61/80)\n",
      "Batch[1270] - loss: 0.535339  acc: 70.0000%(90/128)\n",
      "Batch[1280] - loss: 0.408847  acc: 85.0000%(109/128)\n",
      "Batch[1290] - loss: 0.568410  acc: 72.0000%(58/80)\n",
      "Batch[1300] - loss: 0.486747  acc: 77.0000%(99/128)\n",
      "\n",
      "Evaluation - loss: 0.566137  acc: 68.0000%(142/208) \n",
      "\n",
      "Batch[1310] - loss: 0.527122  acc: 77.0000%(99/128)\n",
      "Batch[1320] - loss: 0.437224  acc: 80.0000%(64/80)\n",
      "Batch[1330] - loss: 0.418814  acc: 80.0000%(103/128)\n",
      "Batch[1340] - loss: 0.487634  acc: 73.0000%(94/128)\n",
      "Batch[1350] - loss: 0.420790  acc: 78.0000%(63/80)\n",
      "\n",
      "Evaluation - loss: 0.564200  acc: 70.0000%(147/208) \n",
      "\n",
      "Batch[1360] - loss: 0.455462  acc: 78.0000%(100/128)\n",
      "Batch[1370] - loss: 0.425053  acc: 78.0000%(101/128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch[1380] - loss: 0.452169  acc: 76.0000%(61/80)\n",
      "Batch[1390] - loss: 0.485420  acc: 76.0000%(98/128)\n",
      "Batch[1400] - loss: 0.413904  acc: 82.0000%(105/128)\n",
      "\n",
      "Evaluation - loss: 0.565057  acc: 70.0000%(147/208) \n",
      "\n",
      "Batch[1410] - loss: 0.413796  acc: 73.0000%(59/80)\n",
      "Batch[1420] - loss: 0.400308  acc: 85.0000%(110/128)\n",
      "Batch[1430] - loss: 0.369230  acc: 83.0000%(107/128)\n",
      "Batch[1440] - loss: 0.379034  acc: 81.0000%(65/80)\n",
      "Batch[1450] - loss: 0.449690  acc: 78.0000%(101/128)\n",
      "\n",
      "Evaluation - loss: 0.565245  acc: 70.0000%(147/208) \n",
      "\n",
      "Batch[1460] - loss: 0.384875  acc: 85.0000%(109/128)\n",
      "Batch[1470] - loss: 0.336992  acc: 86.0000%(69/80)\n",
      "Batch[1480] - loss: 0.411611  acc: 83.0000%(107/128)\n",
      "Batch[1490] - loss: 0.489075  acc: 79.0000%(102/128)\n",
      "Batch[1500] - loss: 0.418026  acc: 82.0000%(66/80)\n",
      "\n",
      "Evaluation - loss: 0.566208  acc: 70.0000%(146/208) \n",
      "\n",
      "Batch[1510] - loss: 0.489313  acc: 75.0000%(97/128)\n",
      "Batch[1520] - loss: 0.502534  acc: 74.0000%(95/128)\n",
      "Batch[1530] - loss: 0.401929  acc: 86.0000%(69/80)\n",
      "Batch[1540] - loss: 0.366955  acc: 82.0000%(106/128)\n",
      "Batch[1550] - loss: 0.552596  acc: 68.0000%(88/128)\n",
      "\n",
      "Evaluation - loss: 0.566076  acc: 68.0000%(143/208) \n",
      "\n",
      "Batch[1560] - loss: 0.385250  acc: 87.0000%(70/80)\n",
      "Batch[1570] - loss: 0.489072  acc: 76.0000%(98/128)\n",
      "Batch[1580] - loss: 0.490242  acc: 78.0000%(101/128)\n",
      "Batch[1590] - loss: 0.460235  acc: 80.0000%(64/80)\n",
      "Batch[1600] - loss: 0.459396  acc: 77.0000%(99/128)\n",
      "\n",
      "Evaluation - loss: 0.571825  acc: 69.0000%(145/208) \n",
      "\n",
      "Batch[1610] - loss: 0.408987  acc: 82.0000%(105/128)\n",
      "Batch[1620] - loss: 0.513710  acc: 72.0000%(58/80)\n",
      "Batch[1630] - loss: 0.432683  acc: 81.0000%(104/128)\n",
      "Batch[1640] - loss: 0.401412  acc: 82.0000%(105/128)\n",
      "Batch[1650] - loss: 0.416270  acc: 78.0000%(63/80)\n",
      "\n",
      "Evaluation - loss: 0.566894  acc: 69.0000%(144/208) \n",
      "\n",
      "Batch[1660] - loss: 0.485662  acc: 78.0000%(101/128)\n",
      "Batch[1670] - loss: 0.410447  acc: 80.0000%(103/128)\n",
      "Batch[1680] - loss: 0.378912  acc: 81.0000%(65/80)\n",
      "Batch[1690] - loss: 0.395956  acc: 77.0000%(99/128)\n",
      "Batch[1700] - loss: 0.457102  acc: 78.0000%(100/128)\n",
      "\n",
      "Evaluation - loss: 0.568297  acc: 68.0000%(143/208) \n",
      "\n",
      "Batch[1710] - loss: 0.489924  acc: 76.0000%(61/80)\n",
      "Batch[1720] - loss: 0.383126  acc: 78.0000%(101/128)\n",
      "Batch[1730] - loss: 0.461569  acc: 78.0000%(100/128)\n",
      "Batch[1740] - loss: 0.385690  acc: 86.0000%(69/80)\n",
      "Batch[1750] - loss: 0.483040  acc: 75.0000%(97/128)\n",
      "\n",
      "Evaluation - loss: 0.569026  acc: 69.0000%(145/208) \n",
      "\n",
      "Batch[1760] - loss: 0.452321  acc: 77.0000%(99/128)\n",
      "Batch[1770] - loss: 0.448197  acc: 85.0000%(68/80)\n",
      "Batch[1780] - loss: 0.440985  acc: 81.0000%(104/128)\n",
      "Batch[1790] - loss: 0.457793  acc: 77.0000%(99/128)\n",
      "Batch[1800] - loss: 0.448497  acc: 78.0000%(63/80)\n",
      "\n",
      "Evaluation - loss: 0.570485  acc: 68.0000%(142/208) \n",
      "\n",
      "Batch[1810] - loss: 0.422481  acc: 79.0000%(102/128)\n",
      "Batch[1820] - loss: 0.442308  acc: 75.0000%(96/128)\n",
      "Batch[1830] - loss: 0.351693  acc: 85.0000%(68/80)\n",
      "Batch[1840] - loss: 0.460874  acc: 77.0000%(99/128)\n",
      "Batch[1850] - loss: 0.461416  acc: 79.0000%(102/128)\n",
      "\n",
      "Evaluation - loss: 0.573695  acc: 68.0000%(143/208) \n",
      "\n",
      "Batch[1860] - loss: 0.399741  acc: 83.0000%(67/80)\n",
      "Batch[1870] - loss: 0.463099  acc: 75.0000%(96/128)\n",
      "Batch[1880] - loss: 0.431034  acc: 79.0000%(102/128)\n",
      "Batch[1890] - loss: 0.421352  acc: 81.0000%(65/80)\n",
      "Batch[1900] - loss: 0.399146  acc: 82.0000%(106/128)\n",
      "\n",
      "Evaluation - loss: 0.570824  acc: 67.0000%(140/208) \n",
      "\n",
      "Batch[1910] - loss: 0.440187  acc: 79.0000%(102/128)\n",
      "Batch[1920] - loss: 0.448483  acc: 81.0000%(65/80)\n",
      "Batch[1930] - loss: 0.400681  acc: 84.0000%(108/128)\n",
      "Batch[1940] - loss: 0.440924  acc: 79.0000%(102/128)\n",
      "Batch[1950] - loss: 0.373425  acc: 83.0000%(67/80)\n",
      "\n",
      "Evaluation - loss: 0.583700  acc: 70.0000%(146/208) \n",
      "\n",
      "Batch[1960] - loss: 0.426095  acc: 83.0000%(107/128)\n",
      "Batch[1970] - loss: 0.462817  acc: 78.0000%(100/128)\n",
      "Batch[1980] - loss: 0.426635  acc: 78.0000%(63/80)\n",
      "Batch[1990] - loss: 0.423395  acc: 81.0000%(104/128)\n",
      "Batch[2000] - loss: 0.368649  acc: 84.0000%(108/128)\n",
      "\n",
      "Evaluation - loss: 0.572398  acc: 68.0000%(143/208) \n",
      "\n",
      "Batch[2010] - loss: 0.384668  acc: 85.0000%(68/80)\n",
      "Batch[2020] - loss: 0.484476  acc: 71.0000%(91/128)\n",
      "Batch[2030] - loss: 0.448290  acc: 75.0000%(97/128)\n",
      "Batch[2040] - loss: 0.426683  acc: 82.0000%(66/80)\n",
      "Batch[2050] - loss: 0.464877  acc: 80.0000%(103/128)\n",
      "\n",
      "Evaluation - loss: 0.577587  acc: 69.0000%(144/208) \n",
      "\n",
      "Batch[2060] - loss: 0.344421  acc: 82.0000%(106/128)\n",
      "Batch[2070] - loss: 0.531330  acc: 78.0000%(63/80)\n",
      "Batch[2080] - loss: 0.422194  acc: 80.0000%(103/128)\n",
      "Batch[2090] - loss: 0.530477  acc: 73.0000%(94/128)\n",
      "Batch[2100] - loss: 0.340678  acc: 85.0000%(68/80)\n",
      "\n",
      "Evaluation - loss: 0.577789  acc: 70.0000%(146/208) \n",
      "\n",
      "Batch[2110] - loss: 0.479220  acc: 76.0000%(98/128)\n",
      "Batch[2120] - loss: 0.386460  acc: 82.0000%(106/128)\n",
      "Batch[2130] - loss: 0.424339  acc: 82.0000%(66/80)\n",
      "Batch[2140] - loss: 0.396322  acc: 81.0000%(104/128)\n",
      "Batch[2150] - loss: 0.446220  acc: 75.0000%(97/128)\n",
      "\n",
      "Evaluation - loss: 0.571126  acc: 67.0000%(141/208) \n",
      "\n",
      "Batch[2160] - loss: 0.392261  acc: 83.0000%(67/80)\n",
      "Batch[2170] - loss: 0.460847  acc: 77.0000%(99/128)\n",
      "Batch[2180] - loss: 0.377082  acc: 84.0000%(108/128)\n",
      "Batch[2190] - loss: 0.309908  acc: 83.0000%(67/80)\n",
      "Batch[2200] - loss: 0.470560  acc: 76.0000%(98/128)\n",
      "\n",
      "Evaluation - loss: 0.575825  acc: 68.0000%(143/208) \n",
      "\n",
      "Batch[2210] - loss: 0.411429  acc: 82.0000%(106/128)\n",
      "Batch[2220] - loss: 0.409749  acc: 85.0000%(68/80)\n",
      "Batch[2230] - loss: 0.405788  acc: 82.0000%(106/128)\n",
      "Batch[2240] - loss: 0.377710  acc: 84.0000%(108/128)\n",
      "Batch[2250] - loss: 0.409547  acc: 82.0000%(66/80)\n",
      "\n",
      "Evaluation - loss: 0.579386  acc: 70.0000%(146/208) \n",
      "\n",
      "Batch[2260] - loss: 0.417477  acc: 78.0000%(101/128)\n",
      "Batch[2270] - loss: 0.430222  acc: 78.0000%(101/128)\n",
      "Batch[2280] - loss: 0.402385  acc: 80.0000%(64/80)\n",
      "Batch[2290] - loss: 0.415599  acc: 78.0000%(101/128)\n",
      "Batch[2300] - loss: 0.460313  acc: 79.0000%(102/128)\n",
      "\n",
      "Evaluation - loss: 0.575152  acc: 67.0000%(140/208) \n",
      "\n",
      "Batch[2310] - loss: 0.477022  acc: 78.0000%(63/80)\n",
      "Batch[2320] - loss: 0.445022  acc: 81.0000%(104/128)\n",
      "Batch[2330] - loss: 0.439723  acc: 78.0000%(101/128)\n",
      "Batch[2340] - loss: 0.533128  acc: 77.0000%(62/80)\n",
      "Batch[2350] - loss: 0.349893  acc: 84.0000%(108/128)\n",
      "\n",
      "Evaluation - loss: 0.574953  acc: 68.0000%(143/208) \n",
      "\n",
      "Batch[2360] - loss: 0.405447  acc: 80.0000%(103/128)\n",
      "Batch[2370] - loss: 0.390489  acc: 76.0000%(61/80)\n",
      "Batch[2380] - loss: 0.366856  acc: 87.0000%(112/128)\n",
      "Batch[2390] - loss: 0.426005  acc: 80.0000%(103/128)\n",
      "Batch[2400] - loss: 0.476297  acc: 72.0000%(58/80)\n",
      "\n",
      "Evaluation - loss: 0.573098  acc: 67.0000%(140/208) \n",
      "\n",
      "Batch[2410] - loss: 0.391115  acc: 82.0000%(106/128)\n",
      "Batch[2420] - loss: 0.496766  acc: 76.0000%(98/128)\n",
      "Batch[2430] - loss: 0.381653  acc: 85.0000%(68/80)\n",
      "Batch[2440] - loss: 0.344622  acc: 84.0000%(108/128)\n",
      "Batch[2450] - loss: 0.463897  acc: 79.0000%(102/128)\n",
      "\n",
      "Evaluation - loss: 0.574797  acc: 68.0000%(142/208) \n",
      "\n",
      "Batch[2460] - loss: 0.335039  acc: 87.0000%(70/80)\n",
      "Batch[2470] - loss: 0.421774  acc: 80.0000%(103/128)\n",
      "Batch[2480] - loss: 0.472622  acc: 78.0000%(100/128)\n",
      "Batch[2490] - loss: 0.416682  acc: 82.0000%(66/80)\n",
      "Batch[2500] - loss: 0.444335  acc: 80.0000%(103/128)\n",
      "\n",
      "Evaluation - loss: 0.573454  acc: 67.0000%(141/208) \n",
      "\n",
      "Batch[2510] - loss: 0.405314  acc: 83.0000%(107/128)\n",
      "Batch[2520] - loss: 0.482527  acc: 71.0000%(57/80)\n",
      "Batch[2530] - loss: 0.395491  acc: 82.0000%(106/128)\n",
      "Batch[2540] - loss: 0.353000  acc: 85.0000%(110/128)\n",
      "Batch[2550] - loss: 0.425674  acc: 81.0000%(65/80)\n",
      "\n",
      "Evaluation - loss: 0.585374  acc: 67.0000%(141/208) \n",
      "\n",
      "Batch[2560] - loss: 0.477213  acc: 80.0000%(103/128)\n",
      "Batch[2570] - loss: 0.388120  acc: 82.0000%(105/128)\n",
      "Batch[2580] - loss: 0.431519  acc: 80.0000%(64/80)\n",
      "Batch[2590] - loss: 0.428087  acc: 81.0000%(104/128)\n",
      "Batch[2600] - loss: 0.464873  acc: 76.0000%(98/128)\n",
      "\n",
      "Evaluation - loss: 0.583869  acc: 69.0000%(145/208) \n",
      "\n",
      "Batch[2610] - loss: 0.379727  acc: 81.0000%(65/80)\n",
      "Batch[2620] - loss: 0.406642  acc: 79.0000%(102/128)\n",
      "Batch[2630] - loss: 0.383771  acc: 81.0000%(104/128)\n",
      "Batch[2640] - loss: 0.647693  acc: 72.0000%(58/80)\n",
      "Batch[2650] - loss: 0.455225  acc: 82.0000%(105/128)\n",
      "\n",
      "Evaluation - loss: 0.588635  acc: 70.0000%(147/208) \n",
      "\n",
      "Batch[2660] - loss: 0.494223  acc: 78.0000%(100/128)\n",
      "Batch[2670] - loss: 0.396367  acc: 77.0000%(62/80)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch[2680] - loss: 0.386030  acc: 80.0000%(103/128)\n",
      "Batch[2690] - loss: 0.452538  acc: 79.0000%(102/128)\n",
      "Batch[2700] - loss: 0.444811  acc: 78.0000%(63/80)\n",
      "\n",
      "Evaluation - loss: 0.574396  acc: 68.0000%(143/208) \n",
      "\n",
      "Batch[2710] - loss: 0.535416  acc: 73.0000%(94/128)\n",
      "Batch[2720] - loss: 0.374715  acc: 83.0000%(107/128)\n",
      "Batch[2730] - loss: 0.371977  acc: 87.0000%(70/80)\n",
      "Batch[2740] - loss: 0.323704  acc: 86.0000%(111/128)\n",
      "Batch[2750] - loss: 0.408059  acc: 79.0000%(102/128)\n",
      "\n",
      "Evaluation - loss: 0.575059  acc: 67.0000%(141/208) \n",
      "\n",
      "Batch[2760] - loss: 0.437441  acc: 82.0000%(66/80)\n",
      "Batch[2770] - loss: 0.425893  acc: 82.0000%(106/128)\n",
      "Batch[2780] - loss: 0.447697  acc: 82.0000%(105/128)\n",
      "Batch[2790] - loss: 0.473466  acc: 81.0000%(65/80)\n",
      "Batch[2800] - loss: 0.366894  acc: 84.0000%(108/128)\n",
      "\n",
      "Evaluation - loss: 0.573347  acc: 68.0000%(142/208) \n",
      "\n",
      "Batch[2810] - loss: 0.458042  acc: 79.0000%(102/128)\n",
      "Batch[2820] - loss: 0.360308  acc: 83.0000%(67/80)\n",
      "Batch[2830] - loss: 0.364969  acc: 82.0000%(105/128)\n",
      "Batch[2840] - loss: 0.362256  acc: 83.0000%(107/128)\n",
      "Batch[2850] - loss: 0.396941  acc: 78.0000%(63/80)\n",
      "\n",
      "Evaluation - loss: 0.573934  acc: 68.0000%(142/208) \n",
      "\n",
      "Batch[2860] - loss: 0.345592  acc: 85.0000%(110/128)\n",
      "Batch[2870] - loss: 0.400387  acc: 82.0000%(105/128)\n",
      "Batch[2880] - loss: 0.398463  acc: 82.0000%(66/80)\n",
      "Batch[2890] - loss: 0.449586  acc: 78.0000%(100/128)\n",
      "Batch[2900] - loss: 0.415591  acc: 79.0000%(102/128)\n",
      "\n",
      "Evaluation - loss: 0.575873  acc: 67.0000%(141/208) \n",
      "\n",
      "Batch[2910] - loss: 0.376892  acc: 83.0000%(67/80)\n",
      "Batch[2920] - loss: 0.441348  acc: 81.0000%(104/128)\n",
      "Batch[2930] - loss: 0.376511  acc: 85.0000%(109/128)\n",
      "Batch[2940] - loss: 0.449213  acc: 80.0000%(64/80)\n",
      "Batch[2950] - loss: 0.450939  acc: 76.0000%(98/128)\n",
      "\n",
      "Evaluation - loss: 0.576394  acc: 67.0000%(140/208) \n",
      "\n",
      "Batch[2960] - loss: 0.416704  acc: 78.0000%(100/128)\n",
      "Batch[2970] - loss: 0.404915  acc: 81.0000%(65/80)\n",
      "Batch[2980] - loss: 0.389547  acc: 83.0000%(107/128)\n",
      "Batch[2990] - loss: 0.399434  acc: 82.0000%(106/128)\n",
      "Batch[3000] - loss: 0.434406  acc: 81.0000%(65/80)\n",
      "\n",
      "Evaluation - loss: 0.576118  acc: 68.0000%(142/208) \n",
      "\n",
      "Batch[3010] - loss: 0.372260  acc: 85.0000%(109/128)\n",
      "Batch[3020] - loss: 0.391996  acc: 84.0000%(108/128)\n",
      "Batch[3030] - loss: 0.358523  acc: 87.0000%(70/80)\n",
      "Batch[3040] - loss: 0.415362  acc: 82.0000%(105/128)\n",
      "Batch[3050] - loss: 0.398326  acc: 82.0000%(105/128)\n",
      "\n",
      "Evaluation - loss: 0.576148  acc: 67.0000%(141/208) \n",
      "\n",
      "Batch[3060] - loss: 0.457977  acc: 78.0000%(63/80)\n",
      "Batch[3070] - loss: 0.418290  acc: 80.0000%(103/128)\n",
      "Batch[3080] - loss: 0.364798  acc: 84.0000%(108/128)\n",
      "Batch[3090] - loss: 0.360531  acc: 81.0000%(65/80)\n",
      "Batch[3100] - loss: 0.450307  acc: 79.0000%(102/128)\n",
      "\n",
      "Evaluation - loss: 0.581729  acc: 68.0000%(143/208) \n",
      "\n",
      "Batch[3110] - loss: 0.458446  acc: 80.0000%(103/128)\n",
      "Batch[3120] - loss: 0.349703  acc: 78.0000%(63/80)\n",
      "Batch[3130] - loss: 0.454801  acc: 80.0000%(103/128)\n",
      "Batch[3140] - loss: 0.478588  acc: 78.0000%(100/128)\n",
      "Batch[3150] - loss: 0.417834  acc: 80.0000%(64/80)\n",
      "\n",
      "Evaluation - loss: 0.578063  acc: 67.0000%(141/208) \n",
      "\n",
      "Batch[3160] - loss: 0.383536  acc: 81.0000%(104/128)\n",
      "Batch[3170] - loss: 0.367602  acc: 82.0000%(106/128)\n",
      "Batch[3180] - loss: 0.397581  acc: 83.0000%(67/80)\n",
      "Batch[3190] - loss: 0.352834  acc: 83.0000%(107/128)\n",
      "Batch[3200] - loss: 0.387444  acc: 83.0000%(107/128)\n",
      "\n",
      "Evaluation - loss: 0.577321  acc: 67.0000%(141/208) \n",
      "\n",
      "Batch[3210] - loss: 0.488446  acc: 82.0000%(66/80)\n",
      "Batch[3220] - loss: 0.459204  acc: 76.0000%(98/128)\n",
      "Batch[3230] - loss: 0.354804  acc: 88.0000%(113/128)\n",
      "Batch[3240] - loss: 0.425884  acc: 83.0000%(67/80)\n",
      "Batch[3250] - loss: 0.453271  acc: 78.0000%(100/128)\n",
      "\n",
      "Evaluation - loss: 0.581448  acc: 68.0000%(143/208) \n",
      "\n",
      "Batch[3260] - loss: 0.363419  acc: 85.0000%(109/128)\n",
      "Batch[3270] - loss: 0.418317  acc: 78.0000%(63/80)\n",
      "Batch[3280] - loss: 0.352564  acc: 85.0000%(109/128)\n",
      "Batch[3290] - loss: 0.328053  acc: 85.0000%(109/128)\n",
      "Batch[3300] - loss: 0.375573  acc: 82.0000%(66/80)\n",
      "\n",
      "Evaluation - loss: 0.577800  acc: 67.0000%(141/208) \n",
      "\n",
      "Batch[3310] - loss: 0.348743  acc: 82.0000%(105/128)\n",
      "Batch[3320] - loss: 0.476006  acc: 76.0000%(98/128)\n",
      "Batch[3330] - loss: 0.336290  acc: 87.0000%(70/80)\n",
      "Batch[3340] - loss: 0.405256  acc: 83.0000%(107/128)\n",
      "Batch[3350] - loss: 0.420979  acc: 82.0000%(105/128)\n",
      "\n",
      "Evaluation - loss: 0.578828  acc: 68.0000%(142/208) \n",
      "\n",
      "Batch[3360] - loss: 0.448621  acc: 75.0000%(60/80)\n",
      "Batch[3370] - loss: 0.424730  acc: 79.0000%(102/128)\n",
      "Batch[3380] - loss: 0.383320  acc: 83.0000%(107/128)\n",
      "Batch[3390] - loss: 0.371242  acc: 82.0000%(66/80)\n",
      "Batch[3400] - loss: 0.379660  acc: 81.0000%(104/128)\n",
      "\n",
      "Evaluation - loss: 0.585493  acc: 69.0000%(144/208) \n",
      "\n",
      "Batch[3410] - loss: 0.394453  acc: 82.0000%(105/128)\n",
      "Batch[3420] - loss: 0.366444  acc: 85.0000%(68/80)\n",
      "Batch[3430] - loss: 0.385659  acc: 83.0000%(107/128)\n",
      "Batch[3440] - loss: 0.380847  acc: 83.0000%(107/128)\n",
      "Batch[3450] - loss: 0.419457  acc: 83.0000%(67/80)\n",
      "\n",
      "Evaluation - loss: 0.585272  acc: 67.0000%(141/208) \n",
      "\n",
      "Batch[3460] - loss: 0.365960  acc: 80.0000%(103/128)\n",
      "Batch[3470] - loss: 0.353640  acc: 83.0000%(107/128)\n",
      "Batch[3480] - loss: 0.364970  acc: 85.0000%(68/80)\n",
      "Batch[3490] - loss: 0.395300  acc: 83.0000%(107/128)\n",
      "Batch[3500] - loss: 0.368652  acc: 82.0000%(106/128)\n",
      "\n",
      "Evaluation - loss: 0.583758  acc: 66.0000%(139/208) \n",
      "\n",
      "Batch[3510] - loss: 0.419219  acc: 78.0000%(63/80)\n",
      "Batch[3520] - loss: 0.390460  acc: 82.0000%(106/128)\n",
      "Batch[3530] - loss: 0.394451  acc: 82.0000%(106/128)\n",
      "Batch[3540] - loss: 0.368623  acc: 80.0000%(64/80)\n",
      "Batch[3550] - loss: 0.412601  acc: 79.0000%(102/128)\n",
      "\n",
      "Evaluation - loss: 0.584558  acc: 67.0000%(140/208) \n",
      "\n",
      "Batch[3560] - loss: 0.463760  acc: 77.0000%(99/128)\n",
      "Batch[3570] - loss: 0.403866  acc: 86.0000%(69/80)\n",
      "Batch[3580] - loss: 0.438284  acc: 78.0000%(100/128)\n",
      "Batch[3590] - loss: 0.389256  acc: 82.0000%(105/128)\n",
      "Batch[3600] - loss: 0.471765  acc: 72.0000%(58/80)\n",
      "\n",
      "Evaluation - loss: 0.586322  acc: 68.0000%(143/208) \n",
      "\n",
      "Batch[3610] - loss: 0.396839  acc: 81.0000%(104/128)\n",
      "Batch[3620] - loss: 0.359191  acc: 86.0000%(111/128)\n",
      "Batch[3630] - loss: 0.398996  acc: 83.0000%(67/80)\n",
      "Batch[3640] - loss: 0.472278  acc: 78.0000%(100/128)\n",
      "Batch[3650] - loss: 0.475848  acc: 75.0000%(97/128)\n",
      "\n",
      "Evaluation - loss: 0.588419  acc: 70.0000%(146/208) \n",
      "\n",
      "Batch[3660] - loss: 0.447983  acc: 75.0000%(60/80)\n",
      "Batch[3670] - loss: 0.362121  acc: 85.0000%(110/128)\n",
      "Batch[3680] - loss: 0.345547  acc: 85.0000%(109/128)\n",
      "Batch[3690] - loss: 0.484886  acc: 78.0000%(63/80)\n",
      "Batch[3700] - loss: 0.417687  acc: 80.0000%(103/128)\n",
      "\n",
      "Evaluation - loss: 0.581804  acc: 68.0000%(142/208) \n",
      "\n",
      "Batch[3710] - loss: 0.375851  acc: 85.0000%(110/128)\n",
      "Batch[3720] - loss: 0.386530  acc: 86.0000%(69/80)\n",
      "Batch[3730] - loss: 0.368836  acc: 84.0000%(108/128)\n",
      "Batch[3740] - loss: 0.346208  acc: 86.0000%(111/128)\n",
      "Batch[3750] - loss: 0.473603  acc: 75.0000%(60/80)\n",
      "\n",
      "Evaluation - loss: 0.587055  acc: 67.0000%(140/208) \n",
      "\n",
      "Batch[3760] - loss: 0.432891  acc: 85.0000%(109/128)\n",
      "Batch[3770] - loss: 0.454230  acc: 73.0000%(94/128)\n",
      "Batch[3780] - loss: 0.365751  acc: 85.0000%(68/80)\n",
      "Batch[3790] - loss: 0.386114  acc: 82.0000%(106/128)\n",
      "Batch[3800] - loss: 0.388107  acc: 82.0000%(106/128)\n",
      "\n",
      "Evaluation - loss: 0.585013  acc: 67.0000%(141/208) \n",
      "\n",
      "Batch[3810] - loss: 0.317121  acc: 86.0000%(69/80)\n",
      "Batch[3820] - loss: 0.441427  acc: 77.0000%(99/128)\n",
      "Batch[3830] - loss: 0.431863  acc: 77.0000%(99/128)\n",
      "Batch[3840] - loss: 0.346627  acc: 83.0000%(67/80)\n"
     ]
    }
   ],
   "source": [
    "def build_tensor(x, y):\n",
    "    label_t = torch.tensor([train_dataset.get_label_id(l) for l in y], dtype=torch.long)\n",
    "    \n",
    "    x_length = max([len(a) for a in x])\n",
    "    x_temp = []\n",
    "    for poem in x:\n",
    "        t = torch.zeros([x_length, N_ALPHABET])\n",
    "        for i,a in enumerate(poem):\n",
    "            t[i][train_dataset.get_alphabet_id(a)] = 1\n",
    "        x_temp.append(t)\n",
    "    x_temp = [a.view(1, x_length, N_ALPHABET) for a in x_temp]\n",
    "    return torch.cat(x_temp, 0), label_t \n",
    "\n",
    "try:\n",
    "    train(model, train_loader, test_loader, args)\n",
    "except KeyboardInterrupt:\n",
    "    print('\\n' + '-' * 89)\n",
    "    print('Exiting from training early')\n",
    "    eval(test_loader, model, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
