{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "class Mydataset(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        if train:\n",
    "            print('loading trainig dataset')\n",
    "            file = 'train_poetry.csv'\n",
    "        else:\n",
    "            print('loading testing dataset')\n",
    "            file = 'test_poetry.csv'\n",
    "        with open(file, 'r', newline='', encoding='utf-8') as myFile:  \n",
    "            rdr = csv.reader(myFile)\n",
    "            temp = list(rdr)\n",
    "            self.x = [a[0][:200] for a in temp]\n",
    "            self.y = [a[1] for a in temp]\n",
    "        self.len = len(self.x)\n",
    "        self.labels = list(sorted(set(self.y)))\n",
    "        self.alphabet = list(sorted(set(['s','w','\\t'])))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "        \n",
    "    def get_labels(self):\n",
    "        return self.labels\n",
    "    \n",
    "    def get_alphabet(self):\n",
    "        return self.alphabet\n",
    "    \n",
    "    def get_alphabet_id(self, c):\n",
    "        return self.alphabet.index(c)\n",
    "    \n",
    "    def get_label(self, id):\n",
    "        return self.labels[id]\n",
    "    \n",
    "    def get_label_id(self, label):\n",
    "        return self.labels.index(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading trainig dataset\n",
      "loading testing dataset\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Mydataset()\n",
    "test_dataset = Mydataset(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "N_LABELS = len(train_dataset.get_labels())\n",
    "N_ALPHABET = len(train_dataset.get_alphabet())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    pass\n",
    "\n",
    "args = args()\n",
    "\n",
    "args.class_num = N_LABELS\n",
    "args.kernel_num = 100\n",
    "args.kernel_sizes = [2,3,4,5]\n",
    "args.dropout = 0.5\n",
    "args.static = True\n",
    "args.lr = 0.001\n",
    "args.epochs = 256\n",
    "args.embeding_num = N_ALPHABET\n",
    "args.embeding_dim = N_ALPHABET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_Text(\n",
       "  (embed): Embedding(3, 3)\n",
       "  (convs1): ModuleList(\n",
       "    (0): Conv2d(1, 100, kernel_size=(2, 3), stride=(1, 1))\n",
       "    (1): Conv2d(1, 100, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (2): Conv2d(1, 100, kernel_size=(4, 3), stride=(1, 1))\n",
       "    (3): Conv2d(1, 100, kernel_size=(5, 3), stride=(1, 1))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (fc1): Linear(in_features=400, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class CNN_Text(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(CNN_Text, self).__init__()\n",
    "        self.args = args\n",
    "        \n",
    "        C = args.class_num\n",
    "        Ci = 1\n",
    "        Co = args.kernel_num\n",
    "        Ks = args.kernel_sizes\n",
    "        V = args.embeding_num\n",
    "        D = args.embeding_dim\n",
    "        \n",
    "        self.embed = nn.Embedding(V, D)\n",
    "\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
    "\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "\n",
    "        self.fc1 = nn.Linear(len(Ks)*Co, C)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.args.static:\n",
    "            x = Variable(x)\n",
    "\n",
    "        x = x.unsqueeze(1)  # (N, Ci, W, D)\n",
    "\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n",
    "\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
    "\n",
    "        x = torch.cat(x, 1)\n",
    "\n",
    "        x = self.dropout(x)  # (N, len(Ks)*Co)\n",
    "\n",
    "        logit = self.fc1(x)  # (N, C)\n",
    "        return logit\n",
    "model = CNN_Text(args)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.log_interval = 10\n",
    "args.test_interval = 50\n",
    "\n",
    "def train(model, train_loader, test_loader, args):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    model.train()\n",
    "    \n",
    "    steps = 0\n",
    "    \n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        batch = 0\n",
    "        for x, y in train_loader:\n",
    "            x_t, y_t = build_tensor(x, y)\n",
    "            optimizer.zero_grad()\n",
    "            logit = model(x_t)\n",
    "            loss = F.cross_entropy(logit, y_t)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            steps += 1\n",
    "            if steps % args.log_interval == 0:\n",
    "                corrects = (torch.max(logit, 1)[1].view(y_t.size()).data == y_t.data).sum()\n",
    "                accuracy = 100.0 * corrects/y_t.shape[0]\n",
    "                print(\n",
    "                    '\\rBatch[{}] - loss: {:.6f}  acc: {:.4f}%({}/{})'.format(steps,\n",
    "                                                                             loss.data[0], \n",
    "                                                                             accuracy,\n",
    "                                                                             corrects,\n",
    "                                                                             y_t.shape[0]))\n",
    "            if steps % args.test_interval == 0:\n",
    "                acc = eval(test_loader, model, args)\n",
    "                \n",
    "def eval(test_loader, model, args):\n",
    "    model.eval()\n",
    "    corrects, avg_loss = 0, 0\n",
    "    for x, y in test_loader:\n",
    "        x_t, y_t = build_tensor(x, y)\n",
    "        logit = model(x_t)\n",
    "        loss = F.cross_entropy(logit, y_t, size_average=False)\n",
    "        avg_loss += loss.data[0]\n",
    "        corrects += (torch.max(logit, 1)[1].view(y_t.size()).data == y_t.data).sum()\n",
    "    size = len(test_loader.dataset)\n",
    "    avg_loss /= size\n",
    "    accuracy = 100.0 * corrects/size\n",
    "    print('\\nEvaluation - loss: {:.6f}  acc: {:.4f}%({}/{}) \\n'.format(avg_loss, \n",
    "                                                                       accuracy, \n",
    "                                                                       corrects, \n",
    "                                                                       size))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\settings\\anaconda\\lib\\site-packages\\ipykernel\\__main__.py:25: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch[10] - loss: 0.703199  acc: 56.0000%(72/128)\n",
      "Batch[20] - loss: 0.697434  acc: 53.0000%(68/128)\n",
      "Batch[30] - loss: 0.695099  acc: 53.0000%(43/80)\n",
      "Batch[40] - loss: 0.690557  acc: 56.0000%(72/128)\n",
      "Batch[50] - loss: 0.659776  acc: 59.0000%(76/128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\settings\\anaconda\\lib\\site-packages\\ipykernel\\__main__.py:39: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation - loss: 0.655568  acc: 68.0000%(143/208) \n",
      "\n",
      "Batch[60] - loss: 0.636766  acc: 73.0000%(59/80)\n",
      "Batch[70] - loss: 0.656228  acc: 60.0000%(77/128)\n",
      "Batch[80] - loss: 0.613737  acc: 78.0000%(100/128)\n",
      "Batch[90] - loss: 0.626658  acc: 73.0000%(59/80)\n",
      "Batch[100] - loss: 0.624308  acc: 67.0000%(86/128)\n",
      "\n",
      "Evaluation - loss: 0.608310  acc: 74.0000%(154/208) \n",
      "\n",
      "Batch[110] - loss: 0.641470  acc: 60.0000%(78/128)\n",
      "Batch[120] - loss: 0.609769  acc: 72.0000%(58/80)\n",
      "Batch[130] - loss: 0.573395  acc: 78.0000%(100/128)\n",
      "Batch[140] - loss: 0.570076  acc: 76.0000%(98/128)\n",
      "Batch[150] - loss: 0.591563  acc: 73.0000%(59/80)\n",
      "\n",
      "Evaluation - loss: 0.567704  acc: 73.0000%(153/208) \n",
      "\n",
      "Batch[160] - loss: 0.544241  acc: 76.0000%(98/128)\n",
      "Batch[170] - loss: 0.601905  acc: 65.0000%(84/128)\n",
      "Batch[180] - loss: 0.609185  acc: 68.0000%(55/80)\n",
      "Batch[190] - loss: 0.584258  acc: 66.0000%(85/128)\n",
      "Batch[200] - loss: 0.532224  acc: 78.0000%(101/128)\n",
      "\n",
      "Evaluation - loss: 0.535509  acc: 73.0000%(152/208) \n",
      "\n",
      "Batch[210] - loss: 0.593572  acc: 66.0000%(53/80)\n",
      "Batch[220] - loss: 0.515687  acc: 79.0000%(102/128)\n",
      "Batch[230] - loss: 0.501293  acc: 77.0000%(99/128)\n",
      "Batch[240] - loss: 0.504171  acc: 75.0000%(60/80)\n",
      "Batch[250] - loss: 0.508758  acc: 78.0000%(101/128)\n",
      "\n",
      "Evaluation - loss: 0.515902  acc: 75.0000%(158/208) \n",
      "\n",
      "Batch[260] - loss: 0.532263  acc: 76.0000%(98/128)\n",
      "Batch[270] - loss: 0.535150  acc: 70.0000%(56/80)\n",
      "Batch[280] - loss: 0.561151  acc: 73.0000%(94/128)\n",
      "Batch[290] - loss: 0.498651  acc: 74.0000%(95/128)\n",
      "Batch[300] - loss: 0.447601  acc: 80.0000%(64/80)\n",
      "\n",
      "Evaluation - loss: 0.501879  acc: 76.0000%(159/208) \n",
      "\n",
      "Batch[310] - loss: 0.538780  acc: 76.0000%(98/128)\n",
      "Batch[320] - loss: 0.517689  acc: 72.0000%(93/128)\n",
      "Batch[330] - loss: 0.444481  acc: 81.0000%(65/80)\n",
      "Batch[340] - loss: 0.502983  acc: 78.0000%(101/128)\n",
      "Batch[350] - loss: 0.487944  acc: 74.0000%(95/128)\n",
      "\n",
      "Evaluation - loss: 0.491644  acc: 76.0000%(160/208) \n",
      "\n",
      "Batch[360] - loss: 0.574535  acc: 71.0000%(57/80)\n",
      "Batch[370] - loss: 0.533781  acc: 71.0000%(92/128)\n",
      "Batch[380] - loss: 0.510931  acc: 75.0000%(97/128)\n",
      "Batch[390] - loss: 0.538472  acc: 68.0000%(55/80)\n",
      "Batch[400] - loss: 0.548538  acc: 71.0000%(91/128)\n",
      "\n",
      "Evaluation - loss: 0.486129  acc: 75.0000%(158/208) \n",
      "\n",
      "Batch[410] - loss: 0.492321  acc: 76.0000%(98/128)\n",
      "Batch[420] - loss: 0.478106  acc: 75.0000%(60/80)\n",
      "Batch[430] - loss: 0.496512  acc: 75.0000%(96/128)\n",
      "Batch[440] - loss: 0.523424  acc: 75.0000%(96/128)\n",
      "Batch[450] - loss: 0.426302  acc: 81.0000%(65/80)\n",
      "\n",
      "Evaluation - loss: 0.485836  acc: 74.0000%(155/208) \n",
      "\n",
      "Batch[460] - loss: 0.495963  acc: 75.0000%(96/128)\n",
      "Batch[470] - loss: 0.515328  acc: 77.0000%(99/128)\n",
      "Batch[480] - loss: 0.479791  acc: 77.0000%(62/80)\n",
      "Batch[490] - loss: 0.490184  acc: 76.0000%(98/128)\n",
      "Batch[500] - loss: 0.451844  acc: 79.0000%(102/128)\n",
      "\n",
      "Evaluation - loss: 0.476325  acc: 75.0000%(157/208) \n",
      "\n",
      "Batch[510] - loss: 0.480525  acc: 75.0000%(60/80)\n",
      "Batch[520] - loss: 0.501247  acc: 74.0000%(95/128)\n",
      "Batch[530] - loss: 0.503353  acc: 76.0000%(98/128)\n",
      "Batch[540] - loss: 0.527587  acc: 73.0000%(59/80)\n",
      "Batch[550] - loss: 0.538068  acc: 70.0000%(90/128)\n",
      "\n",
      "Evaluation - loss: 0.470755  acc: 78.0000%(163/208) \n",
      "\n",
      "Batch[560] - loss: 0.485296  acc: 75.0000%(96/128)\n",
      "Batch[570] - loss: 0.463379  acc: 76.0000%(61/80)\n",
      "Batch[580] - loss: 0.496231  acc: 75.0000%(97/128)\n",
      "Batch[590] - loss: 0.482292  acc: 73.0000%(94/128)\n",
      "Batch[600] - loss: 0.458641  acc: 78.0000%(63/80)\n",
      "\n",
      "Evaluation - loss: 0.469081  acc: 75.0000%(158/208) \n",
      "\n",
      "Batch[610] - loss: 0.545130  acc: 72.0000%(93/128)\n",
      "Batch[620] - loss: 0.503554  acc: 76.0000%(98/128)\n",
      "Batch[630] - loss: 0.421673  acc: 78.0000%(63/80)\n",
      "Batch[640] - loss: 0.545222  acc: 72.0000%(93/128)\n",
      "Batch[650] - loss: 0.437414  acc: 79.0000%(102/128)\n",
      "\n",
      "Evaluation - loss: 0.473435  acc: 75.0000%(158/208) \n",
      "\n",
      "Batch[660] - loss: 0.490147  acc: 80.0000%(64/80)\n",
      "Batch[670] - loss: 0.405030  acc: 82.0000%(105/128)\n",
      "Batch[680] - loss: 0.513288  acc: 75.0000%(96/128)\n",
      "Batch[690] - loss: 0.471723  acc: 71.0000%(57/80)\n",
      "Batch[700] - loss: 0.475991  acc: 79.0000%(102/128)\n",
      "\n",
      "Evaluation - loss: 0.467491  acc: 77.0000%(161/208) \n",
      "\n",
      "Batch[710] - loss: 0.405359  acc: 81.0000%(104/128)\n",
      "Batch[720] - loss: 0.466370  acc: 76.0000%(61/80)\n",
      "Batch[730] - loss: 0.506078  acc: 73.0000%(94/128)\n",
      "Batch[740] - loss: 0.398850  acc: 83.0000%(107/128)\n",
      "Batch[750] - loss: 0.499375  acc: 73.0000%(59/80)\n",
      "\n",
      "Evaluation - loss: 0.465476  acc: 75.0000%(158/208) \n",
      "\n",
      "Batch[760] - loss: 0.443220  acc: 76.0000%(98/128)\n",
      "Batch[770] - loss: 0.524525  acc: 74.0000%(95/128)\n",
      "Batch[780] - loss: 0.435526  acc: 76.0000%(61/80)\n",
      "Batch[790] - loss: 0.409791  acc: 81.0000%(104/128)\n",
      "Batch[800] - loss: 0.519755  acc: 72.0000%(93/128)\n",
      "\n",
      "Evaluation - loss: 0.465052  acc: 76.0000%(159/208) \n",
      "\n",
      "Batch[810] - loss: 0.418971  acc: 85.0000%(68/80)\n",
      "Batch[820] - loss: 0.361703  acc: 91.0000%(117/128)\n",
      "Batch[830] - loss: 0.363196  acc: 85.0000%(109/128)\n",
      "Batch[840] - loss: 0.578611  acc: 70.0000%(56/80)\n",
      "Batch[850] - loss: 0.477107  acc: 80.0000%(103/128)\n",
      "\n",
      "Evaluation - loss: 0.464341  acc: 75.0000%(157/208) \n",
      "\n",
      "Batch[860] - loss: 0.426313  acc: 79.0000%(102/128)\n",
      "Batch[870] - loss: 0.519555  acc: 68.0000%(55/80)\n",
      "Batch[880] - loss: 0.511635  acc: 76.0000%(98/128)\n",
      "Batch[890] - loss: 0.499957  acc: 76.0000%(98/128)\n",
      "Batch[900] - loss: 0.388855  acc: 83.0000%(67/80)\n",
      "\n",
      "Evaluation - loss: 0.481438  acc: 75.0000%(157/208) \n",
      "\n",
      "Batch[910] - loss: 0.467324  acc: 78.0000%(100/128)\n",
      "Batch[920] - loss: 0.441575  acc: 79.0000%(102/128)\n",
      "Batch[930] - loss: 0.419441  acc: 78.0000%(63/80)\n",
      "Batch[940] - loss: 0.436342  acc: 77.0000%(99/128)\n",
      "Batch[950] - loss: 0.474720  acc: 82.0000%(105/128)\n",
      "\n",
      "Evaluation - loss: 0.464679  acc: 77.0000%(161/208) \n",
      "\n",
      "Batch[960] - loss: 0.356948  acc: 83.0000%(67/80)\n",
      "Batch[970] - loss: 0.538889  acc: 73.0000%(94/128)\n",
      "Batch[980] - loss: 0.375970  acc: 85.0000%(109/128)\n",
      "Batch[990] - loss: 0.484781  acc: 72.0000%(58/80)\n",
      "Batch[1000] - loss: 0.465017  acc: 75.0000%(97/128)\n",
      "\n",
      "Evaluation - loss: 0.469087  acc: 76.0000%(160/208) \n",
      "\n",
      "Batch[1010] - loss: 0.505897  acc: 75.0000%(97/128)\n",
      "Batch[1020] - loss: 0.397148  acc: 82.0000%(66/80)\n",
      "Batch[1030] - loss: 0.436004  acc: 78.0000%(101/128)\n",
      "Batch[1040] - loss: 0.468330  acc: 78.0000%(101/128)\n",
      "Batch[1050] - loss: 0.410656  acc: 85.0000%(68/80)\n",
      "\n",
      "Evaluation - loss: 0.475570  acc: 74.0000%(155/208) \n",
      "\n",
      "Batch[1060] - loss: 0.442886  acc: 82.0000%(106/128)\n",
      "Batch[1070] - loss: 0.424422  acc: 79.0000%(102/128)\n",
      "Batch[1080] - loss: 0.421008  acc: 78.0000%(63/80)\n",
      "Batch[1090] - loss: 0.544591  acc: 71.0000%(91/128)\n",
      "Batch[1100] - loss: 0.367573  acc: 85.0000%(110/128)\n",
      "\n",
      "Evaluation - loss: 0.475096  acc: 74.0000%(155/208) \n",
      "\n",
      "Batch[1110] - loss: 0.427693  acc: 81.0000%(65/80)\n",
      "Batch[1120] - loss: 0.464083  acc: 80.0000%(103/128)\n",
      "Batch[1130] - loss: 0.410632  acc: 81.0000%(104/128)\n",
      "Batch[1140] - loss: 0.448208  acc: 78.0000%(63/80)\n",
      "Batch[1150] - loss: 0.473713  acc: 78.0000%(100/128)\n",
      "\n",
      "Evaluation - loss: 0.472250  acc: 75.0000%(156/208) \n",
      "\n",
      "Batch[1160] - loss: 0.506884  acc: 75.0000%(97/128)\n",
      "Batch[1170] - loss: 0.422081  acc: 83.0000%(67/80)\n",
      "Batch[1180] - loss: 0.393781  acc: 82.0000%(106/128)\n",
      "Batch[1190] - loss: 0.424554  acc: 83.0000%(107/128)\n",
      "Batch[1200] - loss: 0.440500  acc: 77.0000%(62/80)\n",
      "\n",
      "Evaluation - loss: 0.467462  acc: 74.0000%(155/208) \n",
      "\n",
      "Batch[1210] - loss: 0.479018  acc: 76.0000%(98/128)\n",
      "Batch[1220] - loss: 0.448343  acc: 76.0000%(98/128)\n",
      "Batch[1230] - loss: 0.407022  acc: 83.0000%(67/80)\n",
      "Batch[1240] - loss: 0.512420  acc: 74.0000%(95/128)\n",
      "Batch[1250] - loss: 0.473258  acc: 75.0000%(97/128)\n",
      "\n",
      "Evaluation - loss: 0.468924  acc: 75.0000%(156/208) \n",
      "\n",
      "Batch[1260] - loss: 0.474772  acc: 80.0000%(64/80)\n",
      "Batch[1270] - loss: 0.456889  acc: 75.0000%(96/128)\n",
      "Batch[1280] - loss: 0.468184  acc: 75.0000%(97/128)\n",
      "Batch[1290] - loss: 0.400980  acc: 85.0000%(68/80)\n",
      "Batch[1300] - loss: 0.425327  acc: 79.0000%(102/128)\n",
      "\n",
      "Evaluation - loss: 0.473475  acc: 74.0000%(154/208) \n",
      "\n",
      "Batch[1310] - loss: 0.437475  acc: 82.0000%(105/128)\n",
      "Batch[1320] - loss: 0.398302  acc: 80.0000%(64/80)\n",
      "Batch[1330] - loss: 0.469983  acc: 75.0000%(97/128)\n",
      "Batch[1340] - loss: 0.482149  acc: 75.0000%(96/128)\n",
      "Batch[1350] - loss: 0.426861  acc: 80.0000%(64/80)\n",
      "\n",
      "Evaluation - loss: 0.465699  acc: 75.0000%(158/208) \n",
      "\n",
      "Batch[1360] - loss: 0.463475  acc: 71.0000%(92/128)\n",
      "Batch[1370] - loss: 0.482946  acc: 75.0000%(97/128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch[1380] - loss: 0.465286  acc: 76.0000%(61/80)\n",
      "Batch[1390] - loss: 0.414584  acc: 80.0000%(103/128)\n",
      "Batch[1400] - loss: 0.450715  acc: 80.0000%(103/128)\n",
      "\n",
      "Evaluation - loss: 0.469546  acc: 75.0000%(156/208) \n",
      "\n",
      "Batch[1410] - loss: 0.421013  acc: 80.0000%(64/80)\n",
      "Batch[1420] - loss: 0.424568  acc: 78.0000%(101/128)\n",
      "Batch[1430] - loss: 0.444514  acc: 78.0000%(101/128)\n",
      "Batch[1440] - loss: 0.473696  acc: 76.0000%(61/80)\n",
      "Batch[1450] - loss: 0.412411  acc: 82.0000%(106/128)\n",
      "\n",
      "Evaluation - loss: 0.465991  acc: 75.0000%(157/208) \n",
      "\n",
      "Batch[1460] - loss: 0.381679  acc: 85.0000%(109/128)\n",
      "Batch[1470] - loss: 0.442635  acc: 81.0000%(65/80)\n",
      "Batch[1480] - loss: 0.491628  acc: 81.0000%(104/128)\n",
      "Batch[1490] - loss: 0.431640  acc: 78.0000%(101/128)\n",
      "Batch[1500] - loss: 0.364067  acc: 85.0000%(68/80)\n",
      "\n",
      "Evaluation - loss: 0.466708  acc: 75.0000%(157/208) \n",
      "\n",
      "Batch[1510] - loss: 0.426157  acc: 80.0000%(103/128)\n",
      "Batch[1520] - loss: 0.465911  acc: 79.0000%(102/128)\n",
      "Batch[1530] - loss: 0.410881  acc: 83.0000%(67/80)\n",
      "Batch[1540] - loss: 0.437270  acc: 78.0000%(100/128)\n",
      "Batch[1550] - loss: 0.460003  acc: 77.0000%(99/128)\n",
      "\n",
      "Evaluation - loss: 0.472687  acc: 75.0000%(157/208) \n",
      "\n",
      "Batch[1560] - loss: 0.392027  acc: 85.0000%(68/80)\n",
      "Batch[1570] - loss: 0.401941  acc: 85.0000%(110/128)\n",
      "Batch[1580] - loss: 0.432522  acc: 80.0000%(103/128)\n",
      "Batch[1590] - loss: 0.369961  acc: 83.0000%(67/80)\n",
      "Batch[1600] - loss: 0.386596  acc: 84.0000%(108/128)\n",
      "\n",
      "Evaluation - loss: 0.468359  acc: 75.0000%(158/208) \n",
      "\n",
      "Batch[1610] - loss: 0.449192  acc: 77.0000%(99/128)\n",
      "Batch[1620] - loss: 0.534696  acc: 72.0000%(58/80)\n",
      "Batch[1630] - loss: 0.432951  acc: 78.0000%(100/128)\n",
      "Batch[1640] - loss: 0.362096  acc: 86.0000%(111/128)\n",
      "Batch[1650] - loss: 0.473714  acc: 78.0000%(63/80)\n",
      "\n",
      "Evaluation - loss: 0.473503  acc: 75.0000%(157/208) \n",
      "\n",
      "Batch[1660] - loss: 0.425109  acc: 77.0000%(99/128)\n",
      "Batch[1670] - loss: 0.371560  acc: 83.0000%(107/128)\n",
      "Batch[1680] - loss: 0.418945  acc: 83.0000%(67/80)\n",
      "Batch[1690] - loss: 0.472807  acc: 76.0000%(98/128)\n",
      "Batch[1700] - loss: 0.477622  acc: 76.0000%(98/128)\n",
      "\n",
      "Evaluation - loss: 0.470643  acc: 75.0000%(158/208) \n",
      "\n",
      "Batch[1710] - loss: 0.342861  acc: 85.0000%(68/80)\n",
      "Batch[1720] - loss: 0.396398  acc: 82.0000%(105/128)\n",
      "Batch[1730] - loss: 0.446761  acc: 78.0000%(100/128)\n",
      "Batch[1740] - loss: 0.358874  acc: 86.0000%(69/80)\n",
      "Batch[1750] - loss: 0.425517  acc: 79.0000%(102/128)\n",
      "\n",
      "Evaluation - loss: 0.467911  acc: 75.0000%(157/208) \n",
      "\n",
      "Batch[1760] - loss: 0.465965  acc: 75.0000%(96/128)\n",
      "Batch[1770] - loss: 0.415271  acc: 83.0000%(67/80)\n",
      "Batch[1780] - loss: 0.425620  acc: 78.0000%(101/128)\n",
      "Batch[1790] - loss: 0.430331  acc: 78.0000%(100/128)\n",
      "Batch[1800] - loss: 0.434414  acc: 81.0000%(65/80)\n",
      "\n",
      "Evaluation - loss: 0.470316  acc: 76.0000%(159/208) \n",
      "\n",
      "Batch[1810] - loss: 0.435648  acc: 78.0000%(101/128)\n",
      "Batch[1820] - loss: 0.430134  acc: 81.0000%(104/128)\n",
      "Batch[1830] - loss: 0.413873  acc: 77.0000%(62/80)\n",
      "Batch[1840] - loss: 0.480397  acc: 78.0000%(100/128)\n",
      "Batch[1850] - loss: 0.472793  acc: 79.0000%(102/128)\n",
      "\n",
      "Evaluation - loss: 0.476823  acc: 75.0000%(157/208) \n",
      "\n",
      "Batch[1860] - loss: 0.467314  acc: 72.0000%(58/80)\n",
      "Batch[1870] - loss: 0.438372  acc: 79.0000%(102/128)\n",
      "Batch[1880] - loss: 0.442879  acc: 79.0000%(102/128)\n",
      "Batch[1890] - loss: 0.383678  acc: 83.0000%(67/80)\n",
      "Batch[1900] - loss: 0.388539  acc: 84.0000%(108/128)\n",
      "\n",
      "Evaluation - loss: 0.475449  acc: 75.0000%(157/208) \n",
      "\n",
      "Batch[1910] - loss: 0.374847  acc: 85.0000%(109/128)\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "Exiting from training early\n",
      "\n",
      "Evaluation - loss: 0.474416  acc: 75.0000%(157/208) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def build_tensor(x, y):\n",
    "    label_t = torch.tensor([train_dataset.get_label_id(l) for l in y], dtype=torch.long)\n",
    "    \n",
    "    x_length = max([len(a) for a in x])\n",
    "    x_temp = []\n",
    "    for poem in x:\n",
    "        t = torch.zeros([x_length, N_ALPHABET])\n",
    "        for i,a in enumerate(poem):\n",
    "            t[i][train_dataset.get_alphabet_id(a)] = 1\n",
    "        x_temp.append(t)\n",
    "    x_temp = [a.view(1, x_length, N_ALPHABET) for a in x_temp]\n",
    "    return torch.cat(x_temp, 0), label_t \n",
    "\n",
    "try:\n",
    "    train(model, train_loader, test_loader, args)\n",
    "except KeyboardInterrupt:\n",
    "    print('\\n' + '-' * 89)\n",
    "    print('Exiting from training early')\n",
    "    eval(test_loader, model, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
