{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 100\n",
    "N_LAYERS = 2\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "class Mydataset(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        if train:\n",
    "            print('loading trainig dataset')\n",
    "            file = 'train_poetry.csv'\n",
    "        else:\n",
    "            print('loading testing dataset')\n",
    "            file = 'test_poetry.csv'\n",
    "        with open(file, 'r', newline='', encoding='utf-8') as myFile:  \n",
    "            rdr = csv.reader(myFile)\n",
    "            temp = list(rdr)\n",
    "            self.x = [a[0][0:200] for a in temp]\n",
    "            self.y = [a[1] for a in temp]\n",
    "        self.len = len(self.x)\n",
    "        self.labels = list(sorted(set(self.y)))\n",
    "        self.alphabet = list(sorted(set(['s','w','\\t'])))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "        \n",
    "    def get_labels(self):\n",
    "        return self.labels\n",
    "    \n",
    "    def get_alphabet(self):\n",
    "        return self.alphabet\n",
    "    \n",
    "    def get_alphabet_id(self, c):\n",
    "        return self.alphabet.index(c)\n",
    "    \n",
    "    def get_label(self, id):\n",
    "        return self.labels[id]\n",
    "    \n",
    "    def get_label_id(self, label):\n",
    "        return self.labels.index(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading trainig dataset\n",
      "loading testing dataset\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Mydataset()\n",
    "test_dataset = Mydataset(train=False)\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alphabet size: 3\n",
      "class size: 2\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "ALL_LETTERS = train_dataset.get_alphabet()\n",
    "N_CHARS = len(ALL_LETTERS)\n",
    "N_LABELS = len(train_dataset.get_labels())\n",
    "print('alphabet size: {}\\nclass size: {}'.format(N_CHARS, N_LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "def create_variable(tensor):\n",
    "    return Variable(tensor)\n",
    "\n",
    "def make_variables(inputs, labels):\n",
    "    sequence_and_length = [str2ascii_arr(input) for input in inputs]\n",
    "    vectorized_seqs = [sl[0] for sl in sequence_and_length]\n",
    "    seq_lengths = torch.LongTensor([sl[1] for sl in sequence_and_length])\n",
    "    return pad_sequences(vectorized_seqs, seq_lengths, labels)\n",
    "\n",
    "def str2ascii_arr(msg):\n",
    "    arr = [ALL_LETTERS.index(c) for c in msg]\n",
    "    return arr, len(arr)\n",
    "\n",
    "def pad_sequences(vectorized_seqs, seq_lengths, labels):\n",
    "    seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long()\n",
    "    for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
    "    seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "    seq_tensor = seq_tensor[perm_idx]\n",
    "    target = labels2tensor(labels)\n",
    "    if len(labels):\n",
    "        target = target[perm_idx]\n",
    "    return create_variable(seq_tensor), \\\n",
    "        create_variable(seq_lengths), \\\n",
    "        create_variable(target)\n",
    "        \n",
    "def labels2tensor(labels):\n",
    "    label_ids = [train_dataset.get_label_id(\n",
    "        label) for label in labels]\n",
    "    return torch.LongTensor(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirectional=True):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_directions = int(bidirectional) + 1\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          bidirectional=bidirectional)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, input, seq_lengths):\n",
    "        input = input.t()\n",
    "        batch_size = input.size(1)\n",
    "        hidden = self._init_hidden(batch_size)\n",
    "        embedded = self.embedding(input)\n",
    "        gru_input = pack_padded_sequence(\n",
    "            embedded, seq_lengths.data.cpu().numpy())\n",
    "        self.gru.flatten_parameters()\n",
    "        output, hidden = self.gru(gru_input, hidden)\n",
    "        fc_output = self.fc(hidden[-1])\n",
    "        return fc_output\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers * self.n_directions,\n",
    "                             batch_size, self.hidden_size)\n",
    "        return create_variable(hidden)\n",
    "    \n",
    "classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_LABELS, N_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "log_interval = 1\n",
    "test_interval = 100\n",
    "\n",
    "def train(model, train_loader, test_loader):\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "    model.train()\n",
    "    steps = 0\n",
    "    for epoch in range(1, N_EPOCHS+1):\n",
    "        batch = 0\n",
    "        for inputs, label in train_loader:\n",
    "            input, seq_lengths, target = make_variables(inputs, label)\n",
    "            optimizer.zero_grad()\n",
    "            output = classifier(input, seq_lengths)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            steps += 1\n",
    "            if steps % log_interval == 0:\n",
    "                corrects = (torch.max(output, 1)[1].view(target.size()).data == target.data).sum()\n",
    "                accuracy = 100.0 * corrects/target.shape[0]\n",
    "                print(\n",
    "                    '\\rBatch[{}] - loss: {:.6f}  acc: {:.4f}%({}/{})'.format(steps,\n",
    "                                                                             loss.data[0], \n",
    "                                                                             accuracy,\n",
    "                                                                             corrects,\n",
    "                                                                             target.shape[0]))\n",
    "            if steps % test_interval == 0:\n",
    "                eval(test_loader, model)\n",
    "\n",
    "def eval(test_loader, classifier):\n",
    "    classifier.eval()\n",
    "    print(\"Evaluating trained model ...\")\n",
    "    correct = 0\n",
    "    train_data_size = len(test_loader.dataset)\n",
    "    for inputs, labels in test_loader:\n",
    "        input, seq_lengths, target = make_variables(inputs, labels)\n",
    "        output = classifier(input, seq_lengths)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, train_data_size, 100. * correct / train_data_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\settings\\anaconda\\lib\\site-packages\\ipykernel\\__main__.py:24: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch[1] - loss: 0.695072  acc: 57.0000%(37/64)\n",
      "Batch[2] - loss: 0.784962  acc: 50.0000%(32/64)\n",
      "Batch[3] - loss: 0.687161  acc: 56.0000%(36/64)\n",
      "Batch[4] - loss: 0.692951  acc: 46.0000%(30/64)\n",
      "Batch[5] - loss: 0.709493  acc: 50.0000%(32/64)\n",
      "Batch[6] - loss: 0.714566  acc: 50.0000%(32/64)\n",
      "Batch[7] - loss: 0.684430  acc: 56.0000%(36/64)\n",
      "Batch[8] - loss: 0.675178  acc: 56.0000%(36/64)\n",
      "Batch[9] - loss: 0.692583  acc: 48.0000%(31/64)\n",
      "Batch[10] - loss: 0.684484  acc: 48.0000%(31/64)\n",
      "Batch[11] - loss: 0.680109  acc: 60.0000%(39/64)\n",
      "Batch[12] - loss: 0.693310  acc: 51.0000%(33/64)\n",
      "Batch[13] - loss: 0.683474  acc: 54.0000%(35/64)\n",
      "Batch[14] - loss: 0.716238  acc: 43.0000%(28/64)\n",
      "Batch[15] - loss: 0.678224  acc: 56.0000%(36/64)\n",
      "Batch[16] - loss: 0.686945  acc: 54.0000%(35/64)\n",
      "Batch[17] - loss: 0.670686  acc: 60.0000%(39/64)\n",
      "Batch[18] - loss: 0.686033  acc: 57.0000%(37/64)\n",
      "Batch[19] - loss: 0.663714  acc: 65.0000%(42/64)\n",
      "Batch[20] - loss: 0.700207  acc: 48.0000%(31/64)\n",
      "Batch[21] - loss: 0.661072  acc: 67.0000%(43/64)\n",
      "Batch[22] - loss: 0.674859  acc: 56.0000%(36/64)\n",
      "Batch[23] - loss: 0.688255  acc: 48.0000%(31/64)\n",
      "Batch[24] - loss: 0.663166  acc: 57.0000%(37/64)\n",
      "Batch[25] - loss: 0.655399  acc: 60.0000%(39/64)\n",
      "Batch[26] - loss: 0.694199  acc: 56.0000%(36/64)\n",
      "Batch[27] - loss: 0.642323  acc: 67.0000%(43/64)\n",
      "Batch[28] - loss: 0.648650  acc: 56.0000%(36/64)\n",
      "Batch[29] - loss: 0.613346  acc: 75.0000%(48/64)\n",
      "Batch[30] - loss: 0.620683  acc: 75.0000%(12/16)\n",
      "Batch[31] - loss: 0.727234  acc: 51.0000%(33/64)\n",
      "Batch[32] - loss: 0.652880  acc: 67.0000%(43/64)\n",
      "Batch[33] - loss: 0.658416  acc: 60.0000%(39/64)\n",
      "Batch[34] - loss: 0.619180  acc: 75.0000%(48/64)\n",
      "Batch[35] - loss: 0.605950  acc: 70.0000%(45/64)\n",
      "Batch[36] - loss: 0.715882  acc: 54.0000%(35/64)\n",
      "Batch[37] - loss: 0.623378  acc: 64.0000%(41/64)\n",
      "Batch[38] - loss: 0.612672  acc: 73.0000%(47/64)\n",
      "Batch[39] - loss: 0.586240  acc: 67.0000%(43/64)\n",
      "Batch[40] - loss: 0.626937  acc: 65.0000%(42/64)\n",
      "Batch[41] - loss: 0.675907  acc: 59.0000%(38/64)\n",
      "Batch[42] - loss: 0.635329  acc: 60.0000%(39/64)\n",
      "Batch[43] - loss: 0.563528  acc: 76.0000%(49/64)\n",
      "Batch[44] - loss: 0.700740  acc: 53.0000%(34/64)\n",
      "Batch[45] - loss: 0.641309  acc: 60.0000%(39/64)\n",
      "Batch[46] - loss: 0.655307  acc: 57.0000%(37/64)\n",
      "Batch[47] - loss: 0.599019  acc: 73.0000%(47/64)\n",
      "Batch[48] - loss: 0.674177  acc: 59.0000%(38/64)\n",
      "Batch[49] - loss: 0.707253  acc: 56.0000%(36/64)\n",
      "Batch[50] - loss: 0.654010  acc: 64.0000%(41/64)\n",
      "Batch[51] - loss: 0.674940  acc: 54.0000%(35/64)\n",
      "Batch[52] - loss: 0.685024  acc: 64.0000%(41/64)\n",
      "Batch[53] - loss: 0.708672  acc: 45.0000%(29/64)\n",
      "Batch[54] - loss: 0.649898  acc: 57.0000%(37/64)\n",
      "Batch[55] - loss: 0.688074  acc: 53.0000%(34/64)\n",
      "Batch[56] - loss: 0.673060  acc: 56.0000%(36/64)\n",
      "Batch[57] - loss: 0.630486  acc: 68.0000%(44/64)\n",
      "Batch[58] - loss: 0.583805  acc: 78.0000%(50/64)\n",
      "Batch[59] - loss: 0.704455  acc: 59.0000%(38/64)\n",
      "Batch[60] - loss: 0.740292  acc: 50.0000%(8/16)\n",
      "Batch[61] - loss: 0.643202  acc: 65.0000%(42/64)\n",
      "Batch[62] - loss: 0.605883  acc: 65.0000%(42/64)\n",
      "Batch[63] - loss: 0.637003  acc: 71.0000%(46/64)\n",
      "Batch[64] - loss: 0.623109  acc: 68.0000%(44/64)\n",
      "Batch[65] - loss: 0.592182  acc: 75.0000%(48/64)\n",
      "Batch[66] - loss: 0.633336  acc: 68.0000%(44/64)\n",
      "Batch[67] - loss: 0.654608  acc: 60.0000%(39/64)\n",
      "Batch[68] - loss: 0.589784  acc: 70.0000%(45/64)\n",
      "Batch[69] - loss: 0.648860  acc: 68.0000%(44/64)\n",
      "Batch[70] - loss: 0.664586  acc: 64.0000%(41/64)\n",
      "Batch[71] - loss: 0.503530  acc: 79.0000%(51/64)\n",
      "Batch[72] - loss: 0.664740  acc: 57.0000%(37/64)\n",
      "Batch[73] - loss: 0.624253  acc: 67.0000%(43/64)\n",
      "Batch[74] - loss: 0.508277  acc: 81.0000%(52/64)\n",
      "Batch[75] - loss: 0.522883  acc: 78.0000%(50/64)\n",
      "Batch[76] - loss: 0.516431  acc: 78.0000%(50/64)\n",
      "Batch[77] - loss: 0.481131  acc: 78.0000%(50/64)\n",
      "Batch[78] - loss: 0.653203  acc: 67.0000%(43/64)\n",
      "Batch[79] - loss: 0.547064  acc: 75.0000%(48/64)\n",
      "Batch[80] - loss: 0.671019  acc: 65.0000%(42/64)\n",
      "Batch[81] - loss: 0.602004  acc: 64.0000%(41/64)\n",
      "Batch[82] - loss: 0.727133  acc: 59.0000%(38/64)\n",
      "Batch[83] - loss: 0.761360  acc: 56.0000%(36/64)\n",
      "Batch[84] - loss: 0.671424  acc: 54.0000%(35/64)\n",
      "Batch[85] - loss: 0.641056  acc: 64.0000%(41/64)\n",
      "Batch[86] - loss: 0.633246  acc: 67.0000%(43/64)\n",
      "Batch[87] - loss: 0.638522  acc: 68.0000%(44/64)\n",
      "Batch[88] - loss: 0.602687  acc: 75.0000%(48/64)\n",
      "Batch[89] - loss: 0.708157  acc: 50.0000%(32/64)\n",
      "Batch[90] - loss: 0.747502  acc: 43.0000%(7/16)\n",
      "Batch[91] - loss: 0.647842  acc: 64.0000%(41/64)\n",
      "Batch[92] - loss: 0.604911  acc: 68.0000%(44/64)\n",
      "Batch[93] - loss: 0.626524  acc: 67.0000%(43/64)\n",
      "Batch[94] - loss: 0.628475  acc: 65.0000%(42/64)\n",
      "Batch[95] - loss: 0.593218  acc: 71.0000%(46/64)\n",
      "Batch[96] - loss: 0.621366  acc: 68.0000%(44/64)\n",
      "Batch[97] - loss: 0.596795  acc: 68.0000%(44/64)\n",
      "Batch[98] - loss: 0.605940  acc: 65.0000%(42/64)\n",
      "Batch[99] - loss: 0.643045  acc: 62.0000%(40/64)\n",
      "Batch[100] - loss: 0.571936  acc: 68.0000%(44/64)\n",
      "Evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 130/208 (62%)\n",
      "\n",
      "Batch[101] - loss: 0.636051  acc: 62.0000%(40/64)\n",
      "Batch[102] - loss: 0.669925  acc: 57.0000%(37/64)\n",
      "Batch[103] - loss: 0.596872  acc: 67.0000%(43/64)\n",
      "Batch[104] - loss: 0.651640  acc: 56.0000%(36/64)\n",
      "Batch[105] - loss: 0.566192  acc: 71.0000%(46/64)\n",
      "Batch[106] - loss: 0.496393  acc: 79.0000%(51/64)\n",
      "Batch[107] - loss: 0.642071  acc: 67.0000%(43/64)\n",
      "Batch[108] - loss: 0.609554  acc: 64.0000%(41/64)\n",
      "Batch[109] - loss: 0.725022  acc: 62.0000%(40/64)\n",
      "Batch[110] - loss: 0.602647  acc: 62.0000%(40/64)\n",
      "Batch[111] - loss: 0.649971  acc: 67.0000%(43/64)\n",
      "Batch[112] - loss: 0.628951  acc: 59.0000%(38/64)\n",
      "Batch[113] - loss: 0.568101  acc: 73.0000%(47/64)\n",
      "Batch[114] - loss: 0.663077  acc: 65.0000%(42/64)\n",
      "Batch[115] - loss: 0.708358  acc: 62.0000%(40/64)\n",
      "Batch[116] - loss: 0.504250  acc: 78.0000%(50/64)\n",
      "Batch[117] - loss: 0.528543  acc: 76.0000%(49/64)\n",
      "Batch[118] - loss: 0.525403  acc: 75.0000%(48/64)\n",
      "Batch[119] - loss: 0.645731  acc: 62.0000%(40/64)\n",
      "Batch[120] - loss: 0.480097  acc: 87.0000%(14/16)\n",
      "Batch[121] - loss: 0.500737  acc: 76.0000%(49/64)\n",
      "Batch[122] - loss: 0.500464  acc: 81.0000%(52/64)\n",
      "Batch[123] - loss: 0.532939  acc: 71.0000%(46/64)\n",
      "Batch[124] - loss: 0.507987  acc: 73.0000%(47/64)\n",
      "Batch[125] - loss: 0.542659  acc: 73.0000%(47/64)\n",
      "Batch[126] - loss: 0.560521  acc: 73.0000%(47/64)\n",
      "Batch[127] - loss: 0.642010  acc: 67.0000%(43/64)\n",
      "Batch[128] - loss: 0.584165  acc: 70.0000%(45/64)\n",
      "Batch[129] - loss: 0.462628  acc: 79.0000%(51/64)\n",
      "Batch[130] - loss: 0.508804  acc: 71.0000%(46/64)\n",
      "Batch[131] - loss: 0.633661  acc: 62.0000%(40/64)\n",
      "Batch[132] - loss: 0.456071  acc: 78.0000%(50/64)\n",
      "Batch[133] - loss: 0.414138  acc: 79.0000%(51/64)\n",
      "Batch[134] - loss: 0.440001  acc: 82.0000%(53/64)\n",
      "Batch[135] - loss: 0.604926  acc: 68.0000%(44/64)\n",
      "Batch[136] - loss: 0.557009  acc: 78.0000%(50/64)\n",
      "Batch[137] - loss: 0.521019  acc: 78.0000%(50/64)\n",
      "Batch[138] - loss: 0.603059  acc: 65.0000%(42/64)\n",
      "Batch[139] - loss: 0.511938  acc: 78.0000%(50/64)\n",
      "Batch[140] - loss: 0.521647  acc: 75.0000%(48/64)\n",
      "Batch[141] - loss: 0.575914  acc: 67.0000%(43/64)\n",
      "Batch[142] - loss: 0.511635  acc: 76.0000%(49/64)\n",
      "Batch[143] - loss: 0.592955  acc: 75.0000%(48/64)\n",
      "Batch[144] - loss: 0.593240  acc: 67.0000%(43/64)\n",
      "Batch[145] - loss: 0.498569  acc: 78.0000%(50/64)\n",
      "Batch[146] - loss: 0.536198  acc: 73.0000%(47/64)\n",
      "Batch[147] - loss: 0.557816  acc: 75.0000%(48/64)\n",
      "Batch[148] - loss: 0.511968  acc: 76.0000%(49/64)\n",
      "Batch[149] - loss: 0.489976  acc: 78.0000%(50/64)\n",
      "Batch[150] - loss: 0.619529  acc: 62.0000%(10/16)\n",
      "Batch[151] - loss: 0.747361  acc: 64.0000%(41/64)\n",
      "Batch[152] - loss: 0.657408  acc: 68.0000%(44/64)\n",
      "Batch[153] - loss: 0.542971  acc: 70.0000%(45/64)\n",
      "Batch[154] - loss: 0.503199  acc: 76.0000%(49/64)\n",
      "Batch[155] - loss: 0.677263  acc: 62.0000%(40/64)\n",
      "Batch[156] - loss: 0.619176  acc: 64.0000%(41/64)\n",
      "Batch[157] - loss: 0.629598  acc: 65.0000%(42/64)\n",
      "Batch[158] - loss: 0.531036  acc: 71.0000%(46/64)\n",
      "Batch[159] - loss: 0.574005  acc: 75.0000%(48/64)\n",
      "Batch[160] - loss: 0.580968  acc: 65.0000%(42/64)\n",
      "Batch[161] - loss: 0.548380  acc: 71.0000%(46/64)\n",
      "Batch[162] - loss: 0.583733  acc: 70.0000%(45/64)\n",
      "Batch[163] - loss: 0.552040  acc: 71.0000%(46/64)\n",
      "Batch[164] - loss: 0.616377  acc: 71.0000%(46/64)\n",
      "Batch[165] - loss: 0.495126  acc: 75.0000%(48/64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch[166] - loss: 0.525621  acc: 73.0000%(47/64)\n",
      "Batch[167] - loss: 0.528198  acc: 76.0000%(49/64)\n",
      "Batch[168] - loss: 0.460855  acc: 82.0000%(53/64)\n",
      "Batch[169] - loss: 0.521287  acc: 73.0000%(47/64)\n",
      "Batch[170] - loss: 0.559714  acc: 64.0000%(41/64)\n",
      "Batch[171] - loss: 0.420093  acc: 81.0000%(52/64)\n",
      "Batch[172] - loss: 0.476868  acc: 79.0000%(51/64)\n",
      "Batch[173] - loss: 0.601155  acc: 71.0000%(46/64)\n",
      "Batch[174] - loss: 0.667982  acc: 62.0000%(40/64)\n",
      "Batch[175] - loss: 0.637026  acc: 65.0000%(42/64)\n",
      "Batch[176] - loss: 0.427899  acc: 79.0000%(51/64)\n",
      "Batch[177] - loss: 0.516559  acc: 76.0000%(49/64)\n",
      "Batch[178] - loss: 0.584525  acc: 73.0000%(47/64)\n",
      "Batch[179] - loss: 0.547917  acc: 67.0000%(43/64)\n",
      "Batch[180] - loss: 0.476676  acc: 87.0000%(14/16)\n",
      "Batch[181] - loss: 0.580649  acc: 68.0000%(44/64)\n",
      "Batch[182] - loss: 0.433930  acc: 84.0000%(54/64)\n",
      "Batch[183] - loss: 0.434861  acc: 84.0000%(54/64)\n",
      "Batch[184] - loss: 0.542010  acc: 76.0000%(49/64)\n",
      "Batch[185] - loss: 0.530967  acc: 75.0000%(48/64)\n",
      "Batch[186] - loss: 0.590161  acc: 78.0000%(50/64)\n",
      "Batch[187] - loss: 0.406060  acc: 90.0000%(58/64)\n",
      "Batch[188] - loss: 0.435704  acc: 81.0000%(52/64)\n",
      "Batch[189] - loss: 0.508719  acc: 75.0000%(48/64)\n",
      "Batch[190] - loss: 0.480974  acc: 78.0000%(50/64)\n",
      "Batch[191] - loss: 0.489237  acc: 81.0000%(52/64)\n",
      "Batch[192] - loss: 0.516207  acc: 75.0000%(48/64)\n",
      "Batch[193] - loss: 0.389498  acc: 82.0000%(53/64)\n",
      "Batch[194] - loss: 0.595525  acc: 65.0000%(42/64)\n",
      "Batch[195] - loss: 0.653554  acc: 65.0000%(42/64)\n",
      "Batch[196] - loss: 0.530353  acc: 71.0000%(46/64)\n",
      "Batch[197] - loss: 0.439411  acc: 82.0000%(53/64)\n",
      "Batch[198] - loss: 0.513504  acc: 76.0000%(49/64)\n",
      "Batch[199] - loss: 0.595431  acc: 70.0000%(45/64)\n",
      "Batch[200] - loss: 0.466923  acc: 82.0000%(53/64)\n",
      "Evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 159/208 (76%)\n",
      "\n",
      "Batch[201] - loss: 0.547992  acc: 76.0000%(49/64)\n",
      "Batch[202] - loss: 0.491848  acc: 76.0000%(49/64)\n",
      "Batch[203] - loss: 0.544205  acc: 75.0000%(48/64)\n",
      "Batch[204] - loss: 0.596478  acc: 67.0000%(43/64)\n",
      "Batch[205] - loss: 0.600112  acc: 68.0000%(44/64)\n",
      "Batch[206] - loss: 0.480485  acc: 78.0000%(50/64)\n",
      "Batch[207] - loss: 0.454224  acc: 79.0000%(51/64)\n",
      "Batch[208] - loss: 0.490314  acc: 78.0000%(50/64)\n",
      "Batch[209] - loss: 0.438769  acc: 84.0000%(54/64)\n",
      "Batch[210] - loss: 0.555128  acc: 75.0000%(12/16)\n",
      "Batch[211] - loss: 0.480359  acc: 76.0000%(49/64)\n",
      "Batch[212] - loss: 0.495159  acc: 79.0000%(51/64)\n",
      "Batch[213] - loss: 0.594294  acc: 68.0000%(44/64)\n",
      "Batch[214] - loss: 0.550339  acc: 68.0000%(44/64)\n",
      "Batch[215] - loss: 0.396234  acc: 84.0000%(54/64)\n",
      "Batch[216] - loss: 0.409052  acc: 82.0000%(53/64)\n",
      "Batch[217] - loss: 0.541766  acc: 75.0000%(48/64)\n",
      "Batch[218] - loss: 0.535551  acc: 71.0000%(46/64)\n",
      "Batch[219] - loss: 0.378732  acc: 85.0000%(55/64)\n",
      "Batch[220] - loss: 0.469349  acc: 79.0000%(51/64)\n",
      "Batch[221] - loss: 0.534284  acc: 73.0000%(47/64)\n",
      "Batch[222] - loss: 0.372332  acc: 84.0000%(54/64)\n",
      "Batch[223] - loss: 0.612185  acc: 70.0000%(45/64)\n",
      "Batch[224] - loss: 0.492906  acc: 78.0000%(50/64)\n",
      "Batch[225] - loss: 0.481596  acc: 79.0000%(51/64)\n",
      "Batch[226] - loss: 0.353188  acc: 79.0000%(51/64)\n",
      "Batch[227] - loss: 0.474200  acc: 85.0000%(55/64)\n",
      "Batch[228] - loss: 0.546375  acc: 68.0000%(44/64)\n",
      "Batch[229] - loss: 0.505365  acc: 79.0000%(51/64)\n",
      "Batch[230] - loss: 0.348207  acc: 89.0000%(57/64)\n",
      "Batch[231] - loss: 0.492312  acc: 76.0000%(49/64)\n",
      "Batch[232] - loss: 0.608310  acc: 68.0000%(44/64)\n",
      "Batch[233] - loss: 0.474285  acc: 79.0000%(51/64)\n",
      "Batch[234] - loss: 0.617932  acc: 67.0000%(43/64)\n",
      "Batch[235] - loss: 0.506428  acc: 73.0000%(47/64)\n",
      "Batch[236] - loss: 0.495087  acc: 81.0000%(52/64)\n",
      "Batch[237] - loss: 0.485128  acc: 78.0000%(50/64)\n",
      "Batch[238] - loss: 0.494580  acc: 75.0000%(48/64)\n",
      "Batch[239] - loss: 0.463450  acc: 73.0000%(47/64)\n",
      "Batch[240] - loss: 0.489616  acc: 62.0000%(10/16)\n",
      "Batch[241] - loss: 0.365329  acc: 89.0000%(57/64)\n",
      "Batch[242] - loss: 0.501580  acc: 76.0000%(49/64)\n",
      "Batch[243] - loss: 0.508305  acc: 76.0000%(49/64)\n",
      "Batch[244] - loss: 0.559413  acc: 73.0000%(47/64)\n",
      "Batch[245] - loss: 0.479708  acc: 81.0000%(52/64)\n",
      "Batch[246] - loss: 0.415091  acc: 79.0000%(51/64)\n",
      "Batch[247] - loss: 0.432217  acc: 84.0000%(54/64)\n",
      "Batch[248] - loss: 0.446295  acc: 78.0000%(50/64)\n",
      "Batch[249] - loss: 0.470089  acc: 78.0000%(50/64)\n",
      "Batch[250] - loss: 0.532397  acc: 75.0000%(48/64)\n",
      "Batch[251] - loss: 0.540581  acc: 71.0000%(46/64)\n",
      "Batch[252] - loss: 0.413249  acc: 81.0000%(52/64)\n",
      "Batch[253] - loss: 0.462381  acc: 73.0000%(47/64)\n",
      "Batch[254] - loss: 0.466696  acc: 76.0000%(49/64)\n",
      "Batch[255] - loss: 0.410051  acc: 73.0000%(47/64)\n",
      "Batch[256] - loss: 0.536623  acc: 78.0000%(50/64)\n",
      "Batch[257] - loss: 0.389288  acc: 78.0000%(50/64)\n",
      "Batch[258] - loss: 0.454923  acc: 78.0000%(50/64)\n",
      "Batch[259] - loss: 0.457114  acc: 81.0000%(52/64)\n",
      "Batch[260] - loss: 0.510783  acc: 68.0000%(44/64)\n",
      "Batch[261] - loss: 0.488381  acc: 70.0000%(45/64)\n",
      "Batch[262] - loss: 0.461731  acc: 79.0000%(51/64)\n",
      "Batch[263] - loss: 0.465692  acc: 79.0000%(51/64)\n",
      "Batch[264] - loss: 0.548953  acc: 71.0000%(46/64)\n",
      "Batch[265] - loss: 0.428409  acc: 82.0000%(53/64)\n",
      "Batch[266] - loss: 0.545206  acc: 78.0000%(50/64)\n",
      "Batch[267] - loss: 0.474155  acc: 76.0000%(49/64)\n",
      "Batch[268] - loss: 0.515120  acc: 81.0000%(52/64)\n",
      "Batch[269] - loss: 0.511919  acc: 78.0000%(50/64)\n",
      "Batch[270] - loss: 0.564468  acc: 81.0000%(13/16)\n",
      "Batch[271] - loss: 0.489903  acc: 81.0000%(52/64)\n",
      "Batch[272] - loss: 0.544810  acc: 73.0000%(47/64)\n",
      "Batch[273] - loss: 0.489484  acc: 79.0000%(51/64)\n",
      "Batch[274] - loss: 0.463359  acc: 75.0000%(48/64)\n",
      "Batch[275] - loss: 0.452312  acc: 70.0000%(45/64)\n",
      "Batch[276] - loss: 0.404283  acc: 81.0000%(52/64)\n",
      "Batch[277] - loss: 0.485923  acc: 79.0000%(51/64)\n",
      "Batch[278] - loss: 0.397760  acc: 87.0000%(56/64)\n",
      "Batch[279] - loss: 0.320471  acc: 85.0000%(55/64)\n",
      "Batch[280] - loss: 0.541080  acc: 73.0000%(47/64)\n",
      "Batch[281] - loss: 0.582514  acc: 68.0000%(44/64)\n",
      "Batch[282] - loss: 0.291168  acc: 87.0000%(56/64)\n",
      "Batch[283] - loss: 0.330108  acc: 89.0000%(57/64)\n",
      "Batch[284] - loss: 0.503460  acc: 75.0000%(48/64)\n",
      "Batch[285] - loss: 0.611850  acc: 68.0000%(44/64)\n",
      "Batch[286] - loss: 0.354609  acc: 85.0000%(55/64)\n",
      "Batch[287] - loss: 0.418746  acc: 82.0000%(53/64)\n",
      "Batch[288] - loss: 0.437468  acc: 79.0000%(51/64)\n",
      "Batch[289] - loss: 0.429186  acc: 81.0000%(52/64)\n",
      "Batch[290] - loss: 0.414092  acc: 79.0000%(51/64)\n",
      "Batch[291] - loss: 0.433560  acc: 76.0000%(49/64)\n",
      "Batch[292] - loss: 0.446866  acc: 81.0000%(52/64)\n",
      "Batch[293] - loss: 0.410149  acc: 79.0000%(51/64)\n",
      "Batch[294] - loss: 0.402377  acc: 82.0000%(53/64)\n",
      "Batch[295] - loss: 0.539934  acc: 71.0000%(46/64)\n",
      "Batch[296] - loss: 0.448378  acc: 79.0000%(51/64)\n",
      "Batch[297] - loss: 0.474679  acc: 76.0000%(49/64)\n",
      "Batch[298] - loss: 0.391986  acc: 82.0000%(53/64)\n",
      "Batch[299] - loss: 0.483577  acc: 73.0000%(47/64)\n",
      "Batch[300] - loss: 0.489820  acc: 81.0000%(13/16)\n",
      "Evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 160/208 (76%)\n",
      "\n",
      "Batch[301] - loss: 0.418914  acc: 78.0000%(50/64)\n",
      "Batch[302] - loss: 0.409730  acc: 81.0000%(52/64)\n",
      "Batch[303] - loss: 0.521213  acc: 75.0000%(48/64)\n",
      "Batch[304] - loss: 0.664559  acc: 68.0000%(44/64)\n",
      "Batch[305] - loss: 0.451314  acc: 81.0000%(52/64)\n",
      "Batch[306] - loss: 0.352264  acc: 82.0000%(53/64)\n",
      "Batch[307] - loss: 0.420926  acc: 79.0000%(51/64)\n",
      "Batch[308] - loss: 0.354775  acc: 85.0000%(55/64)\n",
      "Batch[309] - loss: 0.402051  acc: 79.0000%(51/64)\n",
      "Batch[310] - loss: 0.365344  acc: 81.0000%(52/64)\n",
      "Batch[311] - loss: 0.461331  acc: 75.0000%(48/64)\n",
      "Batch[312] - loss: 0.504083  acc: 73.0000%(47/64)\n",
      "Batch[313] - loss: 0.415979  acc: 81.0000%(52/64)\n",
      "Batch[314] - loss: 0.478573  acc: 73.0000%(47/64)\n",
      "Batch[315] - loss: 0.465976  acc: 82.0000%(53/64)\n",
      "Batch[316] - loss: 0.528241  acc: 71.0000%(46/64)\n",
      "Batch[317] - loss: 0.448031  acc: 82.0000%(53/64)\n",
      "Batch[318] - loss: 0.330967  acc: 90.0000%(58/64)\n",
      "Batch[319] - loss: 0.386523  acc: 87.0000%(56/64)\n",
      "Batch[320] - loss: 0.427223  acc: 82.0000%(53/64)\n",
      "Batch[321] - loss: 0.498523  acc: 75.0000%(48/64)\n",
      "Batch[322] - loss: 0.367645  acc: 84.0000%(54/64)\n",
      "Batch[323] - loss: 0.564955  acc: 78.0000%(50/64)\n",
      "Batch[324] - loss: 0.428519  acc: 81.0000%(52/64)\n",
      "Batch[325] - loss: 0.342868  acc: 82.0000%(53/64)\n",
      "Batch[326] - loss: 0.393898  acc: 85.0000%(55/64)\n",
      "Batch[327] - loss: 0.473997  acc: 75.0000%(48/64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch[328] - loss: 0.362759  acc: 82.0000%(53/64)\n",
      "Batch[329] - loss: 0.456890  acc: 75.0000%(48/64)\n",
      "Batch[330] - loss: 0.403967  acc: 87.0000%(14/16)\n",
      "Batch[331] - loss: 0.392646  acc: 81.0000%(52/64)\n",
      "Batch[332] - loss: 0.416082  acc: 78.0000%(50/64)\n",
      "Batch[333] - loss: 0.492521  acc: 76.0000%(49/64)\n",
      "Batch[334] - loss: 0.647002  acc: 68.0000%(44/64)\n",
      "Batch[335] - loss: 0.365096  acc: 84.0000%(54/64)\n",
      "Batch[336] - loss: 0.408066  acc: 78.0000%(50/64)\n",
      "Batch[337] - loss: 0.479914  acc: 75.0000%(48/64)\n",
      "Batch[338] - loss: 0.422542  acc: 79.0000%(51/64)\n",
      "Batch[339] - loss: 0.379668  acc: 82.0000%(53/64)\n",
      "Batch[340] - loss: 0.455551  acc: 78.0000%(50/64)\n",
      "Batch[341] - loss: 0.463346  acc: 81.0000%(52/64)\n",
      "Batch[342] - loss: 0.534537  acc: 73.0000%(47/64)\n",
      "Batch[343] - loss: 0.579012  acc: 68.0000%(44/64)\n",
      "Batch[344] - loss: 0.393909  acc: 81.0000%(52/64)\n",
      "Batch[345] - loss: 0.302860  acc: 92.0000%(59/64)\n",
      "Batch[346] - loss: 0.402024  acc: 79.0000%(51/64)\n",
      "Batch[347] - loss: 0.423295  acc: 76.0000%(49/64)\n",
      "Batch[348] - loss: 0.422076  acc: 78.0000%(50/64)\n",
      "Batch[349] - loss: 0.418019  acc: 87.0000%(56/64)\n",
      "Batch[350] - loss: 0.383051  acc: 82.0000%(53/64)\n",
      "Batch[351] - loss: 0.295778  acc: 90.0000%(58/64)\n",
      "Batch[352] - loss: 0.421571  acc: 79.0000%(51/64)\n",
      "Batch[353] - loss: 0.389069  acc: 81.0000%(52/64)\n",
      "Batch[354] - loss: 0.265744  acc: 92.0000%(59/64)\n",
      "Batch[355] - loss: 0.282643  acc: 87.0000%(56/64)\n",
      "Batch[356] - loss: 0.419174  acc: 79.0000%(51/64)\n",
      "Batch[357] - loss: 0.433291  acc: 76.0000%(49/64)\n",
      "Batch[358] - loss: 0.513222  acc: 79.0000%(51/64)\n",
      "Batch[359] - loss: 0.389656  acc: 82.0000%(53/64)\n",
      "Batch[360] - loss: 0.642607  acc: 68.0000%(11/16)\n",
      "Batch[361] - loss: 0.440710  acc: 84.0000%(54/64)\n",
      "Batch[362] - loss: 0.343744  acc: 84.0000%(54/64)\n",
      "Batch[363] - loss: 0.514577  acc: 81.0000%(52/64)\n",
      "Batch[364] - loss: 0.372430  acc: 84.0000%(54/64)\n",
      "Batch[365] - loss: 0.380298  acc: 84.0000%(54/64)\n",
      "Batch[366] - loss: 0.305353  acc: 87.0000%(56/64)\n",
      "Batch[367] - loss: 0.330228  acc: 87.0000%(56/64)\n",
      "Batch[368] - loss: 0.360140  acc: 84.0000%(54/64)\n",
      "Batch[369] - loss: 0.300947  acc: 89.0000%(57/64)\n",
      "Batch[370] - loss: 0.445052  acc: 78.0000%(50/64)\n",
      "Batch[371] - loss: 0.275006  acc: 89.0000%(57/64)\n",
      "Batch[372] - loss: 0.370916  acc: 81.0000%(52/64)\n",
      "Batch[373] - loss: 0.465030  acc: 79.0000%(51/64)\n",
      "Batch[374] - loss: 0.377436  acc: 82.0000%(53/64)\n",
      "Batch[375] - loss: 0.367821  acc: 87.0000%(56/64)\n",
      "Batch[376] - loss: 0.384925  acc: 79.0000%(51/64)\n",
      "Batch[377] - loss: 0.259757  acc: 93.0000%(60/64)\n",
      "Batch[378] - loss: 0.467231  acc: 78.0000%(50/64)\n",
      "Batch[379] - loss: 0.364248  acc: 84.0000%(54/64)\n",
      "Batch[380] - loss: 0.329989  acc: 84.0000%(54/64)\n",
      "Batch[381] - loss: 0.438034  acc: 78.0000%(50/64)\n",
      "Batch[382] - loss: 0.397320  acc: 79.0000%(51/64)\n",
      "Batch[383] - loss: 0.459077  acc: 82.0000%(53/64)\n",
      "Batch[384] - loss: 0.381137  acc: 78.0000%(50/64)\n",
      "Batch[385] - loss: 0.439108  acc: 81.0000%(52/64)\n",
      "Batch[386] - loss: 0.498680  acc: 73.0000%(47/64)\n",
      "Batch[387] - loss: 0.581677  acc: 73.0000%(47/64)\n",
      "Batch[388] - loss: 0.462820  acc: 78.0000%(50/64)\n",
      "Batch[389] - loss: 0.326406  acc: 85.0000%(55/64)\n",
      "Batch[390] - loss: 0.377431  acc: 81.0000%(13/16)\n",
      "Batch[391] - loss: 0.398559  acc: 79.0000%(51/64)\n",
      "Batch[392] - loss: 0.315645  acc: 87.0000%(56/64)\n",
      "Batch[393] - loss: 0.339879  acc: 85.0000%(55/64)\n",
      "Batch[394] - loss: 0.338700  acc: 89.0000%(57/64)\n",
      "Batch[395] - loss: 0.420304  acc: 78.0000%(50/64)\n",
      "Batch[396] - loss: 0.460961  acc: 79.0000%(51/64)\n",
      "Batch[397] - loss: 0.456116  acc: 76.0000%(49/64)\n",
      "Batch[398] - loss: 0.343767  acc: 87.0000%(56/64)\n",
      "Batch[399] - loss: 0.298418  acc: 92.0000%(59/64)\n",
      "Batch[400] - loss: 0.456317  acc: 76.0000%(49/64)\n",
      "Evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 167/208 (80%)\n",
      "\n",
      "Batch[401] - loss: 0.354733  acc: 79.0000%(51/64)\n",
      "Batch[402] - loss: 0.329960  acc: 82.0000%(53/64)\n",
      "Batch[403] - loss: 0.245175  acc: 89.0000%(57/64)\n",
      "Batch[404] - loss: 0.422669  acc: 79.0000%(51/64)\n",
      "Batch[405] - loss: 0.239646  acc: 92.0000%(59/64)\n",
      "Batch[406] - loss: 0.205497  acc: 93.0000%(60/64)\n",
      "Batch[407] - loss: 0.527280  acc: 71.0000%(46/64)\n",
      "Batch[408] - loss: 0.372347  acc: 85.0000%(55/64)\n",
      "Batch[409] - loss: 0.459753  acc: 79.0000%(51/64)\n",
      "Batch[410] - loss: 0.322548  acc: 82.0000%(53/64)\n",
      "Batch[411] - loss: 0.490948  acc: 78.0000%(50/64)\n",
      "Batch[412] - loss: 0.262683  acc: 89.0000%(57/64)\n",
      "Batch[413] - loss: 0.401766  acc: 81.0000%(52/64)\n",
      "Batch[414] - loss: 0.378003  acc: 85.0000%(55/64)\n",
      "Batch[415] - loss: 0.359547  acc: 85.0000%(55/64)\n",
      "Batch[416] - loss: 0.365422  acc: 84.0000%(54/64)\n",
      "Batch[417] - loss: 0.382685  acc: 81.0000%(52/64)\n",
      "Batch[418] - loss: 0.421460  acc: 76.0000%(49/64)\n",
      "Batch[419] - loss: 0.439342  acc: 79.0000%(51/64)\n",
      "Batch[420] - loss: 0.464768  acc: 75.0000%(12/16)\n",
      "Batch[421] - loss: 0.383276  acc: 78.0000%(50/64)\n",
      "Batch[422] - loss: 0.315894  acc: 85.0000%(55/64)\n",
      "Batch[423] - loss: 0.425639  acc: 81.0000%(52/64)\n",
      "Batch[424] - loss: 0.334440  acc: 82.0000%(53/64)\n",
      "Batch[425] - loss: 0.383252  acc: 85.0000%(55/64)\n",
      "Batch[426] - loss: 0.377220  acc: 82.0000%(53/64)\n",
      "Batch[427] - loss: 0.354361  acc: 81.0000%(52/64)\n",
      "Batch[428] - loss: 0.449982  acc: 78.0000%(50/64)\n",
      "Batch[429] - loss: 0.353451  acc: 87.0000%(56/64)\n",
      "Batch[430] - loss: 0.289811  acc: 87.0000%(56/64)\n",
      "Batch[431] - loss: 0.421412  acc: 78.0000%(50/64)\n",
      "Batch[432] - loss: 0.271775  acc: 89.0000%(57/64)\n",
      "Batch[433] - loss: 0.356385  acc: 82.0000%(53/64)\n",
      "Batch[434] - loss: 0.397460  acc: 82.0000%(53/64)\n",
      "Batch[435] - loss: 0.362359  acc: 79.0000%(51/64)\n",
      "Batch[436] - loss: 0.278048  acc: 89.0000%(57/64)\n",
      "Batch[437] - loss: 0.367822  acc: 81.0000%(52/64)\n",
      "Batch[438] - loss: 0.351558  acc: 82.0000%(53/64)\n",
      "Batch[439] - loss: 0.480950  acc: 76.0000%(49/64)\n",
      "Batch[440] - loss: 0.365391  acc: 84.0000%(54/64)\n",
      "Batch[441] - loss: 0.288913  acc: 84.0000%(54/64)\n",
      "Batch[442] - loss: 0.303162  acc: 87.0000%(56/64)\n",
      "Batch[443] - loss: 0.430959  acc: 82.0000%(53/64)\n",
      "Batch[444] - loss: 0.365692  acc: 82.0000%(53/64)\n",
      "Batch[445] - loss: 0.378337  acc: 79.0000%(51/64)\n",
      "Batch[446] - loss: 0.406066  acc: 78.0000%(50/64)\n",
      "Batch[447] - loss: 0.396387  acc: 81.0000%(52/64)\n",
      "Batch[448] - loss: 0.293211  acc: 85.0000%(55/64)\n",
      "Batch[449] - loss: 0.345383  acc: 84.0000%(54/64)\n",
      "Batch[450] - loss: 0.216331  acc: 93.0000%(15/16)\n",
      "Batch[451] - loss: 0.310559  acc: 84.0000%(54/64)\n",
      "Batch[452] - loss: 0.427825  acc: 76.0000%(49/64)\n",
      "Batch[453] - loss: 0.409281  acc: 76.0000%(49/64)\n",
      "Batch[454] - loss: 0.300049  acc: 87.0000%(56/64)\n",
      "Batch[455] - loss: 0.288112  acc: 89.0000%(57/64)\n",
      "Batch[456] - loss: 0.341038  acc: 85.0000%(55/64)\n",
      "Batch[457] - loss: 0.250724  acc: 90.0000%(58/64)\n",
      "Batch[458] - loss: 0.474001  acc: 75.0000%(48/64)\n",
      "Batch[459] - loss: 0.340601  acc: 79.0000%(51/64)\n",
      "Batch[460] - loss: 0.397361  acc: 82.0000%(53/64)\n",
      "Batch[461] - loss: 0.430299  acc: 79.0000%(51/64)\n",
      "Batch[462] - loss: 0.270662  acc: 92.0000%(59/64)\n",
      "Batch[463] - loss: 0.434374  acc: 82.0000%(53/64)\n",
      "Batch[464] - loss: 0.300907  acc: 84.0000%(54/64)\n",
      "Batch[465] - loss: 0.350890  acc: 82.0000%(53/64)\n",
      "Batch[466] - loss: 0.354402  acc: 81.0000%(52/64)\n",
      "Batch[467] - loss: 0.303318  acc: 82.0000%(53/64)\n",
      "Batch[468] - loss: 0.285947  acc: 90.0000%(58/64)\n",
      "Batch[469] - loss: 0.352105  acc: 87.0000%(56/64)\n",
      "Batch[470] - loss: 0.345222  acc: 84.0000%(54/64)\n",
      "Batch[471] - loss: 0.423469  acc: 81.0000%(52/64)\n",
      "Batch[472] - loss: 0.300635  acc: 84.0000%(54/64)\n",
      "Batch[473] - loss: 0.350722  acc: 84.0000%(54/64)\n",
      "Batch[474] - loss: 0.386837  acc: 79.0000%(51/64)\n",
      "Batch[475] - loss: 0.408854  acc: 81.0000%(52/64)\n",
      "Batch[476] - loss: 0.487766  acc: 73.0000%(47/64)\n",
      "Batch[477] - loss: 0.433434  acc: 76.0000%(49/64)\n",
      "Batch[478] - loss: 0.468110  acc: 71.0000%(46/64)\n",
      "Batch[479] - loss: 0.354681  acc: 84.0000%(54/64)\n",
      "Batch[480] - loss: 0.382561  acc: 75.0000%(12/16)\n",
      "Batch[481] - loss: 0.377143  acc: 76.0000%(49/64)\n",
      "Batch[482] - loss: 0.349235  acc: 87.0000%(56/64)\n",
      "Batch[483] - loss: 0.412106  acc: 79.0000%(51/64)\n",
      "Batch[484] - loss: 0.294148  acc: 90.0000%(58/64)\n",
      "Batch[485] - loss: 0.326447  acc: 84.0000%(54/64)\n",
      "Batch[486] - loss: 0.384036  acc: 81.0000%(52/64)\n",
      "Batch[487] - loss: 0.296808  acc: 90.0000%(58/64)\n",
      "Batch[488] - loss: 0.390094  acc: 82.0000%(53/64)\n",
      "Batch[489] - loss: 0.354403  acc: 82.0000%(53/64)\n",
      "Batch[490] - loss: 0.478245  acc: 71.0000%(46/64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch[491] - loss: 0.319896  acc: 82.0000%(53/64)\n",
      "Batch[492] - loss: 0.361747  acc: 85.0000%(55/64)\n",
      "Batch[493] - loss: 0.361240  acc: 85.0000%(55/64)\n",
      "Batch[494] - loss: 0.377547  acc: 82.0000%(53/64)\n",
      "Batch[495] - loss: 0.441991  acc: 75.0000%(48/64)\n",
      "Batch[496] - loss: 0.265899  acc: 87.0000%(56/64)\n",
      "Batch[497] - loss: 0.340893  acc: 84.0000%(54/64)\n",
      "Batch[498] - loss: 0.356266  acc: 81.0000%(52/64)\n",
      "Batch[499] - loss: 0.279477  acc: 84.0000%(54/64)\n",
      "Batch[500] - loss: 0.253693  acc: 90.0000%(58/64)\n",
      "Evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 168/208 (80%)\n",
      "\n",
      "Batch[501] - loss: 0.241669  acc: 87.0000%(56/64)\n",
      "Batch[502] - loss: 0.351817  acc: 82.0000%(53/64)\n",
      "Batch[503] - loss: 0.426745  acc: 76.0000%(49/64)\n",
      "Batch[504] - loss: 0.233715  acc: 92.0000%(59/64)\n",
      "Batch[505] - loss: 0.470248  acc: 82.0000%(53/64)\n",
      "Batch[506] - loss: 0.271504  acc: 87.0000%(56/64)\n",
      "Batch[507] - loss: 0.540791  acc: 73.0000%(47/64)\n",
      "Batch[508] - loss: 0.575955  acc: 70.0000%(45/64)\n",
      "Batch[509] - loss: 0.289031  acc: 89.0000%(57/64)\n",
      "Batch[510] - loss: 0.230852  acc: 87.0000%(14/16)\n",
      "Batch[511] - loss: 0.375346  acc: 81.0000%(52/64)\n",
      "Batch[512] - loss: 0.364091  acc: 84.0000%(54/64)\n",
      "Batch[513] - loss: 0.352613  acc: 87.0000%(56/64)\n",
      "Batch[514] - loss: 0.382248  acc: 81.0000%(52/64)\n",
      "Batch[515] - loss: 0.274873  acc: 89.0000%(57/64)\n",
      "Batch[516] - loss: 0.383992  acc: 76.0000%(49/64)\n",
      "Batch[517] - loss: 0.306581  acc: 85.0000%(55/64)\n",
      "Batch[518] - loss: 0.285549  acc: 85.0000%(55/64)\n",
      "Batch[519] - loss: 0.252560  acc: 90.0000%(58/64)\n",
      "Batch[520] - loss: 0.402634  acc: 78.0000%(50/64)\n",
      "Batch[521] - loss: 0.338071  acc: 84.0000%(54/64)\n",
      "Batch[522] - loss: 0.419148  acc: 75.0000%(48/64)\n",
      "Batch[523] - loss: 0.350853  acc: 84.0000%(54/64)\n",
      "Batch[524] - loss: 0.318429  acc: 84.0000%(54/64)\n",
      "Batch[525] - loss: 0.365919  acc: 79.0000%(51/64)\n",
      "Batch[526] - loss: 0.357694  acc: 81.0000%(52/64)\n",
      "Batch[527] - loss: 0.305133  acc: 84.0000%(54/64)\n",
      "Batch[528] - loss: 0.341492  acc: 79.0000%(51/64)\n",
      "Batch[529] - loss: 0.443709  acc: 84.0000%(54/64)\n",
      "Batch[530] - loss: 0.341225  acc: 81.0000%(52/64)\n",
      "Batch[531] - loss: 0.318482  acc: 87.0000%(56/64)\n",
      "Batch[532] - loss: 0.333079  acc: 85.0000%(55/64)\n",
      "Batch[533] - loss: 0.277899  acc: 87.0000%(56/64)\n",
      "Batch[534] - loss: 0.439433  acc: 79.0000%(51/64)\n",
      "Batch[535] - loss: 0.307370  acc: 87.0000%(56/64)\n",
      "Batch[536] - loss: 0.317404  acc: 82.0000%(53/64)\n",
      "Batch[537] - loss: 0.379749  acc: 79.0000%(51/64)\n",
      "Batch[538] - loss: 0.310618  acc: 82.0000%(53/64)\n",
      "Batch[539] - loss: 0.343180  acc: 84.0000%(54/64)\n",
      "Batch[540] - loss: 0.390138  acc: 81.0000%(13/16)\n",
      "Batch[541] - loss: 0.373106  acc: 85.0000%(55/64)\n",
      "Batch[542] - loss: 0.262763  acc: 92.0000%(59/64)\n",
      "Batch[543] - loss: 0.374678  acc: 82.0000%(53/64)\n",
      "Batch[544] - loss: 0.430949  acc: 81.0000%(52/64)\n",
      "Batch[545] - loss: 0.246308  acc: 87.0000%(56/64)\n",
      "Batch[546] - loss: 0.258653  acc: 89.0000%(57/64)\n",
      "Batch[547] - loss: 0.324123  acc: 84.0000%(54/64)\n",
      "Batch[548] - loss: 0.330402  acc: 85.0000%(55/64)\n",
      "Batch[549] - loss: 0.458224  acc: 78.0000%(50/64)\n",
      "Batch[550] - loss: 0.291996  acc: 85.0000%(55/64)\n",
      "Batch[551] - loss: 0.389014  acc: 84.0000%(54/64)\n",
      "Batch[552] - loss: 0.306210  acc: 87.0000%(56/64)\n",
      "Batch[553] - loss: 0.290878  acc: 87.0000%(56/64)\n",
      "Batch[554] - loss: 0.330518  acc: 82.0000%(53/64)\n",
      "Batch[555] - loss: 0.294809  acc: 85.0000%(55/64)\n",
      "Batch[556] - loss: 0.340664  acc: 84.0000%(54/64)\n",
      "Batch[557] - loss: 0.367913  acc: 84.0000%(54/64)\n",
      "Batch[558] - loss: 0.283391  acc: 85.0000%(55/64)\n",
      "Batch[559] - loss: 0.359805  acc: 82.0000%(53/64)\n",
      "Batch[560] - loss: 0.306294  acc: 87.0000%(56/64)\n",
      "Batch[561] - loss: 0.358635  acc: 82.0000%(53/64)\n",
      "Batch[562] - loss: 0.513334  acc: 73.0000%(47/64)\n",
      "Batch[563] - loss: 0.387288  acc: 84.0000%(54/64)\n",
      "Batch[564] - loss: 0.298887  acc: 84.0000%(54/64)\n",
      "Batch[565] - loss: 0.242734  acc: 92.0000%(59/64)\n",
      "Batch[566] - loss: 0.373746  acc: 82.0000%(53/64)\n",
      "Batch[567] - loss: 0.386937  acc: 79.0000%(51/64)\n",
      "Batch[568] - loss: 0.222046  acc: 90.0000%(58/64)\n",
      "Batch[569] - loss: 0.426080  acc: 79.0000%(51/64)\n",
      "Batch[570] - loss: 0.166364  acc: 93.0000%(15/16)\n",
      "Batch[571] - loss: 0.432953  acc: 79.0000%(51/64)\n",
      "Batch[572] - loss: 0.337528  acc: 87.0000%(56/64)\n",
      "Batch[573] - loss: 0.281461  acc: 85.0000%(55/64)\n",
      "Batch[574] - loss: 0.376391  acc: 82.0000%(53/64)\n",
      "Batch[575] - loss: 0.416404  acc: 79.0000%(51/64)\n",
      "Batch[576] - loss: 0.295866  acc: 87.0000%(56/64)\n",
      "Batch[577] - loss: 0.298110  acc: 84.0000%(54/64)\n",
      "Batch[578] - loss: 0.292857  acc: 92.0000%(59/64)\n",
      "Batch[579] - loss: 0.482988  acc: 78.0000%(50/64)\n",
      "Batch[580] - loss: 0.273388  acc: 90.0000%(58/64)\n",
      "Batch[581] - loss: 0.295175  acc: 87.0000%(56/64)\n",
      "Batch[582] - loss: 0.361615  acc: 89.0000%(57/64)\n",
      "Batch[583] - loss: 0.269433  acc: 89.0000%(57/64)\n",
      "Batch[584] - loss: 0.422704  acc: 78.0000%(50/64)\n",
      "Batch[585] - loss: 0.344982  acc: 79.0000%(51/64)\n",
      "Batch[586] - loss: 0.157808  acc: 92.0000%(59/64)\n",
      "Batch[587] - loss: 0.298100  acc: 85.0000%(55/64)\n",
      "Batch[588] - loss: 0.326481  acc: 81.0000%(52/64)\n",
      "Batch[589] - loss: 0.244767  acc: 90.0000%(58/64)\n",
      "Batch[590] - loss: 0.358031  acc: 82.0000%(53/64)\n",
      "Batch[591] - loss: 0.502389  acc: 75.0000%(48/64)\n",
      "Batch[592] - loss: 0.239026  acc: 90.0000%(58/64)\n",
      "Batch[593] - loss: 0.448759  acc: 79.0000%(51/64)\n",
      "Batch[594] - loss: 0.343075  acc: 84.0000%(54/64)\n",
      "Batch[595] - loss: 0.391704  acc: 84.0000%(54/64)\n",
      "Batch[596] - loss: 0.363853  acc: 79.0000%(51/64)\n",
      "Batch[597] - loss: 0.412635  acc: 84.0000%(54/64)\n",
      "Batch[598] - loss: 0.306086  acc: 89.0000%(57/64)\n",
      "Batch[599] - loss: 0.247568  acc: 90.0000%(58/64)\n",
      "Batch[600] - loss: 0.305632  acc: 81.0000%(13/16)\n",
      "Evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 166/208 (79%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train(classifier, train_loader, test_loader)\n",
    "except KeyboardInterrupt:\n",
    "    print('\\n' + '-' * 89)\n",
    "    print('Exiting from training early')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
