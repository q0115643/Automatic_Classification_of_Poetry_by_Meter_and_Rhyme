{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'create_dico'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-89babaea6454>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# met some error while importing utils, it is related to my computer's directory problem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_dico\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_mapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_digits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0miob2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miob_iobes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'create_dico'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "# for data loader\n",
    "from __future__ import print_function, division\n",
    "import os\n",
    "import re\n",
    "import codecs\n",
    "import unicodedata\n",
    "\n",
    "# met some error while importing utils, it is related to my computer's directory problem\n",
    "from utils import create_dico, create_mapping, zero_digits\n",
    "from utils import iob2, iob_iobes\n",
    "import model\n",
    "import string\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words are modeled with **a character-based RNN with LSTM**, which produces two vectors.\n",
    "\n",
    "The forward vector will have a representation of the character sequence from the left to the right.\n",
    "\n",
    "The backward one will have the same in the reversed order.\n",
    "\n",
    "Our insight is that this character-based LSTM captures the phonological structure of the word from its graphemes/characters.\n",
    "\n",
    "These two vectors are concatenated together with the whole **wordâ€™s embedding**\n",
    "(the embeddings could be pre-trained from larger corpora or trained jointly for the task).\n",
    "\n",
    "The vector of these three elements will represent each word in the sequence. Then, for each word, there will be a **word-level LSTM**, which will produce an output for each word, with its right and left context information.\n",
    "\n",
    "Finally, this output will go through a **CRF layer** to get the optimal output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image_ref/BiRNN_before_CRF.png\">\n",
    "\n",
    "BiRNN-CRF structure concating with pre-trainned word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Util functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TAG = '<START>'\n",
    "STOP_TAG = '<STOP>'\n",
    "\n",
    "def to_scalar(var):\n",
    "    return var.view(-1).data.tolist()[0]\n",
    "\n",
    "\n",
    "def argmax(vec):\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return to_scalar(idx)\n",
    "\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return Variable(tensor)\n",
    "\n",
    "\n",
    "def log_sum_exp(vec):\n",
    "    # vec 2D: 1 * tagset_size\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + \\\n",
    "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "def init_embedding(input_embedding):\n",
    "    \"\"\"\n",
    "    Initialize embedding\n",
    "    \"\"\"\n",
    "    bias = np.sqrt(3.0 / input_embedding.size(1))\n",
    "    nn.init.uniform_(input_embedding, -bias, bias)\n",
    "    # torch.nn.init.uniform_(tensor, a=0, b=1)\n",
    "    # fills the input Tensor with values drawn form the uniform distribution U(a, b)\n",
    "    \n",
    "def init_lstm(input_lstm):\n",
    "    \"\"\"\n",
    "    Initialize lstm\n",
    "    \"\"\"\n",
    "    for ind in range(0, input_lstm.num_layers):\n",
    "        weight = eval('input_lstm.weight_ih_l' + str(ind))\n",
    "        bias = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "        nn.init.uniform(weight, -bias, bias)\n",
    "        weight = eval('input_lstm.weight_hh_l' + str(ind))\n",
    "        bias = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "        nn.init.uniform(weight, -bias, bias)\n",
    "    if input_lstm.bidirectional:  # add reverse params\n",
    "        for ind in range(0, input_lstm.num_layers):\n",
    "            weight = eval('input_lstm.weight_ih_l' + str(ind) + '_reverse')\n",
    "            bias = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "            nn.init.uniform(weight, -bias, bias)\n",
    "            weight = eval('input_lstm.weight_hh_l' + str(ind) + '_reverse')\n",
    "            bias = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "            nn.init.uniform(weight, -bias, bias)\n",
    "\n",
    "    if input_lstm.bias:\n",
    "        for ind in range(0, input_lstm.num_layers):\n",
    "            weight = eval('input_lstm.bias_ih_l' + str(ind))\n",
    "            weight.data.zero_()\n",
    "            weight.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
    "            weight = eval('input_lstm.bias_hh_l' + str(ind))\n",
    "            weight.data.zero_()\n",
    "            weight.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
    "        if input_lstm.bidirectional:\n",
    "            for ind in range(0, input_lstm.num_layers):\n",
    "                weight = eval('input_lstm.bias_ih_l' + str(ind) + '_reverse')\n",
    "                weight.data.zero_()\n",
    "                weight.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
    "                weight = eval('input_lstm.bias_hh_l' + str(ind) + '_reverse')\n",
    "                weight.data.zero_()\n",
    "                weight.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the dataset\n",
    "# \n",
    "# trying to figure out the behaviors of function in blew link\n",
    "# https://github.com/ZhixiuYe/NER-pytorch/blob/master/loader.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build the Model**\n",
    "\n",
    "- for character embedding, initialize lookup table\n",
    "- train them by BiRNN with LSTM or CNN\n",
    "\n",
    "- and concatenate character-level representation with pre-trained word-level representation\n",
    "- pre-word lookup-table <= initialize using word2vec like method.\n",
    "- UNK embedding if not in lookup-table\n",
    "- UNK embedding is trained with singletons with probability 0.5\n",
    "\n",
    "- add drop-out to after concanate\n",
    "\n",
    "- done building our embeddings\n",
    "\n",
    "- use new LSTM-CRF model to generate the target sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tage_to_ix, embedding_dim, hidden_dim, char_lstm_dim=25,\n",
    "                      char_to_ix=None, pre_word_embeds=None, char_embedding_dim=25, use_gpu=False,\n",
    "                      n_cap=None, cap_embedding_dim=None, use_crf=True, char_mode='CNN'):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.use_gpu = use_gpu\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.n_cap = n_cap\n",
    "        self.cap_embedding_dim = cap_embedding_dim\n",
    "        self.use_crf = use_crf\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "        self.out_channels = char_lstm_dim  #  #of output channels\n",
    "        self.char_mode = char_mode\n",
    "        # char_mode: CNN or LSTM\n",
    "        # char_lstm_dim: #of output channels\n",
    "        # hidden_dim: #of hidden dimensions\n",
    "        print('char_mode: %s, out_channels: %d, hidden_dim: %d, ' % (char_mode, char_lstm_dim, hidden_dim))\n",
    "        \n",
    "        if self.n_cap and self.cap_embedding_dim:\n",
    "            self.cap_embeds = nn.Embedding(self.n_cap, self.cap_embedding_dim)\n",
    "            # torch.nn.Embedding(num_embeddings, embedding_dim, ...)\n",
    "            # lookup table that stores embeddings of a fixed dictionary and size\n",
    "            # num_embeddings: size of the dictionary of embeddings\n",
    "            # embedding_dim: the size of each embedding vector\n",
    "            init_embedding(self.cap_embeds.weight)  # embeddings for cap initialize\n",
    "        \n",
    "        if char_embedding_dim is not None:\n",
    "            self.char_lstm_dim = char_lstm_dim\n",
    "            self.char_embeds = nn.Embedding(len(char_to_ix), char_embedding_dim)\n",
    "            init_embedding(self.char_embeds.weight)\n",
    "            # embeddings for chars initialize\n",
    "            if self.char_mode == 'LSTM':\n",
    "                self.char_lstm = nn.LSTM(char_embedding_dim, char_lstm_dim, num_layers=1, bidirectional=True)\n",
    "                init_lstm(self.char_lstm)  # initialize LSTM\n",
    "            if self.char_mode == 'CNN':\n",
    "                self.char_cnn3 = nn.Conv2d(in_channels=1, out_channels=self.out_channels, kernel_size=(3, char_embedding_dim), padding=(2, 0))\n",
    "        \n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if pre_word_embeds is not None:\n",
    "            self.pre_word_embeds = True\n",
    "            self.word_embeds.weight = nn.Parameter(torch.FloatTensor(pre_word_embeds))\n",
    "            # torch.nn.Parmeter(data)\n",
    "            # Tensor that is to be considered a module parameter\n",
    "            # data: parameter tensor\n",
    "            # requires_grad(bool, optional): Default: True\n",
    "        else:\n",
    "            self.pre_word_embeds = False\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5) # Dropout layer\n",
    "        # torch.nn.Dropout(p=0.5, inplace=False)\n",
    "        # p: probability of an element to be zeroed\n",
    "        # inplace: if set to True, will do this operation in-place. Default: False\n",
    "        \n",
    "        # until here was making the embedding for words (character embedding + pre-word embedding)\n",
    "        ###############################################################################\n",
    "        # from here is the BiRNN-CRF model using the embeddings above\n",
    "        \n",
    "        # BiRNN or CNN\n",
    "        if self.n_cap and self.cap_embedding_dim:\n",
    "            if self.char_mode == 'LSTM':\n",
    "                self.lstm = nn.LSTM(embedding_dim+char_lstm_dim*2+cap_embedding_dim, hidden_dim, bidirectional=True)\n",
    "            if self.char_mode == 'CNN':\n",
    "                self.lstm = nn.LSTM(embedding_dim+self.out_channels+cap_embedding_dim, hidden_dim, bidirectional=True)\n",
    "        else:\n",
    "            if self.char_mode == 'LSTM':\n",
    "                self.lstm = nn.LSTM(embedding_dim+char_lstm_dim*2, hidden_dim, bidirectional=True)\n",
    "            if self.char_mode == 'CNN':\n",
    "                self.lstm = nn.LSTM(embedding_dim+self.out_channels, hidden_dim, bidirectional=True)\n",
    "        init_lstm(self.lstm)\n",
    "        self.hw_trans = nn.Linear(self.out_channels, self.out_channels)\n",
    "        self.hw_gate = nn.Linear(self.out_channels, self.out_channels)\n",
    "        self.h2_h1 = nn.Linear(hidden_dim*2, hidden_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)\n",
    "        init_linear(self.h2_h1)\n",
    "        init_linear(self.hidden2tag)\n",
    "        init_linear(self.hw_gate)\n",
    "        init_linear(self.hw_trans)\n",
    "        \n",
    "        # CRF\n",
    "        if self.use_crf:\n",
    "            sef.transitions = nn.Parameter(\n",
    "                torch.zeros(self.tagset_size, self.target_size))\n",
    "            self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "            self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "        \n",
    "        # model done\n",
    "        #############################################################################\n",
    "        # from here I'm trying to figure out the specific roles of codes while following on original code.\n",
    "        #\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # tags is ground_truth, a list of ints, length is len(sentence)\n",
    "        # feats is a 2D tensor, len(sentence) * tagset_size\n",
    "        r = torch.LongTensor(range(feats.size()[0])) # torch.tensor with 64-bit integer (signed)\n",
    "        if self.use_gpu:\n",
    "            r = r.cuda()\n",
    "            pad_start_tags = torch.cat([torch.cuda.LongTensor([self.tag_to_ix[START_TAG]]), tags])\n",
    "            pad_stop_tags = torch.cat([tags, torch.cuda.LongTensor([self.tag_to_ix[STOP_TAG]])])\n",
    "        else:\n",
    "            pad_start_tags = torch.cat([torch.LongTensor([self.tag_to_ix[START_TAG]]), tags])\n",
    "            pad_stop_tags = torch.cat([tags, torch.LongTensor([self.tag_to_ix[STOP_TAG]])])\n",
    "\n",
    "        score = torch.sum(self.transitions[pad_stop_tags, pad_start_tags]) + torch.sum(feats[r, tags])\n",
    "        return score\n",
    "\n",
    "    def _get_lstm_features(self, sentence, chars2, caps, chars2_length, d):\n",
    "\n",
    "        if self.char_mode == 'LSTM':\n",
    "            # self.char_lstm_hidden = self.init_lstm_hidden(dim=self.char_lstm_dim, bidirection=True, batchsize=chars2.size(0))\n",
    "            chars_embeds = self.char_embeds(chars2).transpose(0, 1)\n",
    "            packed = torch.nn.utils.rnn.pack_padded_sequence(chars_embeds, chars2_length)\n",
    "            lstm_out, _ = self.char_lstm(packed)\n",
    "            outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(lstm_out)\n",
    "            outputs = outputs.transpose(0, 1)\n",
    "            chars_embeds_temp = Variable(torch.FloatTensor(torch.zeros((outputs.size(0), outputs.size(2)))))\n",
    "            if self.use_gpu:\n",
    "                chars_embeds_temp = chars_embeds_temp.cuda()\n",
    "            for i, index in enumerate(output_lengths):\n",
    "                chars_embeds_temp[i] = torch.cat((outputs[i, index-1, :self.char_lstm_dim], outputs[i, 0, self.char_lstm_dim:]))\n",
    "            chars_embeds = chars_embeds_temp.clone()\n",
    "            for i in range(chars_embeds.size(0)):\n",
    "                chars_embeds[d[i]] = chars_embeds_temp[i]\n",
    "\n",
    "        if self.char_mode == 'CNN':\n",
    "            chars_embeds = self.char_embeds(chars2).unsqueeze(1)\n",
    "            chars_cnn_out3 = self.char_cnn3(chars_embeds)\n",
    "            chars_embeds = nn.functional.max_pool2d(chars_cnn_out3,\n",
    "                                                 kernel_size=(chars_cnn_out3.size(2), 1)).view(chars_cnn_out3.size(0), self.out_channels)\n",
    "\n",
    "        # t = self.hw_gate(chars_embeds)\n",
    "        # g = nn.functional.sigmoid(t)\n",
    "        # h = nn.functional.relu(self.hw_trans(chars_embeds))\n",
    "        # chars_embeds = g * h + (1 - g) * chars_embeds\n",
    "\n",
    "        embeds = self.word_embeds(sentence)\n",
    "        if self.n_cap and self.cap_embedding_dim:\n",
    "            cap_embedding = self.cap_embeds(caps)\n",
    "\n",
    "        if self.n_cap and self.cap_embedding_dim:\n",
    "            embeds = torch.cat((embeds, chars_embeds, cap_embedding), 1)\n",
    "        else:\n",
    "            embeds = torch.cat((embeds, chars_embeds), 1)\n",
    "\n",
    "        embeds = embeds.unsqueeze(1)\n",
    "        embeds = self.dropout(embeds)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim*2)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        # calculate in log domain\n",
    "        # feats is len(sentence) * tagset_size\n",
    "        # initialize alpha with a Tensor with values all equal to -10000.\n",
    "        init_alphas = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "        forward_var = autograd.Variable(init_alphas)\n",
    "        if self.use_gpu:\n",
    "            forward_var = forward_var.cuda()\n",
    "        for feat in feats:\n",
    "            emit_score = feat.view(-1, 1)\n",
    "            tag_var = forward_var + self.transitions + emit_score\n",
    "            max_tag_var, _ = torch.max(tag_var, dim=1)\n",
    "            tag_var = tag_var - max_tag_var.view(-1, 1)\n",
    "            forward_var = max_tag_var + torch.log(torch.sum(torch.exp(tag_var), dim=1)).view(1, -1) # ).view(1, -1)\n",
    "        terminal_var = (forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]).view(1, -1)\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        # Z(x)\n",
    "        return alpha\n",
    "\n",
    "    def viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "        # analogous to forward\n",
    "        init_vvars = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "        forward_var = Variable(init_vvars)\n",
    "        if self.use_gpu:\n",
    "            forward_var = forward_var.cuda()\n",
    "        for feat in feats:\n",
    "            next_tag_var = forward_var.view(1, -1).expand(self.tagset_size, self.tagset_size) + self.transitions\n",
    "            _, bptrs_t = torch.max(next_tag_var, dim=1)\n",
    "            bptrs_t = bptrs_t.squeeze().data.cpu().numpy()\n",
    "            next_tag_var = next_tag_var.data.cpu().numpy()\n",
    "            viterbivars_t = next_tag_var[range(len(bptrs_t)), bptrs_t]\n",
    "            viterbivars_t = Variable(torch.FloatTensor(viterbivars_t))\n",
    "            if self.use_gpu:\n",
    "                viterbivars_t = viterbivars_t.cuda()\n",
    "            forward_var = viterbivars_t + feat\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        terminal_var.data[self.tag_to_ix[STOP_TAG]] = -10000.\n",
    "        terminal_var.data[self.tag_to_ix[START_TAG]] = -10000.\n",
    "        best_tag_id = argmax(terminal_var.unsqueeze(0))\n",
    "        path_score = terminal_var[best_tag_id]\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags, chars2, caps, chars2_length, d):\n",
    "        # sentence, tags is a list of ints\n",
    "        # features is a 2D tensor, len(sentence) * self.tagset_size\n",
    "        feats = self._get_lstm_features(sentence, chars2, caps, chars2_length, d)\n",
    "\n",
    "        if self.use_crf:\n",
    "            forward_score = self._forward_alg(feats)\n",
    "            gold_score = self._score_sentence(feats, tags)\n",
    "            return forward_score - gold_score\n",
    "        else:\n",
    "            tags = Variable(tags)\n",
    "            scores = nn.functional.cross_entropy(feats, tags)\n",
    "            return scores\n",
    "\n",
    "    def forward(self, sentence, chars, caps, chars2_length, d):\n",
    "        feats = self._get_lstm_features(sentence, chars, caps, chars2_length, d)\n",
    "        # viterbi to get tag_seq\n",
    "        if self.use_crf:\n",
    "            score, tag_seq = self.viterbi_decode(feats)\n",
    "        else:\n",
    "            score, tag_seq = torch.max(feats, 1)\n",
    "            tag_seq = list(tag_seq.cpu().data)\n",
    "\n",
    "        return score, tag_seq\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
