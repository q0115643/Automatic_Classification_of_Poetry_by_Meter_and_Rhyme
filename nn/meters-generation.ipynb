{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_embedding(input_embedding):\n",
    "    \"\"\"\n",
    "    Initialize embedding\n",
    "    \"\"\"\n",
    "    bias = np.sqrt(3.0 / input_embedding.size(1))\n",
    "    nn.init.uniform(input_embedding, -bias, bias)\n",
    "    \n",
    "def init_lstm(input_lstm):\n",
    "    \"\"\"\n",
    "    Initialize lstm\n",
    "    \"\"\"\n",
    "    for ind in range(0, input_lstm.num_layers):\n",
    "        weight = eval('input_lstm.weight_ih_l' + str(ind))\n",
    "        bias = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "        nn.init.uniform(weight, -bias, bias)\n",
    "        weight = eval('input_lstm.weight_hh_l' + str(ind))\n",
    "        bias = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "        nn.init.uniform(weight, -bias, bias)\n",
    "    if input_lstm.bidirectional:\n",
    "        for ind in range(0, input_lstm.num_layers):\n",
    "            weight = eval('input_lstm.weight_ih_l' + str(ind) + '_reverse')\n",
    "            bias = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "            nn.init.uniform(weight, -bias, bias)\n",
    "            weight = eval('input_lstm.weight_hh_l' + str(ind) + '_reverse')\n",
    "            bias = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "            nn.init.uniform(weight, -bias, bias)\n",
    "\n",
    "    if input_lstm.bias:\n",
    "        for ind in range(0, input_lstm.num_layers):\n",
    "            weight = eval('input_lstm.bias_ih_l' + str(ind))\n",
    "            weight.data.zero_()\n",
    "            weight.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
    "            weight = eval('input_lstm.bias_hh_l' + str(ind))\n",
    "            weight.data.zero_()\n",
    "            weight.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
    "        if input_lstm.bidirectional:\n",
    "            for ind in range(0, input_lstm.num_layers):\n",
    "                weight = eval('input_lstm.bias_ih_l' + str(ind) + '_reverse')\n",
    "                weight.data.zero_()\n",
    "                weight.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
    "                weight = eval('input_lstm.bias_hh_l' + str(ind) + '_reverse')\n",
    "                weight.data.zero_()\n",
    "                weight.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Build the Model\n",
    "\n",
    "\n",
    "\n",
    "Words are modeled with **a character-based RNN with LSTM**, which produces two vectors.\n",
    "\n",
    "The forward vector will have a representation of the character sequence from the left to the right.\n",
    "\n",
    "The backward one will have the same in the reversed order.\n",
    "\n",
    "Our insight is that this character-based LSTM captures the phonological structure of the word from its graphemes/characters.\n",
    "\n",
    "These two vectors are concatenated together with the whole **wordâ€™s embedding**\n",
    "(the embeddings could be pre-trained from larger corpora or trained jointly for the task).\n",
    "\n",
    "The vector of these three elements will represent each word in the sequence. Then, for each word, there will be a **word-level LSTM**, which will produce an output for each word, with its right and left context information.\n",
    "\n",
    "Finally, this output will go through a **CRF layer** to get the optimal output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image_ref/BiRNN_before_CRF.png\">\n",
    "\n",
    "BiRNN-CRF structure concating with pre-trainned word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tage_to_ix, embedding_dim, hidden_dim, char_lstm_dim=25,\n",
    "                      char_to_ix=None, pre_word_embeds=None, char_embedding_dim=25, use_gpu=False,\n",
    "                      n_cap=None, cap_embedding_dim=None, use_crf=True, char_mode='CNN'):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.use_gpu = use_gpu\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.n_cap = n_cap\n",
    "        self.cap_embedding_dim = cap_embedding_dim\n",
    "        self.use_crf = use_crf\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "        self.out_channels = char_lstm_dim #  #output channels\n",
    "        self.char_mode = char_mode\n",
    "        \n",
    "        print('char_mode: %s, out_channels: %d, hidden_dim: %d, ' % (char_mode, char_lstm_dim, hidden_dim))\n",
    "        \n",
    "        if self.n_cap and self.cap_embedding_dim:\n",
    "            self.cap_embeds = nn.Embedding(self.n_cap, self.cap_embedding_dim)\n",
    "            init_embedding(self.cap_embeds.weight)\n",
    "        \n",
    "        if char_embedding_dim is not None:\n",
    "            self.char_lstm_dim = char_lstm_dim\n",
    "            self.char_embeds = nn.Embedding(len(char_to_ix), char_embedding_dim)\n",
    "            init_embedding(self.char_embeds.weight)\n",
    "            if self.char_mode == 'LSTM':\n",
    "                self.char_lstm = nn.LSTM(char_embedding_dim, char_lstm_dim, num_layers=1, bidirectional=True)\n",
    "                init_lstm(self.char_lstm)\n",
    "            if self.char_mode == 'CNN':\n",
    "                self.char_cnn3 = nn.Conv2d(in_channels=1, out_channels=self.out_channels, kernel_size=(3, char_embedding_dim), padding=(2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
