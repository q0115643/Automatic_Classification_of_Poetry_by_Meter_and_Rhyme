{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 100\n",
    "N_LAYERS = 2\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "class Mydataset(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        if train:\n",
    "            print('loading trainig dataset')\n",
    "            file = 'nn_train_poetry.csv'\n",
    "        else:\n",
    "            print('loading testing dataset')\n",
    "            file = 'nn_test_poetry.csv'\n",
    "        with open(file, 'r', newline='', encoding='utf-8') as myFile:  \n",
    "            rdr = csv.reader(myFile)\n",
    "            temp = list(rdr)\n",
    "            self.x = [a[0][0:200] for a in temp]\n",
    "            self.y = [a[1] for a in temp]\n",
    "        self.len = len(self.x)\n",
    "        self.labels = list(sorted(set(self.y)))\n",
    "        self.alphabet = list(sorted(set(['+','-','\\t'])))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "        \n",
    "    def get_labels(self):\n",
    "        return self.labels\n",
    "    \n",
    "    def get_alphabet(self):\n",
    "        return self.alphabet\n",
    "    \n",
    "    def get_alphabet_id(self, c):\n",
    "        return self.alphabet.index(c)\n",
    "    \n",
    "    def get_label(self, id):\n",
    "        return self.labels[id]\n",
    "    \n",
    "    def get_label_id(self, label):\n",
    "        return self.labels.index(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading trainig dataset\n",
      "loading testing dataset\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Mydataset()\n",
    "test_dataset = Mydataset(train=False)\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alphabet size: 3\n",
      "class size: 2\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "ALL_LETTERS = train_dataset.get_alphabet()\n",
    "N_CHARS = len(ALL_LETTERS)\n",
    "N_LABELS = len(train_dataset.get_labels())\n",
    "print('alphabet size: {}\\nclass size: {}'.format(N_CHARS, N_LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "def create_variable(tensor):\n",
    "    return Variable(tensor)\n",
    "\n",
    "def make_variables(inputs, labels):\n",
    "    sequence_and_length = [str2ascii_arr(input) for input in inputs]\n",
    "    vectorized_seqs = [sl[0] for sl in sequence_and_length]\n",
    "    seq_lengths = torch.LongTensor([sl[1] for sl in sequence_and_length])\n",
    "    return pad_sequences(vectorized_seqs, seq_lengths, labels)\n",
    "\n",
    "def str2ascii_arr(msg):\n",
    "    arr = [ALL_LETTERS.index(c) for c in msg]\n",
    "    return arr, len(arr)\n",
    "\n",
    "def pad_sequences(vectorized_seqs, seq_lengths, labels):\n",
    "    seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long()\n",
    "    for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
    "    seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "    seq_tensor = seq_tensor[perm_idx]\n",
    "    target = labels2tensor(labels)\n",
    "    if len(labels):\n",
    "        target = target[perm_idx]\n",
    "    return create_variable(seq_tensor), \\\n",
    "        create_variable(seq_lengths), \\\n",
    "        create_variable(target)\n",
    "        \n",
    "def labels2tensor(labels):\n",
    "    label_ids = [train_dataset.get_label_id(\n",
    "        label) for label in labels]\n",
    "    return torch.LongTensor(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirectional=True):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_directions = int(bidirectional) + 1\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          bidirectional=bidirectional)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, input, seq_lengths):\n",
    "        input = input.t()\n",
    "        batch_size = input.size(1)\n",
    "        hidden = self._init_hidden(batch_size)\n",
    "        embedded = self.embedding(input)\n",
    "        gru_input = pack_padded_sequence(\n",
    "            embedded, seq_lengths.data.cpu().numpy())\n",
    "        self.gru.flatten_parameters()\n",
    "        output, hidden = self.gru(gru_input, hidden)\n",
    "        fc_output = self.fc(hidden[-1])\n",
    "        return fc_output\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers * self.n_directions,\n",
    "                             batch_size, self.hidden_size)\n",
    "        return create_variable(hidden)\n",
    "    \n",
    "classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_LABELS, N_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "log_interval = 1\n",
    "test_interval = 100\n",
    "\n",
    "def train(model, train_loader, test_loader):\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "    model.train()\n",
    "    steps = 0\n",
    "    for epoch in range(1, N_EPOCHS+1):\n",
    "        batch = 0\n",
    "        for inputs, label in train_loader:\n",
    "            input, seq_lengths, target = make_variables(inputs, label)\n",
    "            optimizer.zero_grad()\n",
    "            output = classifier(input, seq_lengths)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            steps += 1\n",
    "            if steps % log_interval == 0:\n",
    "                corrects = (torch.max(output, 1)[1].view(target.size()).data == target.data).sum()\n",
    "                accuracy = 100.0 * corrects/target.shape[0]\n",
    "                print(\n",
    "                    '\\rBatch[{}] - loss: {:.6f}  acc: {:.4f}%({}/{})'.format(steps,\n",
    "                                                                             loss.data[0], \n",
    "                                                                             accuracy,\n",
    "                                                                             corrects,\n",
    "                                                                             target.shape[0]))\n",
    "            if steps % test_interval == 0:\n",
    "                eval(test_loader, model)\n",
    "\n",
    "def eval(test_loader, classifier):\n",
    "    classifier.eval()\n",
    "    print(\"Evaluating trained model ...\")\n",
    "    correct = 0\n",
    "    train_data_size = len(test_loader.dataset)\n",
    "    for inputs, labels in test_loader:\n",
    "        input, seq_lengths, target = make_variables(inputs, labels)\n",
    "        output = classifier(input, seq_lengths)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, train_data_size, 100. * correct / train_data_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\settings\\anaconda\\lib\\site-packages\\ipykernel\\__main__.py:24: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch[1] - loss: 0.692477  acc: 53.0000%(34/64)\n",
      "Batch[2] - loss: 0.672633  acc: 60.0000%(39/64)\n",
      "Batch[3] - loss: 0.817655  acc: 40.0000%(26/64)\n",
      "Batch[4] - loss: 0.696524  acc: 46.0000%(30/64)\n",
      "Batch[5] - loss: 0.628618  acc: 68.0000%(44/64)\n",
      "Batch[6] - loss: 0.770143  acc: 56.0000%(36/64)\n",
      "Batch[7] - loss: 0.975626  acc: 42.0000%(27/64)\n",
      "Batch[8] - loss: 0.732906  acc: 56.0000%(36/64)\n",
      "Batch[9] - loss: 0.729844  acc: 53.0000%(34/64)\n",
      "Batch[10] - loss: 0.676564  acc: 57.0000%(37/64)\n",
      "Batch[11] - loss: 0.694620  acc: 50.0000%(32/64)\n",
      "Batch[12] - loss: 0.693218  acc: 57.0000%(37/64)\n",
      "Batch[13] - loss: 0.694245  acc: 54.0000%(35/64)\n",
      "Batch[14] - loss: 0.676887  acc: 56.0000%(36/64)\n",
      "Batch[15] - loss: 0.711457  acc: 48.0000%(31/64)\n",
      "Batch[16] - loss: 0.730607  acc: 46.0000%(30/64)\n",
      "Batch[17] - loss: 0.680972  acc: 56.0000%(36/64)\n",
      "Batch[18] - loss: 0.666372  acc: 59.0000%(38/64)\n",
      "Batch[19] - loss: 0.697053  acc: 51.0000%(33/64)\n",
      "Batch[20] - loss: 0.690008  acc: 54.0000%(35/64)\n",
      "Batch[21] - loss: 0.682348  acc: 54.0000%(35/64)\n",
      "Batch[22] - loss: 0.673140  acc: 70.0000%(45/64)\n",
      "Batch[23] - loss: 0.684887  acc: 70.0000%(45/64)\n",
      "Batch[24] - loss: 0.686803  acc: 57.0000%(37/64)\n",
      "Batch[25] - loss: 0.684534  acc: 67.0000%(43/64)\n",
      "Batch[26] - loss: 0.700392  acc: 59.0000%(38/64)\n",
      "Batch[27] - loss: 0.678101  acc: 68.0000%(44/64)\n",
      "Batch[28] - loss: 0.661755  acc: 73.0000%(47/64)\n",
      "Batch[29] - loss: 0.677514  acc: 62.0000%(40/64)\n",
      "Batch[30] - loss: 0.648816  acc: 75.0000%(12/16)\n",
      "Batch[31] - loss: 0.665838  acc: 62.0000%(40/64)\n",
      "Batch[32] - loss: 0.685736  acc: 54.0000%(35/64)\n",
      "Batch[33] - loss: 0.702258  acc: 50.0000%(32/64)\n",
      "Batch[34] - loss: 0.683559  acc: 57.0000%(37/64)\n",
      "Batch[35] - loss: 0.688759  acc: 53.0000%(34/64)\n",
      "Batch[36] - loss: 0.706734  acc: 50.0000%(32/64)\n",
      "Batch[37] - loss: 0.679972  acc: 57.0000%(37/64)\n",
      "Batch[38] - loss: 0.677029  acc: 64.0000%(41/64)\n",
      "Batch[39] - loss: 0.691500  acc: 60.0000%(39/64)\n",
      "Batch[40] - loss: 0.690968  acc: 48.0000%(31/64)\n",
      "Batch[41] - loss: 0.712203  acc: 48.0000%(31/64)\n",
      "Batch[42] - loss: 0.719119  acc: 45.0000%(29/64)\n",
      "Batch[43] - loss: 0.689344  acc: 51.0000%(33/64)\n",
      "Batch[44] - loss: 0.675175  acc: 45.0000%(29/64)\n",
      "Batch[45] - loss: 0.653756  acc: 64.0000%(41/64)\n",
      "Batch[46] - loss: 0.654384  acc: 71.0000%(46/64)\n",
      "Batch[47] - loss: 0.671145  acc: 60.0000%(39/64)\n",
      "Batch[48] - loss: 0.649873  acc: 60.0000%(39/64)\n",
      "Batch[49] - loss: 0.693360  acc: 56.0000%(36/64)\n",
      "Batch[50] - loss: 0.749737  acc: 43.0000%(28/64)\n",
      "Batch[51] - loss: 0.678955  acc: 57.0000%(37/64)\n",
      "Batch[52] - loss: 0.731383  acc: 51.0000%(33/64)\n",
      "Batch[53] - loss: 0.696581  acc: 51.0000%(33/64)\n",
      "Batch[54] - loss: 0.659594  acc: 70.0000%(45/64)\n",
      "Batch[55] - loss: 0.682347  acc: 67.0000%(43/64)\n",
      "Batch[56] - loss: 0.650393  acc: 68.0000%(44/64)\n",
      "Batch[57] - loss: 0.645215  acc: 57.0000%(37/64)\n",
      "Batch[58] - loss: 0.704453  acc: 40.0000%(26/64)\n",
      "Batch[59] - loss: 0.645715  acc: 46.0000%(30/64)\n",
      "Batch[60] - loss: 0.652420  acc: 75.0000%(12/16)\n",
      "Batch[61] - loss: 0.687692  acc: 45.0000%(29/64)\n",
      "Batch[62] - loss: 0.660645  acc: 54.0000%(35/64)\n",
      "Batch[63] - loss: 0.658982  acc: 60.0000%(39/64)\n",
      "Batch[64] - loss: 0.614333  acc: 68.0000%(44/64)\n",
      "Batch[65] - loss: 0.683295  acc: 57.0000%(37/64)\n",
      "Batch[66] - loss: 0.642365  acc: 67.0000%(43/64)\n",
      "Batch[67] - loss: 0.664116  acc: 67.0000%(43/64)\n",
      "Batch[68] - loss: 0.627730  acc: 65.0000%(42/64)\n",
      "Batch[69] - loss: 0.664215  acc: 60.0000%(39/64)\n",
      "Batch[70] - loss: 0.629168  acc: 71.0000%(46/64)\n",
      "Batch[71] - loss: 0.660153  acc: 62.0000%(40/64)\n",
      "Batch[72] - loss: 0.704219  acc: 50.0000%(32/64)\n",
      "Batch[73] - loss: 0.644452  acc: 71.0000%(46/64)\n",
      "Batch[74] - loss: 0.633246  acc: 70.0000%(45/64)\n",
      "Batch[75] - loss: 0.676064  acc: 65.0000%(42/64)\n",
      "Batch[76] - loss: 0.582216  acc: 76.0000%(49/64)\n",
      "Batch[77] - loss: 0.575970  acc: 73.0000%(47/64)\n",
      "Batch[78] - loss: 0.669434  acc: 70.0000%(45/64)\n",
      "Batch[79] - loss: 0.652990  acc: 71.0000%(46/64)\n",
      "Batch[80] - loss: 0.650636  acc: 65.0000%(42/64)\n",
      "Batch[81] - loss: 0.641673  acc: 67.0000%(43/64)\n",
      "Batch[82] - loss: 0.728702  acc: 56.0000%(36/64)\n",
      "Batch[83] - loss: 0.599614  acc: 68.0000%(44/64)\n",
      "Batch[84] - loss: 0.653393  acc: 64.0000%(41/64)\n",
      "Batch[85] - loss: 0.816905  acc: 48.0000%(31/64)\n",
      "Batch[86] - loss: 0.637435  acc: 67.0000%(43/64)\n",
      "Batch[87] - loss: 0.667809  acc: 62.0000%(40/64)\n",
      "Batch[88] - loss: 0.684751  acc: 60.0000%(39/64)\n",
      "Batch[89] - loss: 0.671754  acc: 64.0000%(41/64)\n",
      "Batch[90] - loss: 0.581727  acc: 81.0000%(13/16)\n",
      "Batch[91] - loss: 0.669118  acc: 59.0000%(38/64)\n",
      "Batch[92] - loss: 0.668404  acc: 64.0000%(41/64)\n",
      "Batch[93] - loss: 0.644914  acc: 65.0000%(42/64)\n",
      "Batch[94] - loss: 0.698117  acc: 59.0000%(38/64)\n",
      "Batch[95] - loss: 0.656052  acc: 65.0000%(42/64)\n",
      "Batch[96] - loss: 0.622999  acc: 65.0000%(42/64)\n",
      "Batch[97] - loss: 0.622880  acc: 68.0000%(44/64)\n",
      "Batch[98] - loss: 0.607239  acc: 70.0000%(45/64)\n",
      "Batch[99] - loss: 0.609904  acc: 65.0000%(42/64)\n",
      "Batch[100] - loss: 0.589919  acc: 71.0000%(46/64)\n",
      "Evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 140/208 (67%)\n",
      "\n",
      "Batch[101] - loss: 0.643068  acc: 65.0000%(42/64)\n",
      "Batch[102] - loss: 0.681369  acc: 59.0000%(38/64)\n",
      "Batch[103] - loss: 0.684224  acc: 59.0000%(38/64)\n",
      "Batch[104] - loss: 0.627704  acc: 65.0000%(42/64)\n",
      "Batch[105] - loss: 0.697177  acc: 60.0000%(39/64)\n",
      "Batch[106] - loss: 0.688782  acc: 54.0000%(35/64)\n",
      "Batch[107] - loss: 0.605239  acc: 65.0000%(42/64)\n",
      "Batch[108] - loss: 0.639948  acc: 62.0000%(40/64)\n",
      "Batch[109] - loss: 0.615579  acc: 67.0000%(43/64)\n",
      "Batch[110] - loss: 0.633438  acc: 57.0000%(37/64)\n",
      "Batch[111] - loss: 0.658160  acc: 62.0000%(40/64)\n",
      "Batch[112] - loss: 0.634978  acc: 65.0000%(42/64)\n",
      "Batch[113] - loss: 0.563502  acc: 81.0000%(52/64)\n",
      "Batch[114] - loss: 0.618765  acc: 65.0000%(42/64)\n",
      "Batch[115] - loss: 0.529404  acc: 75.0000%(48/64)\n",
      "Batch[116] - loss: 0.585104  acc: 64.0000%(41/64)\n",
      "Batch[117] - loss: 0.662785  acc: 71.0000%(46/64)\n",
      "Batch[118] - loss: 0.509011  acc: 79.0000%(51/64)\n",
      "Batch[119] - loss: 0.660386  acc: 64.0000%(41/64)\n",
      "Batch[120] - loss: 0.789681  acc: 50.0000%(8/16)\n",
      "Batch[121] - loss: 0.739880  acc: 51.0000%(33/64)\n",
      "Batch[122] - loss: 0.597113  acc: 76.0000%(49/64)\n",
      "Batch[123] - loss: 0.622463  acc: 71.0000%(46/64)\n",
      "Batch[124] - loss: 0.582095  acc: 75.0000%(48/64)\n",
      "Batch[125] - loss: 0.650713  acc: 62.0000%(40/64)\n",
      "Batch[126] - loss: 0.619387  acc: 64.0000%(41/64)\n",
      "Batch[127] - loss: 0.609100  acc: 75.0000%(48/64)\n",
      "Batch[128] - loss: 0.643228  acc: 70.0000%(45/64)\n",
      "Batch[129] - loss: 0.648613  acc: 60.0000%(39/64)\n",
      "Batch[130] - loss: 0.544542  acc: 79.0000%(51/64)\n",
      "Batch[131] - loss: 0.631110  acc: 70.0000%(45/64)\n",
      "Batch[132] - loss: 0.622847  acc: 62.0000%(40/64)\n",
      "Batch[133] - loss: 0.581767  acc: 73.0000%(47/64)\n",
      "Batch[134] - loss: 0.594137  acc: 71.0000%(46/64)\n",
      "Batch[135] - loss: 0.597267  acc: 71.0000%(46/64)\n",
      "Batch[136] - loss: 0.564766  acc: 70.0000%(45/64)\n",
      "Batch[137] - loss: 0.596095  acc: 68.0000%(44/64)\n",
      "Batch[138] - loss: 0.599694  acc: 70.0000%(45/64)\n",
      "Batch[139] - loss: 0.583766  acc: 70.0000%(45/64)\n",
      "Batch[140] - loss: 0.561736  acc: 73.0000%(47/64)\n",
      "Batch[141] - loss: 0.615929  acc: 62.0000%(40/64)\n",
      "Batch[142] - loss: 0.548169  acc: 76.0000%(49/64)\n",
      "Batch[143] - loss: 0.576909  acc: 71.0000%(46/64)\n",
      "Batch[144] - loss: 0.610991  acc: 68.0000%(44/64)\n",
      "Batch[145] - loss: 0.535627  acc: 76.0000%(49/64)\n",
      "Batch[146] - loss: 0.443465  acc: 84.0000%(54/64)\n",
      "Batch[147] - loss: 0.603270  acc: 70.0000%(45/64)\n",
      "Batch[148] - loss: 0.586316  acc: 71.0000%(46/64)\n",
      "Batch[149] - loss: 0.576116  acc: 78.0000%(50/64)\n",
      "Batch[150] - loss: 0.545212  acc: 62.0000%(10/16)\n",
      "Batch[151] - loss: 0.764652  acc: 54.0000%(35/64)\n",
      "Batch[152] - loss: 0.671599  acc: 67.0000%(43/64)\n",
      "Batch[153] - loss: 0.543236  acc: 71.0000%(46/64)\n",
      "Batch[154] - loss: 0.560996  acc: 75.0000%(48/64)\n",
      "Batch[155] - loss: 0.653231  acc: 70.0000%(45/64)\n",
      "Batch[156] - loss: 0.608553  acc: 71.0000%(46/64)\n",
      "Batch[157] - loss: 0.613482  acc: 65.0000%(42/64)\n",
      "Batch[158] - loss: 0.601395  acc: 68.0000%(44/64)\n",
      "Batch[159] - loss: 0.524673  acc: 81.0000%(52/64)\n",
      "Batch[160] - loss: 0.529241  acc: 79.0000%(51/64)\n",
      "Batch[161] - loss: 0.552321  acc: 76.0000%(49/64)\n",
      "Batch[162] - loss: 0.559939  acc: 76.0000%(49/64)\n",
      "Batch[163] - loss: 0.662537  acc: 64.0000%(41/64)\n",
      "Batch[164] - loss: 0.525147  acc: 75.0000%(48/64)\n",
      "Batch[165] - loss: 0.555269  acc: 73.0000%(47/64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch[166] - loss: 0.602369  acc: 70.0000%(45/64)\n",
      "Batch[167] - loss: 0.499577  acc: 78.0000%(50/64)\n",
      "Batch[168] - loss: 0.493970  acc: 76.0000%(49/64)\n",
      "Batch[169] - loss: 0.508791  acc: 79.0000%(51/64)\n",
      "Batch[170] - loss: 0.562518  acc: 73.0000%(47/64)\n",
      "Batch[171] - loss: 0.558234  acc: 70.0000%(45/64)\n",
      "Batch[172] - loss: 0.513959  acc: 82.0000%(53/64)\n",
      "Batch[173] - loss: 0.484478  acc: 76.0000%(49/64)\n",
      "Batch[174] - loss: 0.592575  acc: 75.0000%(48/64)\n",
      "Batch[175] - loss: 0.564011  acc: 71.0000%(46/64)\n",
      "Batch[176] - loss: 0.527070  acc: 75.0000%(48/64)\n",
      "Batch[177] - loss: 0.548400  acc: 73.0000%(47/64)\n",
      "Batch[178] - loss: 0.507420  acc: 78.0000%(50/64)\n",
      "Batch[179] - loss: 0.624671  acc: 64.0000%(41/64)\n",
      "Batch[180] - loss: 0.490966  acc: 75.0000%(12/16)\n",
      "Batch[181] - loss: 0.422290  acc: 81.0000%(52/64)\n",
      "Batch[182] - loss: 0.530989  acc: 76.0000%(49/64)\n",
      "Batch[183] - loss: 0.447628  acc: 79.0000%(51/64)\n",
      "Batch[184] - loss: 0.661369  acc: 59.0000%(38/64)\n",
      "Batch[185] - loss: 0.514285  acc: 76.0000%(49/64)\n",
      "Batch[186] - loss: 0.547341  acc: 73.0000%(47/64)\n",
      "Batch[187] - loss: 0.453617  acc: 76.0000%(49/64)\n",
      "Batch[188] - loss: 0.475739  acc: 73.0000%(47/64)\n",
      "Batch[189] - loss: 0.631838  acc: 70.0000%(45/64)\n",
      "Batch[190] - loss: 0.478861  acc: 75.0000%(48/64)\n",
      "Batch[191] - loss: 0.480438  acc: 75.0000%(48/64)\n",
      "Batch[192] - loss: 0.572085  acc: 67.0000%(43/64)\n",
      "Batch[193] - loss: 0.559063  acc: 76.0000%(49/64)\n",
      "Batch[194] - loss: 0.480932  acc: 79.0000%(51/64)\n",
      "Batch[195] - loss: 0.514336  acc: 76.0000%(49/64)\n",
      "Batch[196] - loss: 0.441877  acc: 81.0000%(52/64)\n",
      "Batch[197] - loss: 0.437641  acc: 79.0000%(51/64)\n",
      "Batch[198] - loss: 0.469564  acc: 76.0000%(49/64)\n",
      "Batch[199] - loss: 0.365363  acc: 87.0000%(56/64)\n",
      "Batch[200] - loss: 0.399870  acc: 81.0000%(52/64)\n",
      "Evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 154/208 (74%)\n",
      "\n",
      "Batch[201] - loss: 0.395364  acc: 82.0000%(53/64)\n",
      "Batch[202] - loss: 0.560309  acc: 71.0000%(46/64)\n",
      "Batch[203] - loss: 0.426407  acc: 82.0000%(53/64)\n",
      "Batch[204] - loss: 0.549254  acc: 70.0000%(45/64)\n",
      "Batch[205] - loss: 0.582623  acc: 67.0000%(43/64)\n",
      "Batch[206] - loss: 0.518055  acc: 78.0000%(50/64)\n",
      "Batch[207] - loss: 0.354110  acc: 85.0000%(55/64)\n",
      "Batch[208] - loss: 0.510953  acc: 73.0000%(47/64)\n",
      "Batch[209] - loss: 0.489889  acc: 73.0000%(47/64)\n",
      "Batch[210] - loss: 0.596179  acc: 75.0000%(12/16)\n",
      "Batch[211] - loss: 0.464330  acc: 78.0000%(50/64)\n",
      "Batch[212] - loss: 0.516530  acc: 79.0000%(51/64)\n",
      "Batch[213] - loss: 0.514947  acc: 78.0000%(50/64)\n",
      "Batch[214] - loss: 0.508584  acc: 78.0000%(50/64)\n",
      "Batch[215] - loss: 0.517991  acc: 76.0000%(49/64)\n",
      "Batch[216] - loss: 0.453979  acc: 82.0000%(53/64)\n",
      "Batch[217] - loss: 0.418327  acc: 81.0000%(52/64)\n",
      "Batch[218] - loss: 0.456423  acc: 76.0000%(49/64)\n",
      "Batch[219] - loss: 0.464537  acc: 75.0000%(48/64)\n",
      "Batch[220] - loss: 0.438082  acc: 78.0000%(50/64)\n",
      "Batch[221] - loss: 0.460436  acc: 78.0000%(50/64)\n",
      "Batch[222] - loss: 0.506981  acc: 75.0000%(48/64)\n",
      "Batch[223] - loss: 0.540260  acc: 71.0000%(46/64)\n",
      "Batch[224] - loss: 0.396666  acc: 84.0000%(54/64)\n",
      "Batch[225] - loss: 0.518915  acc: 75.0000%(48/64)\n",
      "Batch[226] - loss: 0.488080  acc: 71.0000%(46/64)\n",
      "Batch[227] - loss: 0.406110  acc: 82.0000%(53/64)\n",
      "Batch[228] - loss: 0.421777  acc: 79.0000%(51/64)\n",
      "Batch[229] - loss: 0.412089  acc: 82.0000%(53/64)\n",
      "Batch[230] - loss: 0.400023  acc: 82.0000%(53/64)\n",
      "Batch[231] - loss: 0.389652  acc: 84.0000%(54/64)\n",
      "Batch[232] - loss: 0.475210  acc: 79.0000%(51/64)\n",
      "Batch[233] - loss: 0.493325  acc: 78.0000%(50/64)\n",
      "Batch[234] - loss: 0.335605  acc: 84.0000%(54/64)\n",
      "Batch[235] - loss: 0.486934  acc: 76.0000%(49/64)\n",
      "Batch[236] - loss: 0.460091  acc: 75.0000%(48/64)\n",
      "Batch[237] - loss: 0.376002  acc: 85.0000%(55/64)\n",
      "Batch[238] - loss: 0.477927  acc: 75.0000%(48/64)\n",
      "Batch[239] - loss: 0.497821  acc: 79.0000%(51/64)\n",
      "Batch[240] - loss: 0.380610  acc: 81.0000%(13/16)\n",
      "Batch[241] - loss: 0.309070  acc: 85.0000%(55/64)\n",
      "Batch[242] - loss: 0.399460  acc: 84.0000%(54/64)\n",
      "Batch[243] - loss: 0.540393  acc: 75.0000%(48/64)\n",
      "Batch[244] - loss: 0.565969  acc: 78.0000%(50/64)\n",
      "Batch[245] - loss: 0.353156  acc: 84.0000%(54/64)\n",
      "Batch[246] - loss: 0.512890  acc: 67.0000%(43/64)\n",
      "Batch[247] - loss: 0.455896  acc: 82.0000%(53/64)\n",
      "Batch[248] - loss: 0.316498  acc: 84.0000%(54/64)\n",
      "Batch[249] - loss: 0.417080  acc: 84.0000%(54/64)\n",
      "Batch[250] - loss: 0.478363  acc: 76.0000%(49/64)\n",
      "Batch[251] - loss: 0.429446  acc: 79.0000%(51/64)\n",
      "Batch[252] - loss: 0.480575  acc: 73.0000%(47/64)\n",
      "Batch[253] - loss: 0.399702  acc: 79.0000%(51/64)\n",
      "Batch[254] - loss: 0.341839  acc: 84.0000%(54/64)\n",
      "Batch[255] - loss: 0.536565  acc: 73.0000%(47/64)\n",
      "Batch[256] - loss: 0.397668  acc: 82.0000%(53/64)\n",
      "Batch[257] - loss: 0.317238  acc: 87.0000%(56/64)\n",
      "Batch[258] - loss: 0.366921  acc: 79.0000%(51/64)\n",
      "Batch[259] - loss: 0.442475  acc: 84.0000%(54/64)\n",
      "Batch[260] - loss: 0.423781  acc: 79.0000%(51/64)\n",
      "Batch[261] - loss: 0.445601  acc: 81.0000%(52/64)\n",
      "Batch[262] - loss: 0.444692  acc: 78.0000%(50/64)\n",
      "Batch[263] - loss: 0.434074  acc: 81.0000%(52/64)\n",
      "Batch[264] - loss: 0.364348  acc: 85.0000%(55/64)\n",
      "Batch[265] - loss: 0.515969  acc: 73.0000%(47/64)\n",
      "Batch[266] - loss: 0.442673  acc: 81.0000%(52/64)\n",
      "Batch[267] - loss: 0.401290  acc: 76.0000%(49/64)\n",
      "Batch[268] - loss: 0.344374  acc: 87.0000%(56/64)\n",
      "Batch[269] - loss: 0.597380  acc: 70.0000%(45/64)\n",
      "Batch[270] - loss: 0.402039  acc: 81.0000%(13/16)\n",
      "Batch[271] - loss: 0.531381  acc: 75.0000%(48/64)\n",
      "Batch[272] - loss: 0.432554  acc: 79.0000%(51/64)\n",
      "Batch[273] - loss: 0.484655  acc: 78.0000%(50/64)\n",
      "Batch[274] - loss: 0.455404  acc: 79.0000%(51/64)\n",
      "Batch[275] - loss: 0.513876  acc: 73.0000%(47/64)\n",
      "Batch[276] - loss: 0.532816  acc: 73.0000%(47/64)\n",
      "Batch[277] - loss: 0.509000  acc: 75.0000%(48/64)\n",
      "Batch[278] - loss: 0.474875  acc: 73.0000%(47/64)\n",
      "Batch[279] - loss: 0.423244  acc: 78.0000%(50/64)\n",
      "Batch[280] - loss: 0.364756  acc: 81.0000%(52/64)\n",
      "Batch[281] - loss: 0.514401  acc: 76.0000%(49/64)\n",
      "Batch[282] - loss: 0.622636  acc: 56.0000%(36/64)\n",
      "Batch[283] - loss: 0.667187  acc: 65.0000%(42/64)\n",
      "Batch[284] - loss: 0.446309  acc: 81.0000%(52/64)\n",
      "Batch[285] - loss: 0.498071  acc: 78.0000%(50/64)\n",
      "Batch[286] - loss: 0.487027  acc: 75.0000%(48/64)\n",
      "Batch[287] - loss: 0.458047  acc: 79.0000%(51/64)\n",
      "Batch[288] - loss: 0.447534  acc: 81.0000%(52/64)\n",
      "Batch[289] - loss: 0.416148  acc: 85.0000%(55/64)\n",
      "Batch[290] - loss: 0.434521  acc: 76.0000%(49/64)\n",
      "Batch[291] - loss: 0.473682  acc: 78.0000%(50/64)\n",
      "Batch[292] - loss: 0.379927  acc: 81.0000%(52/64)\n",
      "Batch[293] - loss: 0.514134  acc: 75.0000%(48/64)\n",
      "Batch[294] - loss: 0.492027  acc: 78.0000%(50/64)\n",
      "Batch[295] - loss: 0.324591  acc: 87.0000%(56/64)\n",
      "Batch[296] - loss: 0.462736  acc: 78.0000%(50/64)\n",
      "Batch[297] - loss: 0.419128  acc: 81.0000%(52/64)\n",
      "Batch[298] - loss: 0.433546  acc: 78.0000%(50/64)\n",
      "Batch[299] - loss: 0.404063  acc: 78.0000%(50/64)\n",
      "Batch[300] - loss: 0.342729  acc: 87.0000%(14/16)\n",
      "Evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 159/208 (76%)\n",
      "\n",
      "Batch[301] - loss: 0.347423  acc: 85.0000%(55/64)\n",
      "Batch[302] - loss: 0.394031  acc: 82.0000%(53/64)\n",
      "Batch[303] - loss: 0.466933  acc: 78.0000%(50/64)\n",
      "Batch[304] - loss: 0.549059  acc: 78.0000%(50/64)\n",
      "Batch[305] - loss: 0.552642  acc: 76.0000%(49/64)\n",
      "Batch[306] - loss: 0.282426  acc: 89.0000%(57/64)\n",
      "Batch[307] - loss: 0.372453  acc: 82.0000%(53/64)\n",
      "Batch[308] - loss: 0.323460  acc: 87.0000%(56/64)\n",
      "Batch[309] - loss: 0.531490  acc: 78.0000%(50/64)\n",
      "Batch[310] - loss: 0.462622  acc: 79.0000%(51/64)\n",
      "Batch[311] - loss: 0.307097  acc: 85.0000%(55/64)\n",
      "Batch[312] - loss: 0.346412  acc: 84.0000%(54/64)\n",
      "Batch[313] - loss: 0.380035  acc: 84.0000%(54/64)\n",
      "Batch[314] - loss: 0.267582  acc: 89.0000%(57/64)\n",
      "Batch[315] - loss: 0.374160  acc: 81.0000%(52/64)\n",
      "Batch[316] - loss: 0.415110  acc: 82.0000%(53/64)\n",
      "Batch[317] - loss: 0.485507  acc: 76.0000%(49/64)\n",
      "Batch[318] - loss: 0.431725  acc: 81.0000%(52/64)\n",
      "Batch[319] - loss: 0.432224  acc: 79.0000%(51/64)\n",
      "Batch[320] - loss: 0.329853  acc: 90.0000%(58/64)\n",
      "Batch[321] - loss: 0.499421  acc: 68.0000%(44/64)\n",
      "Batch[322] - loss: 0.327807  acc: 84.0000%(54/64)\n",
      "Batch[323] - loss: 0.397208  acc: 85.0000%(55/64)\n",
      "Batch[324] - loss: 0.362164  acc: 81.0000%(52/64)\n",
      "Batch[325] - loss: 0.515771  acc: 73.0000%(47/64)\n",
      "Batch[326] - loss: 0.502624  acc: 76.0000%(49/64)\n",
      "Batch[327] - loss: 0.383799  acc: 81.0000%(52/64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch[328] - loss: 0.456079  acc: 78.0000%(50/64)\n",
      "Batch[329] - loss: 0.387507  acc: 79.0000%(51/64)\n",
      "Batch[330] - loss: 0.355566  acc: 93.0000%(15/16)\n",
      "Batch[331] - loss: 0.299717  acc: 87.0000%(56/64)\n",
      "Batch[332] - loss: 0.648250  acc: 65.0000%(42/64)\n",
      "Batch[333] - loss: 0.369443  acc: 81.0000%(52/64)\n",
      "Batch[334] - loss: 0.356299  acc: 85.0000%(55/64)\n",
      "Batch[335] - loss: 0.421478  acc: 85.0000%(55/64)\n",
      "Batch[336] - loss: 0.262622  acc: 90.0000%(58/64)\n",
      "Batch[337] - loss: 0.489617  acc: 73.0000%(47/64)\n",
      "Batch[338] - loss: 0.541062  acc: 76.0000%(49/64)\n",
      "Batch[339] - loss: 0.521157  acc: 75.0000%(48/64)\n",
      "Batch[340] - loss: 0.492500  acc: 81.0000%(52/64)\n",
      "Batch[341] - loss: 0.286555  acc: 89.0000%(57/64)\n",
      "Batch[342] - loss: 0.412258  acc: 79.0000%(51/64)\n",
      "Batch[343] - loss: 0.455132  acc: 75.0000%(48/64)\n",
      "Batch[344] - loss: 0.459419  acc: 78.0000%(50/64)\n",
      "Batch[345] - loss: 0.563192  acc: 67.0000%(43/64)\n",
      "Batch[346] - loss: 0.403703  acc: 85.0000%(55/64)\n",
      "Batch[347] - loss: 0.384242  acc: 82.0000%(53/64)\n",
      "Batch[348] - loss: 0.386780  acc: 79.0000%(51/64)\n",
      "Batch[349] - loss: 0.443111  acc: 78.0000%(50/64)\n",
      "Batch[350] - loss: 0.387443  acc: 78.0000%(50/64)\n",
      "Batch[351] - loss: 0.301425  acc: 87.0000%(56/64)\n",
      "Batch[352] - loss: 0.386108  acc: 84.0000%(54/64)\n",
      "Batch[353] - loss: 0.450543  acc: 79.0000%(51/64)\n",
      "Batch[354] - loss: 0.356878  acc: 84.0000%(54/64)\n",
      "Batch[355] - loss: 0.387105  acc: 81.0000%(52/64)\n",
      "Batch[356] - loss: 0.414489  acc: 79.0000%(51/64)\n",
      "Batch[357] - loss: 0.370230  acc: 82.0000%(53/64)\n",
      "Batch[358] - loss: 0.369941  acc: 87.0000%(56/64)\n",
      "Batch[359] - loss: 0.497824  acc: 76.0000%(49/64)\n",
      "Batch[360] - loss: 0.384270  acc: 75.0000%(12/16)\n",
      "Batch[361] - loss: 0.280874  acc: 92.0000%(59/64)\n",
      "Batch[362] - loss: 0.395823  acc: 81.0000%(52/64)\n",
      "Batch[363] - loss: 0.352044  acc: 85.0000%(55/64)\n",
      "Batch[364] - loss: 0.343295  acc: 87.0000%(56/64)\n",
      "Batch[365] - loss: 0.475466  acc: 81.0000%(52/64)\n",
      "Batch[366] - loss: 0.288909  acc: 89.0000%(57/64)\n",
      "Batch[367] - loss: 0.493924  acc: 73.0000%(47/64)\n",
      "Batch[368] - loss: 0.471300  acc: 76.0000%(49/64)\n",
      "Batch[369] - loss: 0.410451  acc: 79.0000%(51/64)\n",
      "Batch[370] - loss: 0.443464  acc: 76.0000%(49/64)\n",
      "Batch[371] - loss: 0.333858  acc: 85.0000%(55/64)\n",
      "Batch[372] - loss: 0.334764  acc: 89.0000%(57/64)\n",
      "Batch[373] - loss: 0.460123  acc: 76.0000%(49/64)\n",
      "Batch[374] - loss: 0.548871  acc: 70.0000%(45/64)\n",
      "Batch[375] - loss: 0.428006  acc: 78.0000%(50/64)\n",
      "Batch[376] - loss: 0.500234  acc: 71.0000%(46/64)\n",
      "Batch[377] - loss: 0.295796  acc: 90.0000%(58/64)\n",
      "Batch[378] - loss: 0.361452  acc: 82.0000%(53/64)\n",
      "Batch[379] - loss: 0.504448  acc: 75.0000%(48/64)\n",
      "Batch[380] - loss: 0.427832  acc: 79.0000%(51/64)\n",
      "Batch[381] - loss: 0.429226  acc: 81.0000%(52/64)\n",
      "Batch[382] - loss: 0.451693  acc: 79.0000%(51/64)\n",
      "Batch[383] - loss: 0.355395  acc: 85.0000%(55/64)\n",
      "Batch[384] - loss: 0.331083  acc: 87.0000%(56/64)\n",
      "Batch[385] - loss: 0.292521  acc: 90.0000%(58/64)\n",
      "Batch[386] - loss: 0.465178  acc: 82.0000%(53/64)\n",
      "Batch[387] - loss: 0.347372  acc: 84.0000%(54/64)\n",
      "Batch[388] - loss: 0.402836  acc: 78.0000%(50/64)\n",
      "Batch[389] - loss: 0.402874  acc: 79.0000%(51/64)\n",
      "Batch[390] - loss: 0.236606  acc: 87.0000%(14/16)\n",
      "Batch[391] - loss: 0.442205  acc: 79.0000%(51/64)\n",
      "Batch[392] - loss: 0.397683  acc: 82.0000%(53/64)\n",
      "Batch[393] - loss: 0.465573  acc: 79.0000%(51/64)\n",
      "Batch[394] - loss: 0.343291  acc: 84.0000%(54/64)\n",
      "Batch[395] - loss: 0.410298  acc: 81.0000%(52/64)\n",
      "Batch[396] - loss: 0.397616  acc: 82.0000%(53/64)\n",
      "Batch[397] - loss: 0.423983  acc: 79.0000%(51/64)\n",
      "Batch[398] - loss: 0.307851  acc: 81.0000%(52/64)\n",
      "Batch[399] - loss: 0.480299  acc: 79.0000%(51/64)\n",
      "Batch[400] - loss: 0.432117  acc: 81.0000%(52/64)\n",
      "Evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 156/208 (75%)\n",
      "\n",
      "Batch[401] - loss: 0.377370  acc: 85.0000%(55/64)\n",
      "Batch[402] - loss: 0.270672  acc: 89.0000%(57/64)\n",
      "Batch[403] - loss: 0.332698  acc: 87.0000%(56/64)\n",
      "Batch[404] - loss: 0.451419  acc: 76.0000%(49/64)\n",
      "Batch[405] - loss: 0.328035  acc: 87.0000%(56/64)\n",
      "Batch[406] - loss: 0.280110  acc: 89.0000%(57/64)\n",
      "Batch[407] - loss: 0.457700  acc: 79.0000%(51/64)\n",
      "Batch[408] - loss: 0.447332  acc: 78.0000%(50/64)\n",
      "Batch[409] - loss: 0.321518  acc: 90.0000%(58/64)\n",
      "Batch[410] - loss: 0.411395  acc: 84.0000%(54/64)\n",
      "Batch[411] - loss: 0.380987  acc: 84.0000%(54/64)\n",
      "Batch[412] - loss: 0.380571  acc: 82.0000%(53/64)\n",
      "Batch[413] - loss: 0.336766  acc: 85.0000%(55/64)\n",
      "Batch[414] - loss: 0.310215  acc: 89.0000%(57/64)\n",
      "Batch[415] - loss: 0.480048  acc: 78.0000%(50/64)\n",
      "Batch[416] - loss: 0.276469  acc: 93.0000%(60/64)\n",
      "Batch[417] - loss: 0.386134  acc: 85.0000%(55/64)\n",
      "Batch[418] - loss: 0.270292  acc: 89.0000%(57/64)\n",
      "Batch[419] - loss: 0.472425  acc: 78.0000%(50/64)\n",
      "Batch[420] - loss: 0.760033  acc: 62.0000%(10/16)\n",
      "Batch[421] - loss: 0.365628  acc: 79.0000%(51/64)\n",
      "Batch[422] - loss: 0.249085  acc: 85.0000%(55/64)\n",
      "Batch[423] - loss: 0.283422  acc: 89.0000%(57/64)\n",
      "Batch[424] - loss: 0.262996  acc: 92.0000%(59/64)\n",
      "Batch[425] - loss: 0.332837  acc: 79.0000%(51/64)\n",
      "Batch[426] - loss: 0.298369  acc: 84.0000%(54/64)\n",
      "Batch[427] - loss: 0.444019  acc: 81.0000%(52/64)\n",
      "Batch[428] - loss: 0.359922  acc: 87.0000%(56/64)\n",
      "Batch[429] - loss: 0.390526  acc: 82.0000%(53/64)\n",
      "Batch[430] - loss: 0.309001  acc: 85.0000%(55/64)\n",
      "Batch[431] - loss: 0.346002  acc: 82.0000%(53/64)\n",
      "Batch[432] - loss: 0.431228  acc: 82.0000%(53/64)\n",
      "Batch[433] - loss: 0.401785  acc: 84.0000%(54/64)\n",
      "Batch[434] - loss: 0.287529  acc: 90.0000%(58/64)\n",
      "Batch[435] - loss: 0.365994  acc: 87.0000%(56/64)\n",
      "Batch[436] - loss: 0.356511  acc: 82.0000%(53/64)\n",
      "Batch[437] - loss: 0.353139  acc: 84.0000%(54/64)\n",
      "Batch[438] - loss: 0.388318  acc: 84.0000%(54/64)\n",
      "Batch[439] - loss: 0.356667  acc: 82.0000%(53/64)\n",
      "Batch[440] - loss: 0.452130  acc: 78.0000%(50/64)\n",
      "Batch[441] - loss: 0.492891  acc: 73.0000%(47/64)\n",
      "Batch[442] - loss: 0.405087  acc: 82.0000%(53/64)\n",
      "Batch[443] - loss: 0.382650  acc: 81.0000%(52/64)\n",
      "Batch[444] - loss: 0.473969  acc: 76.0000%(49/64)\n",
      "Batch[445] - loss: 0.483874  acc: 78.0000%(50/64)\n",
      "Batch[446] - loss: 0.365090  acc: 87.0000%(56/64)\n",
      "Batch[447] - loss: 0.366856  acc: 82.0000%(53/64)\n",
      "Batch[448] - loss: 0.304969  acc: 85.0000%(55/64)\n",
      "Batch[449] - loss: 0.401224  acc: 79.0000%(51/64)\n",
      "Batch[450] - loss: 0.646037  acc: 68.0000%(11/16)\n",
      "Batch[451] - loss: 0.315684  acc: 89.0000%(57/64)\n",
      "Batch[452] - loss: 0.384062  acc: 84.0000%(54/64)\n",
      "Batch[453] - loss: 0.336794  acc: 89.0000%(57/64)\n",
      "Batch[454] - loss: 0.301346  acc: 90.0000%(58/64)\n",
      "Batch[455] - loss: 0.303477  acc: 87.0000%(56/64)\n",
      "Batch[456] - loss: 0.337609  acc: 89.0000%(57/64)\n",
      "Batch[457] - loss: 0.401543  acc: 82.0000%(53/64)\n",
      "Batch[458] - loss: 0.371714  acc: 81.0000%(52/64)\n",
      "Batch[459] - loss: 0.345663  acc: 82.0000%(53/64)\n",
      "Batch[460] - loss: 0.359058  acc: 79.0000%(51/64)\n",
      "Batch[461] - loss: 0.350011  acc: 87.0000%(56/64)\n",
      "Batch[462] - loss: 0.285847  acc: 87.0000%(56/64)\n",
      "Batch[463] - loss: 0.331333  acc: 82.0000%(53/64)\n",
      "Batch[464] - loss: 0.468600  acc: 75.0000%(48/64)\n",
      "Batch[465] - loss: 0.351771  acc: 87.0000%(56/64)\n",
      "Batch[466] - loss: 0.381787  acc: 84.0000%(54/64)\n",
      "Batch[467] - loss: 0.385380  acc: 84.0000%(54/64)\n",
      "Batch[468] - loss: 0.489088  acc: 78.0000%(50/64)\n",
      "Batch[469] - loss: 0.327944  acc: 81.0000%(52/64)\n",
      "Batch[470] - loss: 0.345660  acc: 81.0000%(52/64)\n",
      "Batch[471] - loss: 0.355441  acc: 82.0000%(53/64)\n",
      "Batch[472] - loss: 0.287310  acc: 87.0000%(56/64)\n",
      "Batch[473] - loss: 0.429670  acc: 76.0000%(49/64)\n",
      "Batch[474] - loss: 0.280606  acc: 92.0000%(59/64)\n",
      "Batch[475] - loss: 0.505025  acc: 76.0000%(49/64)\n",
      "Batch[476] - loss: 0.479174  acc: 73.0000%(47/64)\n",
      "Batch[477] - loss: 0.420360  acc: 78.0000%(50/64)\n",
      "Batch[478] - loss: 0.342014  acc: 85.0000%(55/64)\n",
      "Batch[479] - loss: 0.295018  acc: 89.0000%(57/64)\n",
      "Batch[480] - loss: 0.710124  acc: 56.0000%(9/16)\n",
      "Batch[481] - loss: 0.357660  acc: 82.0000%(53/64)\n",
      "Batch[482] - loss: 0.318164  acc: 82.0000%(53/64)\n",
      "Batch[483] - loss: 0.353325  acc: 84.0000%(54/64)\n",
      "Batch[484] - loss: 0.358661  acc: 82.0000%(53/64)\n",
      "Batch[485] - loss: 0.279315  acc: 90.0000%(58/64)\n",
      "Batch[486] - loss: 0.361257  acc: 81.0000%(52/64)\n",
      "Batch[487] - loss: 0.354134  acc: 84.0000%(54/64)\n",
      "Batch[488] - loss: 0.479064  acc: 75.0000%(48/64)\n",
      "Batch[489] - loss: 0.360517  acc: 84.0000%(54/64)\n",
      "Batch[490] - loss: 0.387078  acc: 85.0000%(55/64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch[491] - loss: 0.333467  acc: 85.0000%(55/64)\n",
      "Batch[492] - loss: 0.432385  acc: 79.0000%(51/64)\n",
      "Batch[493] - loss: 0.284313  acc: 87.0000%(56/64)\n",
      "Batch[494] - loss: 0.318990  acc: 87.0000%(56/64)\n",
      "Batch[495] - loss: 0.400277  acc: 82.0000%(53/64)\n",
      "Batch[496] - loss: 0.312658  acc: 87.0000%(56/64)\n",
      "Batch[497] - loss: 0.378636  acc: 81.0000%(52/64)\n",
      "Batch[498] - loss: 0.435080  acc: 76.0000%(49/64)\n",
      "Batch[499] - loss: 0.236386  acc: 90.0000%(58/64)\n",
      "Batch[500] - loss: 0.461008  acc: 78.0000%(50/64)\n",
      "Evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 157/208 (75%)\n",
      "\n",
      "Batch[501] - loss: 0.386515  acc: 82.0000%(53/64)\n",
      "Batch[502] - loss: 0.359872  acc: 78.0000%(50/64)\n",
      "Batch[503] - loss: 0.314172  acc: 87.0000%(56/64)\n",
      "Batch[504] - loss: 0.337637  acc: 84.0000%(54/64)\n",
      "Batch[505] - loss: 0.456527  acc: 79.0000%(51/64)\n",
      "Batch[506] - loss: 0.481016  acc: 71.0000%(46/64)\n",
      "Batch[507] - loss: 0.294910  acc: 89.0000%(57/64)\n",
      "Batch[508] - loss: 0.402767  acc: 81.0000%(52/64)\n",
      "Batch[509] - loss: 0.369449  acc: 82.0000%(53/64)\n",
      "Batch[510] - loss: 0.240297  acc: 93.0000%(15/16)\n",
      "Batch[511] - loss: 0.513614  acc: 76.0000%(49/64)\n",
      "Batch[512] - loss: 0.289399  acc: 90.0000%(58/64)\n",
      "Batch[513] - loss: 0.344425  acc: 84.0000%(54/64)\n",
      "Batch[514] - loss: 0.526053  acc: 76.0000%(49/64)\n",
      "Batch[515] - loss: 0.285310  acc: 93.0000%(60/64)\n",
      "Batch[516] - loss: 0.365039  acc: 81.0000%(52/64)\n",
      "Batch[517] - loss: 0.294925  acc: 87.0000%(56/64)\n",
      "Batch[518] - loss: 0.284871  acc: 87.0000%(56/64)\n",
      "Batch[519] - loss: 0.511263  acc: 70.0000%(45/64)\n",
      "Batch[520] - loss: 0.396026  acc: 76.0000%(49/64)\n",
      "Batch[521] - loss: 0.405038  acc: 84.0000%(54/64)\n",
      "Batch[522] - loss: 0.393646  acc: 81.0000%(52/64)\n",
      "Batch[523] - loss: 0.383333  acc: 84.0000%(54/64)\n",
      "Batch[524] - loss: 0.400451  acc: 82.0000%(53/64)\n",
      "Batch[525] - loss: 0.345425  acc: 87.0000%(56/64)\n",
      "Batch[526] - loss: 0.277336  acc: 89.0000%(57/64)\n",
      "Batch[527] - loss: 0.322827  acc: 89.0000%(57/64)\n",
      "Batch[528] - loss: 0.315624  acc: 85.0000%(55/64)\n",
      "Batch[529] - loss: 0.445403  acc: 84.0000%(54/64)\n",
      "Batch[530] - loss: 0.373218  acc: 82.0000%(53/64)\n",
      "Batch[531] - loss: 0.273221  acc: 89.0000%(57/64)\n",
      "Batch[532] - loss: 0.316325  acc: 82.0000%(53/64)\n",
      "Batch[533] - loss: 0.272411  acc: 87.0000%(56/64)\n",
      "Batch[534] - loss: 0.209631  acc: 96.0000%(62/64)\n",
      "Batch[535] - loss: 0.300355  acc: 85.0000%(55/64)\n",
      "Batch[536] - loss: 0.339503  acc: 85.0000%(55/64)\n",
      "Batch[537] - loss: 0.405464  acc: 82.0000%(53/64)\n",
      "Batch[538] - loss: 0.354175  acc: 84.0000%(54/64)\n",
      "Batch[539] - loss: 0.448678  acc: 81.0000%(52/64)\n",
      "Batch[540] - loss: 0.602329  acc: 75.0000%(12/16)\n",
      "Batch[541] - loss: 0.373773  acc: 85.0000%(55/64)\n",
      "Batch[542] - loss: 0.320066  acc: 90.0000%(58/64)\n",
      "Batch[543] - loss: 0.292841  acc: 87.0000%(56/64)\n",
      "Batch[544] - loss: 0.282725  acc: 84.0000%(54/64)\n",
      "Batch[545] - loss: 0.347890  acc: 85.0000%(55/64)\n",
      "Batch[546] - loss: 0.343661  acc: 82.0000%(53/64)\n",
      "Batch[547] - loss: 0.380615  acc: 84.0000%(54/64)\n",
      "Batch[548] - loss: 0.277218  acc: 89.0000%(57/64)\n",
      "Batch[549] - loss: 0.360915  acc: 79.0000%(51/64)\n",
      "Batch[550] - loss: 0.312530  acc: 87.0000%(56/64)\n",
      "Batch[551] - loss: 0.323281  acc: 82.0000%(53/64)\n",
      "Batch[552] - loss: 0.476554  acc: 78.0000%(50/64)\n",
      "Batch[553] - loss: 0.333715  acc: 87.0000%(56/64)\n",
      "Batch[554] - loss: 0.391342  acc: 85.0000%(55/64)\n",
      "Batch[555] - loss: 0.240762  acc: 90.0000%(58/64)\n",
      "Batch[556] - loss: 0.437387  acc: 82.0000%(53/64)\n",
      "Batch[557] - loss: 0.407374  acc: 81.0000%(52/64)\n",
      "Batch[558] - loss: 0.355255  acc: 84.0000%(54/64)\n",
      "Batch[559] - loss: 0.312432  acc: 89.0000%(57/64)\n",
      "Batch[560] - loss: 0.360904  acc: 82.0000%(53/64)\n",
      "Batch[561] - loss: 0.325402  acc: 87.0000%(56/64)\n",
      "Batch[562] - loss: 0.373238  acc: 85.0000%(55/64)\n",
      "Batch[563] - loss: 0.243280  acc: 90.0000%(58/64)\n",
      "Batch[564] - loss: 0.380829  acc: 82.0000%(53/64)\n",
      "Batch[565] - loss: 0.426794  acc: 81.0000%(52/64)\n",
      "Batch[566] - loss: 0.446023  acc: 76.0000%(49/64)\n",
      "Batch[567] - loss: 0.313684  acc: 87.0000%(56/64)\n",
      "Batch[568] - loss: 0.263881  acc: 87.0000%(56/64)\n",
      "Batch[569] - loss: 0.402060  acc: 84.0000%(54/64)\n",
      "Batch[570] - loss: 0.294570  acc: 87.0000%(14/16)\n",
      "Batch[571] - loss: 0.311858  acc: 85.0000%(55/64)\n",
      "Batch[572] - loss: 0.271005  acc: 89.0000%(57/64)\n",
      "Batch[573] - loss: 0.391746  acc: 79.0000%(51/64)\n",
      "Batch[574] - loss: 0.295777  acc: 85.0000%(55/64)\n",
      "Batch[575] - loss: 0.271032  acc: 90.0000%(58/64)\n",
      "Batch[576] - loss: 0.297870  acc: 90.0000%(58/64)\n",
      "Batch[577] - loss: 0.335116  acc: 85.0000%(55/64)\n",
      "Batch[578] - loss: 0.366995  acc: 82.0000%(53/64)\n",
      "Batch[579] - loss: 0.355313  acc: 85.0000%(55/64)\n",
      "Batch[580] - loss: 0.385451  acc: 79.0000%(51/64)\n",
      "Batch[581] - loss: 0.383677  acc: 84.0000%(54/64)\n",
      "Batch[582] - loss: 0.577094  acc: 75.0000%(48/64)\n",
      "Batch[583] - loss: 0.297649  acc: 87.0000%(56/64)\n",
      "Batch[584] - loss: 0.274124  acc: 89.0000%(57/64)\n",
      "Batch[585] - loss: 0.299882  acc: 84.0000%(54/64)\n",
      "Batch[586] - loss: 0.349833  acc: 85.0000%(55/64)\n",
      "Batch[587] - loss: 0.403478  acc: 85.0000%(55/64)\n",
      "Batch[588] - loss: 0.313590  acc: 82.0000%(53/64)\n",
      "Batch[589] - loss: 0.511582  acc: 78.0000%(50/64)\n",
      "Batch[590] - loss: 0.342020  acc: 82.0000%(53/64)\n",
      "Batch[591] - loss: 0.295214  acc: 89.0000%(57/64)\n",
      "Batch[592] - loss: 0.329070  acc: 82.0000%(53/64)\n",
      "Batch[593] - loss: 0.485645  acc: 75.0000%(48/64)\n",
      "Batch[594] - loss: 0.304077  acc: 85.0000%(55/64)\n",
      "Batch[595] - loss: 0.409460  acc: 78.0000%(50/64)\n",
      "Batch[596] - loss: 0.360744  acc: 78.0000%(50/64)\n",
      "Batch[597] - loss: 0.345772  acc: 84.0000%(54/64)\n",
      "Batch[598] - loss: 0.326330  acc: 87.0000%(56/64)\n",
      "Batch[599] - loss: 0.389731  acc: 81.0000%(52/64)\n",
      "Batch[600] - loss: 0.341658  acc: 87.0000%(14/16)\n",
      "Evaluating trained model ...\n",
      "\n",
      "Test set: Accuracy: 153/208 (73%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train(classifier, train_loader, test_loader)\n",
    "except KeyboardInterrupt:\n",
    "    print('\\n' + '-' * 89)\n",
    "    print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'nn_train_poetry.csv'\n",
    "with open(file, 'r', newline='', encoding='utf-8') as myFile:  \n",
    "    rdr = csv.reader(myFile)\n",
    "    data = list(rdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-+-+-+-+-+\\t+-++--++-+\\t++-+-+-+-+\\t-+-+-+++-+\\t+-++-+-+-+\\t-+-+-+-+-+\\t-+-+-++--+\\t-+-+-+-+-+\\t-+-+-+-+-+\\t-+-+-+-+-+\\t+-++-+-+-+\\t-+-+-+-+-+\\t-+-+-+-+-+\\t-+++-+-+-+'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
